{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-24T17:59:06.296973Z",
          "start_time": "2023-12-24T17:59:06.288204Z"
        },
        "id": "eRR3bAjgFfSY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul, SparseStorage\n",
        "# from torch_sparse.tensor import SparseTensor\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import OptTensor, PairTensor\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch_scatter import scatter\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        ")\n",
        "from typing import Tuple, Union, Dict, Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary functions and classes from GraphBEAN.\n",
        "# You can clone their repo here: https://github.com/grab/GraphBEAN/tree/master\n",
        "\n",
        "from models.data import BipartiteData\n",
        "from models.net import GraphBEAN\n",
        "from models.sampler import EdgePredictionSampler\n",
        "from models.loss import reconstruction_loss\n",
        "from models.score import compute_anomaly_score, edge_prediction_metric\n",
        "\n",
        "from anomaly_insert import (\n",
        "    choose,\n",
        "    dense_block,\n",
        "    inject_dense_block_anomaly,\n",
        "    inject_dense_block_and_feature_anomaly,\n",
        "    inject_feature_anomaly,\n",
        "    inject_random_block_anomaly,\n",
        "    outside_confidence_interval,\n",
        "    scaled_gaussian_noise\n",
        ")"
      ],
      "metadata": {
        "id": "KNliQpCcGntr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPO08_qJFfSa"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:22.808332Z",
          "start_time": "2023-12-09T02:46:22.443445Z"
        },
        "id": "6QJUf03TFfSb"
      },
      "outputs": [],
      "source": [
        "df_in = pd.read_csv('./dataset/Train_Inpatientdata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:22.943355Z",
          "start_time": "2023-12-09T02:46:22.917750Z"
        },
        "id": "9mq9XN0bFfSb",
        "outputId": "6f77c92a-7de0-4ee3-f221-873191b05718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40474, 30)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BeneID</th>\n",
              "      <th>ClaimID</th>\n",
              "      <th>ClaimStartDt</th>\n",
              "      <th>ClaimEndDt</th>\n",
              "      <th>Provider</th>\n",
              "      <th>InscClaimAmtReimbursed</th>\n",
              "      <th>AttendingPhysician</th>\n",
              "      <th>OperatingPhysician</th>\n",
              "      <th>OtherPhysician</th>\n",
              "      <th>AdmissionDt</th>\n",
              "      <th>...</th>\n",
              "      <th>ClmDiagnosisCode_7</th>\n",
              "      <th>ClmDiagnosisCode_8</th>\n",
              "      <th>ClmDiagnosisCode_9</th>\n",
              "      <th>ClmDiagnosisCode_10</th>\n",
              "      <th>ClmProcedureCode_1</th>\n",
              "      <th>ClmProcedureCode_2</th>\n",
              "      <th>ClmProcedureCode_3</th>\n",
              "      <th>ClmProcedureCode_4</th>\n",
              "      <th>ClmProcedureCode_5</th>\n",
              "      <th>ClmProcedureCode_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BENE11001</td>\n",
              "      <td>CLM46614</td>\n",
              "      <td>2009-04-12</td>\n",
              "      <td>2009-04-18</td>\n",
              "      <td>PRV55912</td>\n",
              "      <td>26000</td>\n",
              "      <td>PHY390922</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-04-12</td>\n",
              "      <td>...</td>\n",
              "      <td>2724</td>\n",
              "      <td>19889</td>\n",
              "      <td>5849</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BENE11001</td>\n",
              "      <td>CLM66048</td>\n",
              "      <td>2009-08-31</td>\n",
              "      <td>2009-09-02</td>\n",
              "      <td>PRV55907</td>\n",
              "      <td>5000</td>\n",
              "      <td>PHY318495</td>\n",
              "      <td>PHY318495</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-08-31</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7092.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BENE11001</td>\n",
              "      <td>CLM68358</td>\n",
              "      <td>2009-09-17</td>\n",
              "      <td>2009-09-20</td>\n",
              "      <td>PRV56046</td>\n",
              "      <td>5000</td>\n",
              "      <td>PHY372395</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PHY324689</td>\n",
              "      <td>2009-09-17</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BENE11011</td>\n",
              "      <td>CLM38412</td>\n",
              "      <td>2009-02-14</td>\n",
              "      <td>2009-02-22</td>\n",
              "      <td>PRV52405</td>\n",
              "      <td>5000</td>\n",
              "      <td>PHY369659</td>\n",
              "      <td>PHY392961</td>\n",
              "      <td>PHY349768</td>\n",
              "      <td>2009-02-14</td>\n",
              "      <td>...</td>\n",
              "      <td>25062</td>\n",
              "      <td>40390</td>\n",
              "      <td>4019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>331.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BENE11014</td>\n",
              "      <td>CLM63689</td>\n",
              "      <td>2009-08-13</td>\n",
              "      <td>2009-08-30</td>\n",
              "      <td>PRV56614</td>\n",
              "      <td>10000</td>\n",
              "      <td>PHY379376</td>\n",
              "      <td>PHY398258</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-08-13</td>\n",
              "      <td>...</td>\n",
              "      <td>5119</td>\n",
              "      <td>29620</td>\n",
              "      <td>20300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3893.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      BeneID   ClaimID ClaimStartDt  ClaimEndDt  Provider  \\\n",
              "0  BENE11001  CLM46614   2009-04-12  2009-04-18  PRV55912   \n",
              "1  BENE11001  CLM66048   2009-08-31  2009-09-02  PRV55907   \n",
              "2  BENE11001  CLM68358   2009-09-17  2009-09-20  PRV56046   \n",
              "3  BENE11011  CLM38412   2009-02-14  2009-02-22  PRV52405   \n",
              "4  BENE11014  CLM63689   2009-08-13  2009-08-30  PRV56614   \n",
              "\n",
              "   InscClaimAmtReimbursed AttendingPhysician OperatingPhysician  \\\n",
              "0                   26000          PHY390922                NaN   \n",
              "1                    5000          PHY318495          PHY318495   \n",
              "2                    5000          PHY372395                NaN   \n",
              "3                    5000          PHY369659          PHY392961   \n",
              "4                   10000          PHY379376          PHY398258   \n",
              "\n",
              "  OtherPhysician AdmissionDt  ... ClmDiagnosisCode_7  ClmDiagnosisCode_8  \\\n",
              "0            NaN  2009-04-12  ...               2724               19889   \n",
              "1            NaN  2009-08-31  ...                NaN                 NaN   \n",
              "2      PHY324689  2009-09-17  ...                NaN                 NaN   \n",
              "3      PHY349768  2009-02-14  ...              25062               40390   \n",
              "4            NaN  2009-08-13  ...               5119               29620   \n",
              "\n",
              "  ClmDiagnosisCode_9 ClmDiagnosisCode_10 ClmProcedureCode_1  \\\n",
              "0               5849                 NaN                NaN   \n",
              "1                NaN                 NaN             7092.0   \n",
              "2                NaN                 NaN                NaN   \n",
              "3               4019                 NaN              331.0   \n",
              "4              20300                 NaN             3893.0   \n",
              "\n",
              "  ClmProcedureCode_2 ClmProcedureCode_3 ClmProcedureCode_4 ClmProcedureCode_5  \\\n",
              "0                NaN                NaN                NaN                NaN   \n",
              "1                NaN                NaN                NaN                NaN   \n",
              "2                NaN                NaN                NaN                NaN   \n",
              "3                NaN                NaN                NaN                NaN   \n",
              "4                NaN                NaN                NaN                NaN   \n",
              "\n",
              "  ClmProcedureCode_6  \n",
              "0                NaN  \n",
              "1                NaN  \n",
              "2                NaN  \n",
              "3                NaN  \n",
              "4                NaN  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_in.shape)\n",
        "df_in.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:23.863336Z",
          "start_time": "2023-12-09T02:46:23.846867Z"
        },
        "id": "YiHxY5GHFfSb"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"./dataset/Train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:24.602323Z",
          "start_time": "2023-12-09T02:46:24.590951Z"
        },
        "id": "_C2bqhsYFfSb",
        "outputId": "4ca616c5-61bb-4977-bbbe-9d0b562a276f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5410, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Provider</th>\n",
              "      <th>PotentialFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PRV51001</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PRV51003</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PRV51004</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PRV51005</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PRV51007</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Provider PotentialFraud\n",
              "0  PRV51001             No\n",
              "1  PRV51003            Yes\n",
              "2  PRV51004             No\n",
              "3  PRV51005            Yes\n",
              "4  PRV51007             No"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_train.shape)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:25.275512Z",
          "start_time": "2023-12-09T02:46:25.208052Z"
        },
        "id": "ayqhq2EUFfSc"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(\n",
        "    df_in,\n",
        "    df_train,\n",
        "    on=\"Provider\",\n",
        "    how=\"left\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:25.908388Z",
          "start_time": "2023-12-09T02:46:25.883959Z"
        },
        "id": "MwJu-d63FfSc",
        "outputId": "a3a54725-3ace-4233-b999-b20e729fe95e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40474, 31)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BeneID</th>\n",
              "      <th>ClaimID</th>\n",
              "      <th>ClaimStartDt</th>\n",
              "      <th>ClaimEndDt</th>\n",
              "      <th>Provider</th>\n",
              "      <th>InscClaimAmtReimbursed</th>\n",
              "      <th>AttendingPhysician</th>\n",
              "      <th>OperatingPhysician</th>\n",
              "      <th>OtherPhysician</th>\n",
              "      <th>AdmissionDt</th>\n",
              "      <th>...</th>\n",
              "      <th>ClmDiagnosisCode_8</th>\n",
              "      <th>ClmDiagnosisCode_9</th>\n",
              "      <th>ClmDiagnosisCode_10</th>\n",
              "      <th>ClmProcedureCode_1</th>\n",
              "      <th>ClmProcedureCode_2</th>\n",
              "      <th>ClmProcedureCode_3</th>\n",
              "      <th>ClmProcedureCode_4</th>\n",
              "      <th>ClmProcedureCode_5</th>\n",
              "      <th>ClmProcedureCode_6</th>\n",
              "      <th>PotentialFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BENE11001</td>\n",
              "      <td>CLM46614</td>\n",
              "      <td>2009-04-12</td>\n",
              "      <td>2009-04-18</td>\n",
              "      <td>PRV55912</td>\n",
              "      <td>26000</td>\n",
              "      <td>PHY390922</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-04-12</td>\n",
              "      <td>...</td>\n",
              "      <td>19889</td>\n",
              "      <td>5849</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BENE11001</td>\n",
              "      <td>CLM66048</td>\n",
              "      <td>2009-08-31</td>\n",
              "      <td>2009-09-02</td>\n",
              "      <td>PRV55907</td>\n",
              "      <td>5000</td>\n",
              "      <td>PHY318495</td>\n",
              "      <td>PHY318495</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-08-31</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7092.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BENE11001</td>\n",
              "      <td>CLM68358</td>\n",
              "      <td>2009-09-17</td>\n",
              "      <td>2009-09-20</td>\n",
              "      <td>PRV56046</td>\n",
              "      <td>5000</td>\n",
              "      <td>PHY372395</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PHY324689</td>\n",
              "      <td>2009-09-17</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BENE11011</td>\n",
              "      <td>CLM38412</td>\n",
              "      <td>2009-02-14</td>\n",
              "      <td>2009-02-22</td>\n",
              "      <td>PRV52405</td>\n",
              "      <td>5000</td>\n",
              "      <td>PHY369659</td>\n",
              "      <td>PHY392961</td>\n",
              "      <td>PHY349768</td>\n",
              "      <td>2009-02-14</td>\n",
              "      <td>...</td>\n",
              "      <td>40390</td>\n",
              "      <td>4019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>331.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BENE11014</td>\n",
              "      <td>CLM63689</td>\n",
              "      <td>2009-08-13</td>\n",
              "      <td>2009-08-30</td>\n",
              "      <td>PRV56614</td>\n",
              "      <td>10000</td>\n",
              "      <td>PHY379376</td>\n",
              "      <td>PHY398258</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-08-13</td>\n",
              "      <td>...</td>\n",
              "      <td>29620</td>\n",
              "      <td>20300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3893.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      BeneID   ClaimID ClaimStartDt  ClaimEndDt  Provider  \\\n",
              "0  BENE11001  CLM46614   2009-04-12  2009-04-18  PRV55912   \n",
              "1  BENE11001  CLM66048   2009-08-31  2009-09-02  PRV55907   \n",
              "2  BENE11001  CLM68358   2009-09-17  2009-09-20  PRV56046   \n",
              "3  BENE11011  CLM38412   2009-02-14  2009-02-22  PRV52405   \n",
              "4  BENE11014  CLM63689   2009-08-13  2009-08-30  PRV56614   \n",
              "\n",
              "   InscClaimAmtReimbursed AttendingPhysician OperatingPhysician  \\\n",
              "0                   26000          PHY390922                NaN   \n",
              "1                    5000          PHY318495          PHY318495   \n",
              "2                    5000          PHY372395                NaN   \n",
              "3                    5000          PHY369659          PHY392961   \n",
              "4                   10000          PHY379376          PHY398258   \n",
              "\n",
              "  OtherPhysician AdmissionDt  ... ClmDiagnosisCode_8  ClmDiagnosisCode_9  \\\n",
              "0            NaN  2009-04-12  ...              19889                5849   \n",
              "1            NaN  2009-08-31  ...                NaN                 NaN   \n",
              "2      PHY324689  2009-09-17  ...                NaN                 NaN   \n",
              "3      PHY349768  2009-02-14  ...              40390                4019   \n",
              "4            NaN  2009-08-13  ...              29620               20300   \n",
              "\n",
              "  ClmDiagnosisCode_10 ClmProcedureCode_1 ClmProcedureCode_2  \\\n",
              "0                 NaN                NaN                NaN   \n",
              "1                 NaN             7092.0                NaN   \n",
              "2                 NaN                NaN                NaN   \n",
              "3                 NaN              331.0                NaN   \n",
              "4                 NaN             3893.0                NaN   \n",
              "\n",
              "  ClmProcedureCode_3 ClmProcedureCode_4 ClmProcedureCode_5 ClmProcedureCode_6  \\\n",
              "0                NaN                NaN                NaN                NaN   \n",
              "1                NaN                NaN                NaN                NaN   \n",
              "2                NaN                NaN                NaN                NaN   \n",
              "3                NaN                NaN                NaN                NaN   \n",
              "4                NaN                NaN                NaN                NaN   \n",
              "\n",
              "  PotentialFraud  \n",
              "0            Yes  \n",
              "1             No  \n",
              "2             No  \n",
              "3             No  \n",
              "4             No  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:26.505837Z",
          "start_time": "2023-12-09T02:46:26.433472Z"
        },
        "id": "8D53jmhZFfSc"
      },
      "outputs": [],
      "source": [
        "# Replace string with 1's and 0's for PotentialFraud column\n",
        "df.loc[df[\"PotentialFraud\"] == \"Yes\", \"PotentialFraud\"] = 1\n",
        "df.loc[df[\"PotentialFraud\"] == \"No\", \"PotentialFraud\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:27.077131Z",
          "start_time": "2023-12-09T02:46:27.053367Z"
        },
        "id": "qt5XrEeqFfSc",
        "outputId": "bc46692c-2bd4-4679-acc3-aa68c400b6c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    0.578198\n",
              "0    0.421802\n",
              "Name: PotentialFraud, dtype: float64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"PotentialFraud\"].value_counts(dropna=False, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc_f35v8FfSc"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "925qUxZ8FfSc"
      },
      "source": [
        "## Edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:32.443085Z",
          "start_time": "2023-12-09T02:46:29.010000Z"
        },
        "id": "uv1gXZ2nFfSd"
      },
      "outputs": [],
      "source": [
        "df_edge = df.groupby([\"BeneID\", \"AttendingPhysician\"]).agg(\n",
        "    count_claims=(\"ClaimID\", \"nunique\"),\n",
        "    count_providers=(\"Provider\", \"nunique\"),\n",
        "    total_reimbursed=(\"InscClaimAmtReimbursed\", \"sum\"),\n",
        "    avg_reimbursed=(\"InscClaimAmtReimbursed\", \"mean\"),\n",
        "    pct_fraud=(\"PotentialFraud\", lambda x: round(x.mean() * 100))\n",
        ").reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:32.525476Z",
          "start_time": "2023-12-09T02:46:32.513940Z"
        },
        "id": "kSl_LJ6zFfSd",
        "outputId": "79e189e2-5a77-436f-fa87-dcc9ad6033ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(38559, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BeneID</th>\n",
              "      <th>AttendingPhysician</th>\n",
              "      <th>count_claims</th>\n",
              "      <th>count_providers</th>\n",
              "      <th>total_reimbursed</th>\n",
              "      <th>avg_reimbursed</th>\n",
              "      <th>pct_fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BENE100002</td>\n",
              "      <td>PHY424317</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BENE100004</td>\n",
              "      <td>PHY319940</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3000</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BENE100006</td>\n",
              "      <td>PHY325217</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17000</td>\n",
              "      <td>17000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BENE100007</td>\n",
              "      <td>PHY415056</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4000</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BENE100010</td>\n",
              "      <td>PHY403299</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       BeneID AttendingPhysician  count_claims  count_providers  \\\n",
              "0  BENE100002          PHY424317             1                1   \n",
              "1  BENE100004          PHY319940             1                1   \n",
              "2  BENE100006          PHY325217             1                1   \n",
              "3  BENE100007          PHY415056             1                1   \n",
              "4  BENE100010          PHY403299             1                1   \n",
              "\n",
              "   total_reimbursed  avg_reimbursed  pct_fraud  \n",
              "0             12000         12000.0        100  \n",
              "1              3000          3000.0          0  \n",
              "2             17000         17000.0        100  \n",
              "3              4000          4000.0        100  \n",
              "4              8000          8000.0          0  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_edge.shape)\n",
        "df_edge.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR7emJjUFfSd"
      },
      "source": [
        "## Beneficiary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:36.113512Z",
          "start_time": "2023-12-09T02:46:33.420853Z"
        },
        "scrolled": false,
        "id": "6Mz8070ZFfSd"
      },
      "outputs": [],
      "source": [
        "df_bene = df.groupby(\"BeneID\").agg(\n",
        "    count_claims=(\"ClaimID\", \"nunique\"),\n",
        "    count_physicians=(\"AttendingPhysician\", \"nunique\"),\n",
        "    count_providers=(\"Provider\", \"nunique\"),\n",
        "    total_reimbursed=(\"InscClaimAmtReimbursed\", \"sum\"),\n",
        "    avg_reimbursed=(\"InscClaimAmtReimbursed\", \"mean\"),\n",
        "    pct_fraud=(\"PotentialFraud\", lambda x: round(x.mean() * 100))\n",
        ").reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:36.199178Z",
          "start_time": "2023-12-09T02:46:36.187864Z"
        },
        "id": "gVb5Yi9sFfSd",
        "outputId": "772778ac-9aa7-4c23-9766-f94a4c6e32f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31289, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BeneID</th>\n",
              "      <th>count_claims</th>\n",
              "      <th>count_physicians</th>\n",
              "      <th>count_providers</th>\n",
              "      <th>total_reimbursed</th>\n",
              "      <th>avg_reimbursed</th>\n",
              "      <th>pct_fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BENE100002</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BENE100004</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3000</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BENE100006</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17000</td>\n",
              "      <td>17000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BENE100007</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4000</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BENE100010</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>12000</td>\n",
              "      <td>6000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       BeneID  count_claims  count_physicians  count_providers  \\\n",
              "0  BENE100002             1                 1                1   \n",
              "1  BENE100004             1                 1                1   \n",
              "2  BENE100006             1                 1                1   \n",
              "3  BENE100007             1                 1                1   \n",
              "4  BENE100010             2                 2                1   \n",
              "\n",
              "   total_reimbursed  avg_reimbursed  pct_fraud  \n",
              "0             12000         12000.0        100  \n",
              "1              3000          3000.0          0  \n",
              "2             17000         17000.0        100  \n",
              "3              4000          4000.0        100  \n",
              "4             12000          6000.0          0  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_bene.shape)\n",
        "df_bene.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxpgdy3cFfSd"
      },
      "source": [
        "## Physician"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:38.722031Z",
          "start_time": "2023-12-09T02:46:37.672535Z"
        },
        "id": "GSkG73zIFfSd"
      },
      "outputs": [],
      "source": [
        "df_physician = df.groupby(\"AttendingPhysician\").agg(\n",
        "    count_claims=(\"ClaimID\", \"nunique\"),\n",
        "    count_beneficiaries=(\"BeneID\", \"nunique\"),\n",
        "    count_providers=(\"Provider\", \"nunique\"),\n",
        "    total_reimbursed=(\"InscClaimAmtReimbursed\", \"sum\"),\n",
        "    avg_reimbursed=(\"InscClaimAmtReimbursed\", \"mean\"),\n",
        "    pct_fraud=(\"PotentialFraud\", lambda x: round(x.mean() * 100))\n",
        ").reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:40.128407Z",
          "start_time": "2023-12-09T02:46:40.115365Z"
        },
        "id": "JphIKzUcFfSd",
        "outputId": "52a940e3-ebb0-49ea-ea16-1c4348e7b8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11604, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AttendingPhysician</th>\n",
              "      <th>count_claims</th>\n",
              "      <th>count_beneficiaries</th>\n",
              "      <th>count_providers</th>\n",
              "      <th>total_reimbursed</th>\n",
              "      <th>avg_reimbursed</th>\n",
              "      <th>pct_fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PHY311002</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3000</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PHY311023</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6000</td>\n",
              "      <td>6000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PHY311028</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11000</td>\n",
              "      <td>11000.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PHY311035</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9000</td>\n",
              "      <td>9000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PHY311056</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>57000</td>\n",
              "      <td>57000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  AttendingPhysician  count_claims  count_beneficiaries  count_providers  \\\n",
              "0          PHY311002             1                    1                1   \n",
              "1          PHY311023             1                    1                1   \n",
              "2          PHY311028             1                    1                1   \n",
              "3          PHY311035             1                    1                1   \n",
              "4          PHY311056             1                    1                1   \n",
              "\n",
              "   total_reimbursed  avg_reimbursed  pct_fraud  \n",
              "0              3000          3000.0        100  \n",
              "1              6000          6000.0          0  \n",
              "2             11000         11000.0        100  \n",
              "3              9000          9000.0          0  \n",
              "4             57000         57000.0          0  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df_physician.shape)\n",
        "df_physician.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0plOKPz8FfSd"
      },
      "source": [
        "# Create Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:42.324196Z",
          "start_time": "2023-12-09T02:46:42.318840Z"
        },
        "id": "cxZ5An-IFfSd"
      },
      "outputs": [],
      "source": [
        "df_bene[\"bid\"] = df_bene.index\n",
        "df_physician[\"pid\"] = df_physician.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:43.006903Z",
          "start_time": "2023-12-09T02:46:42.995458Z"
        },
        "id": "uN6tuxMJFfSd"
      },
      "outputs": [],
      "source": [
        "df_bene_id = df_bene[[\"BeneID\", \"bid\"]]\n",
        "df_physician_id = df_physician[[\"AttendingPhysician\", \"pid\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:44.349105Z",
          "start_time": "2023-12-09T02:46:44.288038Z"
        },
        "id": "xHeq_nrwFfSd"
      },
      "outputs": [],
      "source": [
        "df_edge_2 = df_edge.merge(\n",
        "    df_bene_id,\n",
        "    on=\"BeneID\"\n",
        ").merge(df_physician_id, on=\"AttendingPhysician\")\n",
        "df_edge_2 = df_edge_2.sort_values([\"bid\",\"pid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:45.007877Z",
          "start_time": "2023-12-09T02:46:44.996940Z"
        },
        "id": "Hux_L5ldFfSe"
      },
      "outputs": [],
      "source": [
        "bid = torch.tensor(df_edge_2[\"bid\"].to_numpy())\n",
        "pid = torch.tensor(df_edge_2[\"pid\"].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:47.141423Z",
          "start_time": "2023-12-09T02:46:47.123194Z"
        },
        "id": "F0VxyYJXFfSe"
      },
      "outputs": [],
      "source": [
        "adj = SparseTensor(row=bid, col=pid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:47.894038Z",
          "start_time": "2023-12-09T02:46:47.888652Z"
        },
        "id": "m8qNfFBfFfSe"
      },
      "outputs": [],
      "source": [
        "def standardize(features: np.ndarray) -> np.ndarray:\n",
        "    scaler = StandardScaler()\n",
        "    results = scaler.fit_transform(features)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:48.454772Z",
          "start_time": "2023-12-09T02:46:48.440471Z"
        },
        "id": "pF7QICnIFfSe",
        "outputId": "6fa750fe-63e2-470f-e868-d1225c2f1b30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BeneID</th>\n",
              "      <th>AttendingPhysician</th>\n",
              "      <th>count_claims</th>\n",
              "      <th>count_providers</th>\n",
              "      <th>total_reimbursed</th>\n",
              "      <th>avg_reimbursed</th>\n",
              "      <th>pct_fraud</th>\n",
              "      <th>bid</th>\n",
              "      <th>pid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BENE100002</td>\n",
              "      <td>PHY424317</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>10661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>BENE100004</td>\n",
              "      <td>PHY319940</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3000</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>BENE100006</td>\n",
              "      <td>PHY325217</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17000</td>\n",
              "      <td>17000.0</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>1317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>BENE100007</td>\n",
              "      <td>PHY415056</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4000</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>9801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>BENE100010</td>\n",
              "      <td>PHY403299</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        BeneID AttendingPhysician  count_claims  count_providers  \\\n",
              "0   BENE100002          PHY424317             1                1   \n",
              "46  BENE100004          PHY319940             1                1   \n",
              "49  BENE100006          PHY325217             1                1   \n",
              "66  BENE100007          PHY415056             1                1   \n",
              "77  BENE100010          PHY403299             1                1   \n",
              "\n",
              "    total_reimbursed  avg_reimbursed  pct_fraud  bid    pid  \n",
              "0              12000         12000.0        100    0  10661  \n",
              "46              3000          3000.0          0    1    788  \n",
              "49             17000         17000.0        100    2   1317  \n",
              "66              4000          4000.0        100    3   9801  \n",
              "77              8000          8000.0          0    4   8627  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_edge_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:48.959693Z",
          "start_time": "2023-12-09T02:46:48.947065Z"
        },
        "scrolled": true,
        "id": "hiZSaqPTFfSe"
      },
      "outputs": [],
      "source": [
        "# Encode attributes of edge as tensors\n",
        "edge_attr = torch.tensor(\n",
        "    standardize(\n",
        "        df_edge_2.iloc[:, 2: -2].to_numpy()\n",
        "    )\n",
        ").float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:49.635987Z",
          "start_time": "2023-12-09T02:46:49.622160Z"
        },
        "id": "RSQnkwgaFfSe"
      },
      "outputs": [],
      "source": [
        "# Encode attributes of beneficiaries as tensors\n",
        "bene_attr = torch.tensor(\n",
        "    standardize(\n",
        "        df_bene.iloc[:, 1: -1].to_numpy()\n",
        "    )\n",
        ").float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:50.226994Z",
          "start_time": "2023-12-09T02:46:50.217619Z"
        },
        "id": "TQL6zSf6FfSe"
      },
      "outputs": [],
      "source": [
        "# Encode attributes of physicians as tensors\n",
        "physician_attr = torch.tensor(\n",
        "    standardize(\n",
        "        df_physician.iloc[:, 1: -1].to_numpy()\n",
        "    )\n",
        ").float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:51.493805Z",
          "start_time": "2023-12-09T02:46:51.489198Z"
        },
        "id": "YeJppb1xFfSe"
      },
      "outputs": [],
      "source": [
        "data = BiPartiteData(adj, xu=bene_attr, xv=physician_attr, xe=edge_attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-09T02:46:52.295747Z",
          "start_time": "2023-12-09T02:46:52.287911Z"
        },
        "id": "aLcERd6yFfSe",
        "outputId": "973dfd56-0676-45df-9eb9-44da3621ef3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiPartiteData(adj=[31289, 11604, nnz=38559], xu=[31289, 6], xv=[11604, 6], xe=[38559, 5])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZb4kxHWFfSe"
      },
      "source": [
        "### Inject anomaly\n",
        "\n",
        "If name includes \"anomaly\", call inject_random_block_anomaly() to add random anomalies.\n",
        "\n",
        "The function adds anomalies such as variations in block structure,features, node/edge features and random variations in number of nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXtMCkKXFfSi"
      },
      "source": [
        "#### Inject_random_block_anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T02:57:41.515132Z",
          "start_time": "2024-01-02T02:57:41.509273Z"
        },
        "id": "_C5f-grQFfSi"
      },
      "outputs": [],
      "source": [
        "block_anomalies = [\"full_dense_block\", \"partial_full_dense_block\"]\n",
        "feature_anomalies = [\"outside_ci\", \"scaled_gaussian\", \"none\"]\n",
        "node_edge_feat_anomalies = [\"node_only\", \"edge_only\", \"node_edge\"]\n",
        "\n",
        "block_anomalies_weight = [0.2, 0.8]\n",
        "feature_anomalies_weight = [0.5, 0.4, 0.1]\n",
        "node_edge_feat_anomalies_weight = [0.1, 0.3, 0.6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T02:57:46.552829Z",
          "start_time": "2024-01-02T02:57:46.547710Z"
        },
        "id": "rpaYFC-EFfSi"
      },
      "outputs": [],
      "source": [
        "num_graph = 5\n",
        "num_group = 20\n",
        "num_nodes_range=(1, 20)\n",
        "num_nodes_range2=(1, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T02:57:51.510209Z",
          "start_time": "2024-01-02T02:57:51.505785Z"
        },
        "id": "7_Y3KxStFfSi"
      },
      "outputs": [],
      "source": [
        "data_new = BipartiteData(data.adj, xu=data.xu, xv=data.xv, xe=data.xe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T02:57:56.652834Z",
          "start_time": "2024-01-02T02:57:56.645580Z"
        },
        "id": "094tjONrFfSi",
        "outputId": "b4a26df4-f18d-465b-c07c-aa5c0ba06ef1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BipartiteData(adj=[31289, 11604, nnz=38559], xu=[31289, 6], xv=[11604, 6], xe=[38559, 5])"
            ]
          },
          "execution_count": 418,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:03:36.363851Z",
          "start_time": "2024-01-02T03:03:33.912417Z"
        },
        "id": "Lt6E7u1-FfSj",
        "outputId": "2f00fdd1-460a-4f5d-cf51-aa5a571a96dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it 0: affected: yu = 49, yv = 34, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.61, outside_ci, (13, 7), True]\n",
            "it 1: affected: yu = 61, yv = 41, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.49, outside_ci, (12, 7), True]\n",
            "it 2: affected: yu = 75, yv = 48, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.95, outside_ci, (14, 7), False]\n",
            "it 3: affected: yu = 90, yv = 55, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.61, scaled_gaussian, (15, 7), False]\n",
            "it 4: affected: yu = 103, yv = 62, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.38, outside_ci, (13, 7), False]\n",
            "it 5: affected: yu = 111, yv = 69, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.47, scaled_gaussian, (8, 7), True]\n",
            "it 6: affected: yu = 120, yv = 76, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.37, scaled_gaussian, (9, 7), True]\n",
            "it 7: affected: yu = 130, yv = 83, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.65, outside_ci, (10, 7), False]\n",
            "it 8: affected: yu = 142, yv = 90, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.58, scaled_gaussian, (12, 7), True]\n",
            "it 9: affected: yu = 148, yv = 97, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.61, scaled_gaussian, (6, 7), False]\n",
            "it 10: affected: yu = 156, yv = 104, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.67, outside_ci, (8, 7), False]\n",
            "it 11: affected: yu = 168, yv = 111, ye = (data_new.ye.sum()) [full_dense_block: 1.00, outside_ci, (12, 7), True]\n",
            "it 12: affected: yu = 178, yv = 118, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.95, outside_ci, (10, 7), True]\n",
            "it 13: affected: yu = 183, yv = 125, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.76, outside_ci, (5, 7), True]\n",
            "it 14: affected: yu = 191, yv = 132, ye = (data_new.ye.sum()) [full_dense_block: 1.00, scaled_gaussian, (8, 7), False]\n",
            "it 15: affected: yu = 202, yv = 138, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.54, scaled_gaussian, (11, 6), False]\n",
            "it 16: affected: yu = 213, yv = 143, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.59, outside_ci, (11, 5), True]\n",
            "it 17: affected: yu = 216, yv = 150, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.98, scaled_gaussian, (3, 7), True]\n",
            "it 18: affected: yu = 224, yv = 157, ye = (data_new.ye.sum()) [partial_full_dense_block: 0.52, scaled_gaussian, (8, 7), False]\n",
            "it 19: affected: yu = 229, yv = 164, ye = (data_new.ye.sum()) [full_dense_block: 1.00, outside_ci, (5, 7), True]\n"
          ]
        }
      ],
      "source": [
        "# code copied from \"inject_random_block_anomaly()\" in GraphBEAN's anomaly_insert\n",
        "\n",
        "\n",
        "for itg in range(num_group):\n",
        "    print(f\"it {itg}: \", end=\"\")\n",
        "\n",
        "    # prints 3 random floats between 0 to 1 with 4d.p.\n",
        "    rnd = torch.rand(3)\n",
        "    # Using first random float, choose between block_anomalies choices with probability = weight\n",
        "    block_an = choose(rnd[0], block_anomalies, block_anomalies_weight)\n",
        "    # choose feature anomalies using second random float, choices for feature anomalies and weights\n",
        "    feature_an = choose(rnd[1], feature_anomalies, feature_anomalies_weight)\n",
        "    # choose node_edge_feature anomalies using third random float, choices and weights\n",
        "    node_edge_an = choose(rnd[2], node_edge_feat_anomalies, node_edge_feat_anomalies_weight)\n",
        "\n",
        "    # lr = min in range, rr = max in range, mr = median in range\n",
        "    lr, rr, mr = (\n",
        "        num_nodes_range[0],\n",
        "        num_nodes_range[1],\n",
        "        num_nodes_range[0] + num_nodes_range[1] / 2,\n",
        "    )\n",
        "    if num_nodes_range2 is not None:\n",
        "        # generate random integers in range\n",
        "        nn1 = int(\n",
        "            np.minimum(\n",
        "                # select minimum between output from previous function and max_range + 1\n",
        "                np.maximum(\n",
        "                    # select max between low_range and range tensor + median_range\n",
        "                    lr,\n",
        "                    # a tensor filled with random number from normal distribution\n",
        "                    # .item() grabs item inside tensor\n",
        "                    (torch.randn(1).item() * np.sqrt(mr)) + mr\n",
        "                ),\n",
        "                rr + 1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # get node ranges from num_nodes_range2\n",
        "        lr2, rr2, mr2 = (\n",
        "            num_nodes_range2[0],\n",
        "            num_nodes_range2[1],\n",
        "            num_nodes_range2[0] + num_nodes_range2[1] / 2,\n",
        "        )\n",
        "        # generate random integers in range from num_nodes_range2\n",
        "        nn2 = int(\n",
        "            np.minimum(\n",
        "                np.maximum(\n",
        "                    lr2,\n",
        "                    (torch.randn(1).item() * np.sqrt(mr2)) + mr\n",
        "                ),\n",
        "                rr2 + 1\n",
        "            )\n",
        "        )\n",
        "        num_nodes = (nn1, nn2)\n",
        "\n",
        "        # setup kwargs\n",
        "        connected_prop = 1.0\n",
        "        if block_an == \"partial_full_dense_block\":\n",
        "            # generate random prob between 0.2 and 1.0\n",
        "            connected_prop = np.minimum(\n",
        "                np.maximum(\n",
        "                    0.2,\n",
        "                    (torch.randn(1).item() / 4) + 0.5\n",
        "                ),\n",
        "                1.0\n",
        "            )\n",
        "        # generate random prob between 0.1 to 0.9\n",
        "        prop_feat = np.minimum(\n",
        "            np.maximum(\n",
        "                0.1,\n",
        "                (torch.randn(1).item() / 8) + 0.3\n",
        "            ),\n",
        "            0.9\n",
        "        )\n",
        "        # generate random float between 2.0 to 3 + random float from std normal dist\n",
        "        std_cutoff = np.maximum(\n",
        "            2.0,\n",
        "            torch.randn(1).item() + 3.0\n",
        "        )\n",
        "        scale = np.maximum(\n",
        "            2.0,\n",
        "            torch.randn(1).item() + 3.0\n",
        "        )\n",
        "\n",
        "        # inject anomaly\n",
        "        node_feature_anomaly = None\n",
        "        if block_an != \"none\" and feature_an != \"none\":\n",
        "            node_feature_anomaly = False if node_edge_an == \"edge_only\" else True\n",
        "            edge_feature_anomaly = False if node_edge_an == \"node_only\" else True\n",
        "\n",
        "            if feature_an == \"outside_ci\":\n",
        "                data_new = inject_dense_block_and_feature_anomaly(\n",
        "                    data_new,\n",
        "                    node_feature_anomaly,\n",
        "                    edge_feature_anomaly,\n",
        "                    num_group=1,\n",
        "                    num_nodes=num_nodes,\n",
        "                    connected_prop=connected_prop,\n",
        "                    feature_anomaly_type=\"outside_ci\",\n",
        "                    prop_feat=prop_feat,\n",
        "                    std_cutoff=std_cutoff\n",
        "                )\n",
        "            elif feature_an == \"scaled_gaussian\":\n",
        "                data_new = inject_dense_block_and_feature_anomaly(\n",
        "                    data_new,\n",
        "                    node_feature_anomaly,\n",
        "                    edge_feature_anomaly,\n",
        "                    num_group=1,\n",
        "                    num_nodes=num_nodes,\n",
        "                    connected_prop=connected_prop,\n",
        "                    feature_anomaly_type=\"scaled_gaussian\",\n",
        "                    scale=scale,\n",
        "                )\n",
        "        elif block_an != \"none\" and feature_an == \"none\":\n",
        "            data_new = inject_dense_block_anomaly(\n",
        "                data_new,\n",
        "                num_group=1,\n",
        "                num_nodes=num_nodes,\n",
        "                connected_prop=connected_prop\n",
        "            )\n",
        "\n",
        "        elif block_an == \"none\" and feature_an != \"none\":\n",
        "            node_anomaly = False if node_edge_an == \"edge_only\" else True\n",
        "            edge_anomaly = False if node_edge_an == \"node_only\" else True\n",
        "\n",
        "            if feature_an == \"outside_ci\":\n",
        "                data_new = inject_feature_anomaly(\n",
        "                    data_new,\n",
        "                    node_anomaly,\n",
        "                    edge_anomaly,\n",
        "                    feature_anomaly_type=\"outside_ci\",\n",
        "                    prop_feat=prop_feat,\n",
        "                    std_cutoff=std_cutoff,\n",
        "                )\n",
        "\n",
        "            elif feature_an == \"scaled_gaussian\":\n",
        "                data_new = inject_feature_anomaly(\n",
        "                    data_new,\n",
        "                    node_anomaly,\n",
        "                    edge_anomaly,\n",
        "                    feature_anomaly_type=\"scaled_gaussian\",\n",
        "                    scale=scale,\n",
        "                )\n",
        "\n",
        "        print(\n",
        "            f\"affected: yu = {data_new.yu.sum()}, yv = {data_new.yv.sum()}, ye = (data_new.ye.sum()) \",\n",
        "            end=\"\",\n",
        "        )\n",
        "        print(\n",
        "            f\"[{block_an}: {connected_prop:.2f}, {feature_an}, {num_nodes}, {node_feature_anomaly}]\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02wovLufFfSj"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:12:10.777704Z",
          "start_time": "2024-01-02T03:12:10.771125Z"
        },
        "id": "rEsOkQeuFfSj",
        "outputId": "bdccb29e-4484-41f7-ff27-a484d5ea76f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data dimensions: U = torch.Size([31289, 6]); V = torch.Size([11604, 6]); E = torch.Size([39604, 5])\n"
          ]
        }
      ],
      "source": [
        "u_ch = data_new.xu.shape[1]\n",
        "v_ch = data_new.xv.shape[1]\n",
        "e_ch = data_new.xe.shape[1]\n",
        "\n",
        "print(f\"Data dimensions: U = {data_new.xu.shape}; V = {data_new.xv.shape}; E = {data_new.xe.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:04:07.681500Z",
          "start_time": "2024-01-02T03:04:07.676188Z"
        },
        "id": "bPoiPe97FfSj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dynamically choose computing device depending on availability of GPUs.\n",
        "Device repesents the device where the code will run.\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsiYiXxFfSj"
      },
      "source": [
        "### GraphBEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWYG42nOFfSk"
      },
      "source": [
        "## Define variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:13:45.827708Z",
          "start_time": "2024-01-02T03:13:45.822114Z"
        },
        "id": "j5_j_hBpFfSk"
      },
      "outputs": [],
      "source": [
        "in_channels = (u_ch, v_ch, e_ch)\n",
        "hidden_channels = make_tuple(32)\n",
        "latent_channels = make_tuple(32, 2)\n",
        "out_channels = make_tuple(32)\n",
        "edge_pred_latent = 32\n",
        "n_layers_encoder = 2\n",
        "n_layers_decoder = 2\n",
        "n_layers_mlp = 2\n",
        "dropout_prob = 0.0\n",
        "lr = 0.01\n",
        "scheduler_milestone = []\n",
        "gamma = 0.2\n",
        "\n",
        "node_self_loop = False\n",
        "normalize = True\n",
        "bias=True\n",
        "input_has_edge_channel = len(in_channels) == 3\n",
        "output_has_edge_channel = len(out_channels) == 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzYlxYQ6FfSk"
      },
      "source": [
        "## Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:19.981298Z",
          "start_time": "2024-01-02T03:14:19.976359Z"
        },
        "id": "lzawoVYNFfSk",
        "outputId": "7329cfb1-0d1b-4262-f524-63b18cbb5b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data dimensions: \n",
            "\tU nodes = torch.Size([31289, 6]); \n",
            "\tV nodes = torch.Size([11604, 6]); \n",
            "\tE edge = torch.Size([39604, 5])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Data dimensions: \\n\\tU nodes = {data_new.xu.shape}; \\n\\tV nodes = {data_new.xv.shape}; \\n\\tE edge = {data_new.xe.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:25.243534Z",
          "start_time": "2024-01-02T03:14:25.228687Z"
        },
        "id": "C_kQ4DnjFfSk",
        "outputId": "694a1b8c-e6f3-492c-f773-2a7facb53669"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GraphBEAN(\n",
              "  (encoder_convs): ModuleList(\n",
              "    (0): BEANConv((6, 6, 5), (32, 32, 32))\n",
              "    (1): BEANConv((32, 32, 32), (32, 32))\n",
              "  )\n",
              "  (decoder_convs): ModuleList(\n",
              "    (0): BEANConv((32, 32), (32, 32, 32))\n",
              "    (1): BEANConv((32, 32, 32), (6, 6, 5))\n",
              "  )\n",
              "  (u_mlp_layers): ModuleList(\n",
              "    (0): Linear(32, 32, bias=True)\n",
              "    (1): Linear(32, 32, bias=True)\n",
              "  )\n",
              "  (v_mlp_layers): ModuleList(\n",
              "    (0): Linear(32, 32, bias=True)\n",
              "    (1): Linear(32, 32, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 511,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GraphBEAN(\n",
        "    in_channels=in_channels,\n",
        "    hidden_channels=hidden_channels,\n",
        "    latent_channels=latent_channels,\n",
        "    edge_pred_latent=edge_pred_latent,\n",
        "    n_layers_encoder=n_layers_encoder,\n",
        "    n_layers_decoder=n_layers_decoder,\n",
        "    n_layers_mlp=n_layers_mlp,\n",
        "    dropout_prob=dropout_prob\n",
        ")\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:30.151873Z",
          "start_time": "2024-01-02T03:14:30.147630Z"
        },
        "id": "m2ChoTDZFfSl"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:35.550317Z",
          "start_time": "2024-01-02T03:14:35.544103Z"
        },
        "id": "lP06-fYlFfSl"
      },
      "outputs": [],
      "source": [
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer, milestones=scheduler_milestone, gamma=gamma\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:40.410497Z",
          "start_time": "2024-01-02T03:14:40.402974Z"
        },
        "id": "3herFxy_FfSl"
      },
      "outputs": [],
      "source": [
        "xu, xv = data_new.xu.to(device), data_new.xv.to(device)\n",
        "xe, adj = data_new.xe.to(device), data_new.adj.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRYc-h6jFfSl"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:45.024484Z",
          "start_time": "2024-01-02T03:14:45.017909Z"
        },
        "scrolled": true,
        "id": "I0G5u96DFfSl",
        "outputId": "6bbbc2c9-63c2-44de-d4db-b2b79133d764"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GraphBEAN(\n",
              "  (encoder_convs): ModuleList(\n",
              "    (0): BEANConv((6, 6, 5), (32, 32, 32))\n",
              "    (1): BEANConv((32, 32, 32), (32, 32))\n",
              "  )\n",
              "  (decoder_convs): ModuleList(\n",
              "    (0): BEANConv((32, 32), (32, 32, 32))\n",
              "    (1): BEANConv((32, 32, 32), (6, 6, 5))\n",
              "  )\n",
              "  (u_mlp_layers): ModuleList(\n",
              "    (0): Linear(32, 32, bias=True)\n",
              "    (1): Linear(32, 32, bias=True)\n",
              "  )\n",
              "  (v_mlp_layers): ModuleList(\n",
              "    (0): Linear(32, 32, bias=True)\n",
              "    (1): Linear(32, 32, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 515,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1x1CjfjFfSl"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpg42eocFfSl"
      },
      "source": [
        "EdgePredictionSampler class is used to generate samples for edge prediction tasks in a graph for negative sampling. It creates a set of negative samples by randomly selecting edges that don't exist in input adjacency matrix.\n",
        "\n",
        "The purpose is to **generate negative samples** for edge prediction tasks, e.g. those used in GCN where positive samples are existing edges and negative samples are randomly sampled non-existing edges. **The generated samples are used as negative examples during training (to handle class imbalance that can lead to biases, slow convergence and high computational costs).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT1RKu9xFfSl"
      },
      "source": [
        "#### Sprand method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:50.399204Z",
          "start_time": "2024-01-02T03:14:50.394585Z"
        },
        "id": "nle2wWtbFfSl"
      },
      "outputs": [],
      "source": [
        "# Get size of adj (adjacency matrix)\n",
        "nu, nv = adj.sparse_sizes()\n",
        "\n",
        "# random samples to generate = multiple of # of positive samples in adjacency matrix\n",
        "n_random = 2 * adj.nnz()\n",
        "\n",
        "row = torch.randint(nu, (n_random,))\n",
        "col = torch.randint(nv, (n_random,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:14:55.107433Z",
          "start_time": "2024-01-02T03:14:55.084109Z"
        },
        "id": "erOsIQkPFfSl"
      },
      "outputs": [],
      "source": [
        "storage = SparseStorage(row=row, col=col, sparse_sizes=(nu, nv))\n",
        "storage = storage.coalesce(reduce=\"max\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:00.095256Z",
          "start_time": "2024-01-02T03:15:00.088732Z"
        },
        "id": "j9cNzrPVFfSl"
      },
      "outputs": [],
      "source": [
        "# Generate negative examples using sparse random values and sets values to -1\n",
        "rnd_samples = SparseTensor.from_storage(storage)\n",
        "rnd_samples = rnd_samples.fill_value(-1)\n",
        "rnd_samples = rnd_samples.to(adj.device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:04.736570Z",
          "start_time": "2024-01-02T03:15:04.728541Z"
        },
        "id": "XzFad_6OFfSl",
        "outputId": "e97b3592-0d2e-4d19-b157-6dc01bde5074"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([    0,     2,     2,  ..., 31288, 31288, 31288]),\n",
              "             col=tensor([ 2975,  6152,  8638,  ...,   903,  5551, 10455]),\n",
              "             val=tensor([-1, -1, -1,  ..., -1, -1, -1]),\n",
              "             size=(31289, 11604), nnz=79202, density=0.02%)"
            ]
          },
          "execution_count": 519,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnd_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:09.920828Z",
          "start_time": "2024-01-02T03:15:09.915543Z"
        },
        "id": "W53udBt4FfSl"
      },
      "outputs": [],
      "source": [
        "# fill adjacency matrix with value of 2 as positive samples (assumed value for positive samples)\n",
        "pos_samples = adj.fill_value(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:14.739175Z",
          "start_time": "2024-01-02T03:15:14.730843Z"
        },
        "scrolled": true,
        "id": "V1u-g7N4FfSl",
        "outputId": "3cf8e68a-4a1d-4afc-aae5-520ac7dc9618"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([    0,     1,     2,  ..., 31286, 31287, 31288]),\n",
              "             col=tensor([10661,   788,  1317,  ..., 10271,  9699,  9519]),\n",
              "             val=tensor([2, 2, 2,  ..., 2, 2, 2]),\n",
              "             size=(31289, 11604), nnz=39604, density=0.01%)"
            ]
          },
          "execution_count": 521,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMdemyQkFfSl"
      },
      "source": [
        "#### Spadd Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r33bJRiMFfSl"
      },
      "source": [
        "This function is used to combine random and positive samples by concatenating their row, column and value tensors to create a new sparse tensor.\n",
        "\n",
        "Then, the values of the combined samples are set to the minimum of {current values, 1}.\n",
        "\n",
        "So, resulting tensor contains negative samples with -1 values and positive samples with 1 values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:19.849504Z",
          "start_time": "2024-01-02T03:15:19.845860Z"
        },
        "id": "fFoqRgRfFfSl"
      },
      "outputs": [],
      "source": [
        "assert rnd_samples.sparse_sizes() == pos_samples.sparse_sizes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:25.208659Z",
          "start_time": "2024-01-02T03:15:25.192780Z"
        },
        "id": "MArC_RcoFfSl",
        "outputId": "bea75a59-c205-4172-a38d-abdb30105008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31289 11604\n"
          ]
        }
      ],
      "source": [
        "m, n = rnd_samples.sparse_sizes()\n",
        "print(m, n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:30.789504Z",
          "start_time": "2024-01-02T03:15:30.780628Z"
        },
        "id": "tlZkDNUuFfSl",
        "outputId": "66c05531-5f2f-4fd8-b3c0-2a5554722c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([    0,     2,     2,  ..., 31286, 31287, 31288]) tensor([ 2975,  6152,  8638,  ..., 10271,  9699,  9519]) tensor([-1, -1, -1,  ...,  2,  2,  2])\n"
          ]
        }
      ],
      "source": [
        "# Concatenate sequence of tensors\n",
        "row = torch.cat([rnd_samples.storage.row(), pos_samples.storage.row()], dim=-1)\n",
        "col = torch.cat([rnd_samples.storage.col(), pos_samples.storage.col()], dim=-1)\n",
        "value = torch.cat([rnd_samples.storage.value(), pos_samples.storage.value()], dim=-1)\n",
        "\n",
        "print(row, col, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:35.797565Z",
          "start_time": "2024-01-02T03:15:35.772259Z"
        },
        "id": "6Sf719jWFfSl"
      },
      "outputs": [],
      "source": [
        "storage = SparseStorage(row=row, col=col, value=value, sparse_sizes=(m, n))\n",
        "storage = storage.coalesce(reduce=\"add\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:40.525367Z",
          "start_time": "2024-01-02T03:15:40.520629Z"
        },
        "id": "zK2Rfd9DFfSl"
      },
      "outputs": [],
      "source": [
        "samples = SparseTensor.from_storage(storage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:45.355124Z",
          "start_time": "2024-01-02T03:15:45.346628Z"
        },
        "id": "FYu0HGHJFfSl",
        "outputId": "cfd631d0-0bd5-459d-e45e-7c88b96c924e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([    0,     0,     1,  ..., 31288, 31288, 31288]),\n",
              "             col=tensor([ 2975, 10661,   788,  ...,  5551,  9519, 10455]),\n",
              "             val=tensor([-1,  2,  2,  ..., -1,  2, -1]),\n",
              "             size=(31289, 11604), nnz=118796, density=0.03%)"
            ]
          },
          "execution_count": 527,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrNltBvCFfSm"
      },
      "source": [
        "#### Set value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:50.312719Z",
          "start_time": "2024-01-02T03:15:50.306497Z"
        },
        "id": "c_W5h2kgFfSm",
        "outputId": "59d162c0-cd2f-40d1-b7d6-9073b49fa80f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1,  2,  2,  ..., -1,  2, -1])"
            ]
          },
          "execution_count": 528,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "samples.storage.value()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:15:55.403305Z",
          "start_time": "2024-01-02T03:15:55.395110Z"
        },
        "id": "EjsdQ6ckFfSm",
        "outputId": "c68c0225-ad32-4008-c983-477f39df10e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1,  ..., 1, 1, 1])"
            ]
          },
          "execution_count": 529,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ones_like(samples.storage.value())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:00.566615Z",
          "start_time": "2024-01-02T03:16:00.557536Z"
        },
        "id": "BlRm7-p7FfSm",
        "outputId": "51c60d07-38de-4b69-ae16-bb4446fab336"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1,  1,  1,  ..., -1,  1, -1])"
            ]
          },
          "execution_count": 530,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.minimum(samples.storage.value(), torch.ones_like(samples.storage.value()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:05.625489Z",
          "start_time": "2024-01-02T03:16:05.620793Z"
        },
        "id": "4YzXWi1bFfSm"
      },
      "outputs": [],
      "source": [
        "edge_pred_samples = samples.set_value_(\n",
        "    # Take the min of tensor values from samples or 1\n",
        "    torch.minimum(samples.storage.value(), torch.ones_like(samples.storage.value())),\n",
        "    layout=\"coo\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:10.511073Z",
          "start_time": "2024-01-02T03:16:10.502814Z"
        },
        "id": "QPTwgClOFfSm",
        "outputId": "c97b6f41-9042-4198-9733-1577cec5b48f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([    0,     0,     1,  ..., 31288, 31288, 31288]),\n",
              "             col=tensor([ 2975, 10661,   788,  ...,  5551,  9519, 10455]),\n",
              "             val=tensor([-1,  1,  1,  ..., -1,  1, -1]),\n",
              "             size=(31289, 11604), nnz=118796, density=0.03%)"
            ]
          },
          "execution_count": 532,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Is edge_pred_samples just a sampled/ randomized version of adj?\n",
        "edge_pred_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:15.149651Z",
          "start_time": "2024-01-02T03:16:15.142595Z"
        },
        "id": "fLqEt2nZFfSm",
        "outputId": "e78dc2b7-fd83-49d4-ff64-cc5221a7894d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([    0,     1,     2,  ..., 31286, 31287, 31288]),\n",
              "             col=tensor([10661,   788,  1317,  ..., 10271,  9699,  9519]),\n",
              "             size=(31289, 11604), nnz=39604, density=0.01%)"
            ]
          },
          "execution_count": 533,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiTuSMiYFfSm"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:20.174490Z",
          "start_time": "2024-01-02T03:16:20.171450Z"
        },
        "id": "LuOBIIKhFfSm"
      },
      "outputs": [],
      "source": [
        "# Set gradients of all optimized tensors in optimizer to 0\n",
        "optimzer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_X4BErFfSm"
      },
      "source": [
        "### Model Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:25.374109Z",
          "start_time": "2024-01-02T03:16:25.017961Z"
        },
        "scrolled": false,
        "id": "TFSgaMgjFfSm",
        "outputId": "fd3e18b4-c26c-4249-f000-970b4770b68b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'xu': tensor([[-1.1299, -0.5371,  1.3822, -0.0695, -0.8118, -0.2574],\n",
              "         [ 0.3705,  0.2595, -0.6070,  0.7119,  0.0336, -0.0526],\n",
              "         [-0.4676,  0.0101,  0.5994,  0.1599,  0.0699, -0.2593],\n",
              "         ...,\n",
              "         [ 0.3586,  0.0052,  0.3125,  0.4646,  0.3878, -0.2834],\n",
              "         [-0.4865, -0.0712,  1.5861, -0.1915,  0.6385,  0.5243],\n",
              "         [-1.1713, -1.2177,  1.6912, -0.6179, -0.4070, -0.8524]],\n",
              "        grad_fn=<NativeBatchNormBackward0>),\n",
              " 'xv': tensor([[-0.0701,  0.6556,  0.3961,  0.0722, -1.1396,  0.9028],\n",
              "         [ 0.5342, -0.3572,  0.3545,  0.4283, -0.4329,  0.7224],\n",
              "         [ 0.4972,  0.0519,  0.4546,  0.5205, -0.8659,  0.6476],\n",
              "         ...,\n",
              "         [ 0.2775, -0.0249,  0.1434,  0.0439,  0.8636,  0.5784],\n",
              "         [ 0.0469,  0.4745,  0.3947,  0.2023, -1.0996,  0.8779],\n",
              "         [ 0.6132, -0.3767,  0.7363,  0.9775,  0.9059,  0.0291]],\n",
              "        grad_fn=<NativeBatchNormBackward0>),\n",
              " 'xe': tensor([[ 1.0431,  0.4862,  0.7719, -0.9827, -0.3857],\n",
              "         [ 0.6909, -0.6962, -0.6540,  0.5339, -0.1131],\n",
              "         [ 0.2729,  0.9005, -0.2704, -0.6216,  0.2529],\n",
              "         ...,\n",
              "         [ 0.3302, -0.3644, -0.3221,  0.1868, -0.3034],\n",
              "         [-0.3030,  0.0166,  0.1801, -0.4412,  0.2101],\n",
              "         [ 0.7225, -0.0609,  0.9434, -0.8823, -1.1011]],\n",
              "        grad_fn=<NativeBatchNormBackward0>),\n",
              " 'zu': tensor([[ 0.2785,  0.6937,  0.3449,  ...,  0.1574,  0.7150,  0.3660],\n",
              "         [ 0.9609, -0.1650, -0.4897,  ...,  0.7967, -0.9050,  0.1494],\n",
              "         [ 0.3289, -0.5539, -0.3457,  ...,  0.1963,  0.0209, -0.5100],\n",
              "         ...,\n",
              "         [ 0.1735, -0.0415,  0.2599,  ...,  0.5315,  0.0885, -0.4242],\n",
              "         [ 0.0646, -0.0887, -0.2249,  ..., -0.0678,  0.9296,  0.1419],\n",
              "         [-0.0527, -0.1807,  0.3212,  ..., -0.3352,  0.9514,  0.8383]],\n",
              "        grad_fn=<NativeBatchNormBackward0>),\n",
              " 'zv': tensor([[ 0.3301, -0.0214,  0.4917,  ..., -0.6551, -0.7103,  0.8489],\n",
              "         [-0.1165, -0.1512,  0.4763,  ..., -0.4344, -0.5469,  0.0799],\n",
              "         [-0.3096,  0.0629,  0.5041,  ..., -0.6048, -0.2504,  0.6665],\n",
              "         ...,\n",
              "         [ 0.2773, -0.3652,  0.7741,  ...,  0.7412,  0.0965,  0.6562],\n",
              "         [ 0.1724,  0.0226,  0.5025,  ..., -0.6096, -0.5960,  0.8124],\n",
              "         [-0.8517, -0.0113,  0.2729,  ..., -0.8428,  0.5119, -0.2216]],\n",
              "        grad_fn=<NativeBatchNormBackward0>),\n",
              " 'eprob': tensor([0.5428, 0.6193, 0.4455,  ..., 0.5217, 0.6297, 0.5645],\n",
              "        grad_fn=<SigmoidBackward0>)}"
            ]
          },
          "execution_count": 535,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = model(xu, xv, xe, adj, edge_pred_samples)\n",
        "\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIELxp0LFfSm"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:30.685346Z",
          "start_time": "2024-01-02T03:16:30.679554Z"
        },
        "id": "DD0pMTgLFfSm"
      },
      "outputs": [],
      "source": [
        "xe_loss_weight = 1\n",
        "structure_loss_weight = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:35.576655Z",
          "start_time": "2024-01-02T03:16:35.561866Z"
        },
        "id": "Ispg5VJlFfSm",
        "outputId": "cebd5132-60c7-4bc1-c74a-0af5f06c718f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6.1822, grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Feature MSE\n",
        "xu_loss = nn.functional.mse_loss(xu, out[\"xu\"])\n",
        "xv_loss = nn.functional.mse_loss(xv, out[\"xv\"])\n",
        "xe_loss = nn.functional.mse_loss(xe, out[\"xe\"])\n",
        "\n",
        "feature_loss = xu_loss + xv_loss + xe_loss_weight * xe_loss\n",
        "\n",
        "print(feature_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:40.590627Z",
          "start_time": "2024-01-02T03:16:40.577119Z"
        },
        "id": "KyvU-cyLFfSm",
        "outputId": "9ba4bc8c-8880-4352-ab6f-cc5831cc2358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Structure Loss\n",
        "edge_gt = (edge_pred_samples.storage.value() > 0).float()\n",
        "structure_loss = nn.functional.binary_cross_entropy(out[\"eprob\"], edge_gt)\n",
        "\n",
        "print(structure_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:45.953091Z",
          "start_time": "2024-01-02T03:16:45.948024Z"
        },
        "id": "tmS7ojy6FfSm",
        "outputId": "4e199060-f3d2-42b9-d06b-288ad1ebd842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6.8822, grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss = feature_loss + structure_loss_weight * structure_loss\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:51.198855Z",
          "start_time": "2024-01-02T03:16:51.190066Z"
        },
        "id": "GVvNiwxOFfSm",
        "outputId": "e11cc320-74fe-45f8-e8be-5020eb2a1773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'xu': tensor(1.9850, grad_fn=<MseLossBackward0>), 'xv': tensor(2.0911, grad_fn=<MseLossBackward0>), 'xe': tensor(2.1060, grad_fn=<MseLossBackward0>), 'e': tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward0>), 'total': tensor(6.8822, grad_fn=<AddBackward0>)}\n"
          ]
        }
      ],
      "source": [
        "loss_component = {\n",
        "    \"xu\": xu_loss,\n",
        "    \"xv\": xv_loss,\n",
        "    \"xe\": xe_loss,\n",
        "    \"e\": structure_loss,\n",
        "    \"total\": loss,\n",
        "}\n",
        "\n",
        "print(loss_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcblvtCsFfSm"
      },
      "source": [
        "### Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:16:57.040877Z",
          "start_time": "2024-01-02T03:16:56.514982Z"
        },
        "id": "i5EMlVHYFfSm"
      },
      "outputs": [],
      "source": [
        "# Initiate backpropagation in PyTorch to compute gradients with respect to model parameters\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xe3Esa2FfSm"
      },
      "source": [
        "### Update Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:17:02.751745Z",
          "start_time": "2024-01-02T03:17:02.743992Z"
        },
        "id": "isMc_yTBFfSm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Call optimzer's step to perform a parameter update to improve model performance.\n",
        "Optimizer (e.g. stochastic gradient descent, Adam etc.) uses computed gradients to adjust model parameters\n",
        "in the direction that minimizes loss.\n",
        "\"\"\"\n",
        "optimzer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:17:07.508128Z",
          "start_time": "2024-01-02T03:17:07.502737Z"
        },
        "id": "SbX_3U6RFfSn",
        "outputId": "1b03ebad-df67-4360-96f6-821477bfeed6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/clau/Documents/Python/DS_Portfolio/venv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Adjust learning rate during training according to the predefined schedule.\n",
        "Learning rate schedulers are useful for improving training stability and convergence.\n",
        "\"\"\"\n",
        "scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5BFB1xNFfSn"
      },
      "source": [
        "### Edge_prediction_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:17:12.524037Z",
          "start_time": "2024-01-02T03:17:12.516996Z"
        },
        "id": "YCjbAuB7FfSn",
        "outputId": "3e5eb242-adb5-4f22-807e-76fabf25ecb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 0 ... 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "edge_pred = (out[\"eprob\"] >= 0.5).int().cpu().numpy()\n",
        "\n",
        "print(edge_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:17:17.274667Z",
          "start_time": "2024-01-02T03:17:17.268256Z"
        },
        "id": "MAWicpyCFfSn",
        "outputId": "cb1617f9-fdc4-4cce-ab1b-7035094f6260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 1 ... 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "edge_gt = (edge_pred_samples.storage.value() > 0).int().cpu().numpy()\n",
        "\n",
        "print(edge_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:17:17.995325Z",
          "start_time": "2024-01-02T03:17:17.880437Z"
        },
        "id": "U8BE2mG1FfSn"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(edge_gt, edge_pred)\n",
        "prec = precision_score(edge_gt, edge_pred)\n",
        "rec = recall_score(edge_gt, edge_pred)\n",
        "f1 = f1_score(edge_gt, edge_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:17:27.440193Z",
          "start_time": "2024-01-02T03:17:27.431453Z"
        },
        "id": "XTOVIX3mFfSn",
        "outputId": "1b5e2274-9e9f-4706-f9c8-10b167dc88b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'acc': 0.5030977473989023, 'prec': 0.3364649627908543, 'rec': 0.5045954954045045, 'f1': 0.4037253277843996}\n"
          ]
        }
      ],
      "source": [
        "result = {\n",
        "    \"acc\": acc,\n",
        "    \"prec\": prec,\n",
        "    \"rec\": rec,\n",
        "    \"f1\": f1\n",
        "}\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz3srLnxFfSn"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:21:03.151452Z",
          "start_time": "2024-01-02T03:21:03.139302Z"
        },
        "id": "IqeunSmQFfSn",
        "outputId": "9214b22e-7af6-489b-ff6a-bbd2a8c0096e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yu: torch.Size([31289]), yv: torch.Size([11604]), ye: torch.Size([39604])\n"
          ]
        }
      ],
      "source": [
        "yu = data_new.yu\n",
        "yv = data_new.yv\n",
        "ye = data_new.ye\n",
        "\n",
        "print(f\"yu: {yu.shape}, yv: {yv.shape}, ye: {ye.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:23:27.221953Z",
          "start_time": "2024-01-02T03:23:26.784019Z"
        },
        "id": "pd-6GRrbFfSn"
      },
      "outputs": [],
      "source": [
        "# Disable gradient descent\n",
        "with torch.no_grad():\n",
        "    out = model(xu, xv, xe, adj, edge_pred_samples)\n",
        "\n",
        "    # Calculate loss\n",
        "    xu_loss = nn.functional.mse_loss(xu, out[\"xu\"])\n",
        "    xv_loss = nn.functional.mse_loss(xv, out[\"xv\"])\n",
        "    xe_loss = nn.functional.mse_loss(xe, out[\"xe\"])\n",
        "\n",
        "    feature_loss = xu_loss + xv_loss + xe_loss_weight * xe_loss\n",
        "\n",
        "    edge_gt = (edge_pred_samples.storage.value() > 0).float()\n",
        "    structure_loss = nn.functional.binary_cross_entropy(out[\"eprob\"], edge_gt)\n",
        "\n",
        "    loss = feature_loss + structure_loss_weight * structure_loss\n",
        "\n",
        "    loss_component = {\n",
        "        \"xu\": xu_loss,\n",
        "        \"xv\": xv_loss,\n",
        "        \"xe\": xe_loss,\n",
        "        \"e\": structure_loss,\n",
        "        \"total\": loss,\n",
        "    }\n",
        "\n",
        "    # edge prediciton metric\n",
        "    edge_pred = (out[\"eprob\"] >= 0.5).int().cpu().numpy()\n",
        "    edge_gt = (edge_pred_samples.storage.value() > 0).int().cpu().numpy()\n",
        "\n",
        "    acc = accuracy_score(edge_gt, edge_pred)\n",
        "    prec = precision_score(edge_gt, edge_pred)\n",
        "    rec = recall_score(edge_gt, edge_pred)\n",
        "    f1 = f1_score(edge_gt, edge_pred)\n",
        "\n",
        "    epred_metric = {\n",
        "        \"acc\": acc,\n",
        "        \"prec\": prec,\n",
        "        \"rec\": rec,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "    anomaly_score = compute_anomaly_score(\n",
        "        xu,\n",
        "        xv,\n",
        "        xe,\n",
        "        adj,\n",
        "        edge_pred_samples,\n",
        "        out,\n",
        "        xe_loss_weight,\n",
        "        structure_loss_weight,\n",
        "    )\n",
        "    eval_metrics = compute_evaluation_metrics(\n",
        "        anomaly_score, yu, yv, ye, agg=\"max\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:25:47.774681Z",
          "start_time": "2024-01-02T03:25:47.769634Z"
        },
        "id": "_MJdJQUwFfSn",
        "outputId": "a6041e81-b5fa-49fe-fc18-25d7ab44216c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval, loss: 6.8822,  u auc-roc: 0.9499, v auc-roc: 0.9681, e auc-roc: 0.9331,u auc-pr: 0.0829, v auc-pr: 0.1910, e auc-pr: 0.2238\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f\"Eval, loss: {loss:.4f}, \",\n",
        "    f\"u auc-roc: {eval_metrics['u_roc_auc']:.4f}, v auc-roc: {eval_metrics['v_roc_auc']:.4f}, e auc-roc: {eval_metrics['e_roc_auc']:.4f},\"\n",
        "    f\"u auc-pr: {eval_metrics['u_pr_auc']:.4f}, v auc-pr: {eval_metrics['v_pr_auc']:.4f}, e auc-pr: {eval_metrics['e_pr_auc']:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:28:09.285775Z",
          "start_time": "2024-01-02T03:28:09.147138Z"
        },
        "scrolled": true,
        "id": "wNKCfFlmFfSn",
        "outputId": "98c0e268-8e77-46bf-efa0-7f09676ff06a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'loss': tensor(6.8822),\n",
              " 'loss_component': {'xu': tensor(1.9850),\n",
              "  'xv': tensor(2.0911),\n",
              "  'xe': tensor(2.1060),\n",
              "  'e': tensor(0.7000),\n",
              "  'total': tensor(6.8822)},\n",
              " 'epred_metric': {'acc': 0.5030977473989023,\n",
              "  'prec': 0.3364649627908543,\n",
              "  'rec': 0.5045954954045045,\n",
              "  'f1': 0.4037253277843996},\n",
              " 'eval_metrics': {'u_roc_curve': (array([0.00000000e+00, 3.21957502e-05, 2.57566001e-04, ...,\n",
              "          9.99871217e-01, 9.99967804e-01, 1.00000000e+00]),\n",
              "   array([0., 0., 0., ..., 1., 1., 1.]),\n",
              "   array([68.040634 , 67.040634 , 21.329073 , ...,  1.2298568,  1.2289169,\n",
              "           1.2007246], dtype=float32)),\n",
              "  'u_pr_curve': (array([0.02964785, 0.02952221, 0.02952985, ..., 0.        , 0.        ,\n",
              "          1.        ]),\n",
              "   array([1.        , 0.99563319, 0.99563319, ..., 0.        , 0.        ,\n",
              "          0.        ]),\n",
              "   array([ 3.1530578,  3.1530614,  3.1540833, ..., 25.04745  , 66.25704  ,\n",
              "          67.040634 ], dtype=float32)),\n",
              "  'u_roc_auc': 0.949856595348628,\n",
              "  'u_pr_auc': 0.08292987170270778,\n",
              "  'v_roc_curve': (array([0.00000000e+00, 8.74125874e-05, 1.31118881e-03, 1.31118881e-03,\n",
              "          2.09790210e-03, 2.09790210e-03, 3.32167832e-03, 3.32167832e-03,\n",
              "          3.67132867e-03, 3.67132867e-03, 4.02097902e-03, 4.02097902e-03,\n",
              "          5.24475524e-03, 5.24475524e-03, 5.50699301e-03, 5.50699301e-03,\n",
              "          5.68181818e-03, 5.68181818e-03, 6.11888112e-03, 6.11888112e-03,\n",
              "          6.64335664e-03, 6.64335664e-03, 6.73076923e-03, 6.73076923e-03,\n",
              "          7.16783217e-03, 7.16783217e-03, 7.60489510e-03, 7.60489510e-03,\n",
              "          7.86713287e-03, 7.86713287e-03, 8.21678322e-03, 8.21678322e-03,\n",
              "          8.56643357e-03, 8.56643357e-03, 9.17832168e-03, 9.17832168e-03,\n",
              "          9.87762238e-03, 9.87762238e-03, 1.02272727e-02, 1.02272727e-02,\n",
              "          1.05769231e-02, 1.05769231e-02, 1.07517483e-02, 1.07517483e-02,\n",
              "          1.08391608e-02, 1.08391608e-02, 1.10139860e-02, 1.10139860e-02,\n",
              "          1.18006993e-02, 1.18006993e-02, 1.19755245e-02, 1.19755245e-02,\n",
              "          1.20629371e-02, 1.20629371e-02, 1.21503497e-02, 1.21503497e-02,\n",
              "          1.22377622e-02, 1.22377622e-02, 1.23251748e-02, 1.23251748e-02,\n",
              "          1.25000000e-02, 1.25000000e-02, 1.28496503e-02, 1.28496503e-02,\n",
              "          1.29370629e-02, 1.29370629e-02, 1.30244755e-02, 1.30244755e-02,\n",
              "          1.33741259e-02, 1.33741259e-02, 1.38986014e-02, 1.38986014e-02,\n",
              "          1.45104895e-02, 1.45104895e-02, 1.45979021e-02, 1.45979021e-02,\n",
              "          1.46853147e-02, 1.46853147e-02, 1.47727273e-02, 1.47727273e-02,\n",
              "          1.58216783e-02, 1.58216783e-02, 1.59965035e-02, 1.59965035e-02,\n",
              "          1.74825175e-02, 1.74825175e-02, 1.76573427e-02, 1.76573427e-02,\n",
              "          1.79195804e-02, 1.79195804e-02, 1.80944056e-02, 1.80944056e-02,\n",
              "          1.81818182e-02, 1.81818182e-02, 1.87937063e-02, 1.87937063e-02,\n",
              "          1.90559441e-02, 1.90559441e-02, 1.94055944e-02, 1.94055944e-02,\n",
              "          1.95804196e-02, 1.95804196e-02, 1.96678322e-02, 1.96678322e-02,\n",
              "          1.98426573e-02, 1.98426573e-02, 2.14160839e-02, 2.14160839e-02,\n",
              "          2.16783217e-02, 2.16783217e-02, 2.17657343e-02, 2.17657343e-02,\n",
              "          2.36013986e-02, 2.36013986e-02, 2.38636364e-02, 2.38636364e-02,\n",
              "          2.42132867e-02, 2.42132867e-02, 2.45629371e-02, 2.45629371e-02,\n",
              "          2.57867133e-02, 2.57867133e-02, 2.58741259e-02, 2.58741259e-02,\n",
              "          2.60489510e-02, 2.60489510e-02, 2.62237762e-02, 2.62237762e-02,\n",
              "          2.65734266e-02, 2.65734266e-02, 2.68356643e-02, 2.68356643e-02,\n",
              "          2.70979021e-02, 2.70979021e-02, 2.71853147e-02, 2.71853147e-02,\n",
              "          2.77972028e-02, 2.77972028e-02, 2.84090909e-02, 2.84090909e-02,\n",
              "          2.96328671e-02, 2.96328671e-02, 2.98076923e-02, 2.98076923e-02,\n",
              "          2.99825175e-02, 2.99825175e-02, 3.01573427e-02, 3.01573427e-02,\n",
              "          3.03321678e-02, 3.03321678e-02, 3.08566434e-02, 3.08566434e-02,\n",
              "          3.09440559e-02, 3.09440559e-02, 3.23426573e-02, 3.23426573e-02,\n",
              "          3.28671329e-02, 3.28671329e-02, 3.31293706e-02, 3.31293706e-02,\n",
              "          3.34790210e-02, 3.36538462e-02, 3.36538462e-02, 3.42657343e-02,\n",
              "          3.42657343e-02, 3.43531469e-02, 3.43531469e-02, 3.45279720e-02,\n",
              "          3.45279720e-02, 3.49650350e-02, 3.49650350e-02, 3.54020979e-02,\n",
              "          3.54020979e-02, 3.58391608e-02, 3.58391608e-02, 3.59265734e-02,\n",
              "          3.59265734e-02, 3.60139860e-02, 3.60139860e-02, 3.64510490e-02,\n",
              "          3.64510490e-02, 3.78496503e-02, 3.78496503e-02, 3.81118881e-02,\n",
              "          3.81118881e-02, 3.83741259e-02, 3.83741259e-02, 3.88111888e-02,\n",
              "          3.88111888e-02, 4.01223776e-02, 4.01223776e-02, 4.12587413e-02,\n",
              "          4.12587413e-02, 4.17832168e-02, 4.17832168e-02, 4.26573427e-02,\n",
              "          4.26573427e-02, 4.67657343e-02, 4.67657343e-02, 4.68531469e-02,\n",
              "          4.68531469e-02, 4.71153846e-02, 4.71153846e-02, 4.74650350e-02,\n",
              "          4.74650350e-02, 4.89510490e-02, 4.89510490e-02, 4.98251748e-02,\n",
              "          5.00000000e-02, 5.13986014e-02, 5.32342657e-02, 5.33216783e-02,\n",
              "          5.33216783e-02, 5.34090909e-02, 5.34090909e-02, 5.48076923e-02,\n",
              "          5.48076923e-02, 5.62937063e-02, 5.62937063e-02, 5.66433566e-02,\n",
              "          5.66433566e-02, 5.69055944e-02, 5.69055944e-02, 5.70804196e-02,\n",
              "          5.76923077e-02, 5.76923077e-02, 5.79545455e-02, 5.79545455e-02,\n",
              "          5.92657343e-02, 5.92657343e-02, 5.93531469e-02, 5.93531469e-02,\n",
              "          6.19755245e-02, 6.19755245e-02, 6.34615385e-02, 6.34615385e-02,\n",
              "          6.44230769e-02, 6.44230769e-02, 6.50349650e-02, 6.50349650e-02,\n",
              "          6.52097902e-02, 6.55594406e-02, 6.58216783e-02, 6.58216783e-02,\n",
              "          6.65209790e-02, 6.65209790e-02, 6.71328671e-02, 6.71328671e-02,\n",
              "          6.76573427e-02, 6.78321678e-02, 6.92307692e-02, 6.92307692e-02,\n",
              "          7.00174825e-02, 7.00174825e-02, 7.12412587e-02, 7.14160839e-02,\n",
              "          7.15034965e-02, 7.15034965e-02, 7.25524476e-02, 7.25524476e-02,\n",
              "          7.51748252e-02, 7.72727273e-02, 7.74475524e-02, 7.74475524e-02,\n",
              "          7.88461538e-02, 7.88461538e-02, 8.02447552e-02, 8.04195804e-02,\n",
              "          8.05069930e-02, 8.06818182e-02, 8.24300699e-02, 8.24300699e-02,\n",
              "          8.45279720e-02, 8.47027972e-02, 8.66258741e-02, 8.70629371e-02,\n",
              "          9.15209790e-02, 9.15209790e-02, 9.48426573e-02, 9.51048951e-02,\n",
              "          9.56293706e-02, 9.58041958e-02, 9.58916084e-02, 9.60664336e-02,\n",
              "          9.94755245e-02, 9.96503497e-02, 1.00262238e-01, 1.00437063e-01,\n",
              "          1.02709790e-01, 1.02709790e-01, 1.03059441e-01, 1.03234266e-01,\n",
              "          1.07167832e-01, 1.07167832e-01, 1.12674825e-01, 1.12937063e-01,\n",
              "          1.13024476e-01, 1.13374126e-01, 1.16083916e-01, 1.16258741e-01,\n",
              "          1.16783217e-01, 1.16783217e-01, 1.17395105e-01, 1.17395105e-01,\n",
              "          1.17919580e-01, 1.18356643e-01, 1.23688811e-01, 1.23951049e-01,\n",
              "          1.24475524e-01, 1.29632867e-01, 1.30069930e-01, 1.36713287e-01,\n",
              "          1.36888112e-01, 1.37237762e-01, 1.42482517e-01, 1.43618881e-01,\n",
              "          1.46765734e-01, 1.46940559e-01, 1.47202797e-01, 1.47377622e-01,\n",
              "          1.51486014e-01, 1.52097902e-01, 1.52447552e-01, 1.52972028e-01,\n",
              "          1.58479021e-01, 1.58653846e-01, 1.59527972e-01, 1.60052448e-01,\n",
              "          1.66695804e-01, 1.67744755e-01, 1.70629371e-01, 1.71153846e-01,\n",
              "          1.74388112e-01, 1.74388112e-01, 1.74475524e-01, 1.74650350e-01,\n",
              "          1.76223776e-01, 1.76748252e-01, 1.77709790e-01, 1.78059441e-01,\n",
              "          1.82779720e-01, 1.82954545e-01, 1.83391608e-01, 1.84003497e-01,\n",
              "          1.86451049e-01, 1.87062937e-01, 1.90646853e-01, 1.91171329e-01,\n",
              "          1.96066434e-01, 1.96765734e-01, 1.97377622e-01, 1.97727273e-01,\n",
              "          2.03583916e-01, 2.04108392e-01, 2.05856643e-01, 2.06905594e-01,\n",
              "          2.10926573e-01, 2.11363636e-01, 2.15472028e-01, 2.16608392e-01,\n",
              "          2.20367133e-01, 2.20979021e-01, 2.27010490e-01, 2.27534965e-01,\n",
              "          2.29370629e-01, 2.29807692e-01, 2.39423077e-01, 2.40472028e-01,\n",
              "          2.40559441e-01, 2.41433566e-01, 2.48513986e-01, 2.48688811e-01,\n",
              "          2.49562937e-01, 2.49737762e-01, 2.50437063e-01, 2.51136364e-01,\n",
              "          2.51660839e-01, 2.53234266e-01, 2.59615385e-01, 2.59790210e-01,\n",
              "          2.63723776e-01, 2.64510490e-01, 2.64685315e-01, 2.66083916e-01,\n",
              "          2.68094406e-01, 2.68269231e-01, 2.70541958e-01, 2.70716783e-01,\n",
              "          2.75961538e-01, 2.76923077e-01, 2.77010490e-01, 2.78671329e-01,\n",
              "          2.81818182e-01, 2.81993007e-01, 2.88286713e-01, 2.90122378e-01,\n",
              "          2.91608392e-01, 2.93006993e-01, 2.99038462e-01, 2.99213287e-01,\n",
              "          2.99562937e-01, 3.01660839e-01, 3.02972028e-01, 3.03146853e-01,\n",
              "          3.03409091e-01, 3.03583916e-01, 3.05856643e-01, 3.06905594e-01,\n",
              "          3.08129371e-01, 3.08304196e-01, 3.09090909e-01, 3.09440559e-01,\n",
              "          3.11538462e-01, 3.13898601e-01, 3.17132867e-01, 3.17307692e-01,\n",
              "          3.18444056e-01, 3.18618881e-01, 3.22115385e-01, 3.22814685e-01,\n",
              "          3.24300699e-01, 3.26923077e-01, 3.27709790e-01, 3.27884615e-01,\n",
              "          3.28409091e-01, 3.28583916e-01, 3.29895105e-01, 3.30244755e-01,\n",
              "          3.31118881e-01, 3.31643357e-01, 3.32779720e-01, 3.39685315e-01,\n",
              "          3.42045455e-01, 3.46590909e-01, 3.46765734e-01, 3.46853147e-01,\n",
              "          3.47202797e-01, 3.47377622e-01, 3.49038462e-01, 3.49562937e-01,\n",
              "          3.49737762e-01, 3.49912587e-01, 3.51748252e-01, 3.52010490e-01,\n",
              "          3.52097902e-01, 3.53583916e-01, 3.53933566e-01, 3.54108392e-01,\n",
              "          3.55944056e-01, 3.56293706e-01, 3.56555944e-01, 3.57167832e-01,\n",
              "          3.60052448e-01, 3.60227273e-01, 3.60314685e-01, 3.60664336e-01,\n",
              "          3.61101399e-01, 3.61363636e-01, 3.63024476e-01, 3.63199301e-01,\n",
              "          3.64248252e-01, 3.64423077e-01, 3.66346154e-01, 3.70979021e-01,\n",
              "          3.71765734e-01, 3.72115385e-01, 3.72814685e-01, 3.72989510e-01,\n",
              "          3.74213287e-01, 3.80856643e-01, 3.81293706e-01, 3.81381119e-01,\n",
              "          3.81730769e-01, 3.81993007e-01, 3.82167832e-01, 3.83216783e-01,\n",
              "          3.83566434e-01, 3.83828671e-01, 3.84003497e-01, 3.84178322e-01,\n",
              "          3.84353147e-01, 3.84440559e-01, 3.84615385e-01, 3.84790210e-01,\n",
              "          3.86451049e-01, 3.86625874e-01, 3.86800699e-01, 3.87587413e-01,\n",
              "          3.94755245e-01, 3.95104895e-01, 3.96765734e-01, 3.98601399e-01,\n",
              "          4.07604895e-01, 4.08216783e-01, 4.08391608e-01, 4.08653846e-01,\n",
              "          4.08828671e-01, 4.09003497e-01, 4.09440559e-01, 4.10227273e-01,\n",
              "          4.10402098e-01, 4.10576923e-01, 4.10839161e-01, 4.11538462e-01,\n",
              "          4.11888112e-01, 4.11975524e-01, 4.22639860e-01, 4.22814685e-01,\n",
              "          4.23164336e-01, 4.23513986e-01, 4.23688811e-01, 4.26223776e-01,\n",
              "          4.26398601e-01, 4.26748252e-01, 4.27097902e-01, 4.27185315e-01,\n",
              "          4.42482517e-01, 4.43793706e-01, 4.43968531e-01, 4.44230769e-01,\n",
              "          4.44405594e-01, 4.44755245e-01, 4.46066434e-01, 4.60576923e-01,\n",
              "          4.60751748e-01, 4.61451049e-01, 4.61625874e-01, 4.61888112e-01,\n",
              "          4.61975524e-01, 4.62150350e-01, 4.62587413e-01, 4.62937063e-01,\n",
              "          4.63286713e-01, 4.65821678e-01, 4.65996503e-01, 4.66433566e-01,\n",
              "          4.66608392e-01, 4.67657343e-01, 4.67832168e-01, 4.68881119e-01,\n",
              "          4.83653846e-01, 4.84790210e-01, 4.93094406e-01, 4.93269231e-01,\n",
              "          4.93618881e-01, 4.94318182e-01, 4.94930070e-01, 4.95104895e-01,\n",
              "          4.96590909e-01, 4.96940559e-01, 4.97027972e-01, 4.97202797e-01,\n",
              "          4.97902098e-01, 4.98164336e-01, 4.98601399e-01, 4.98776224e-01,\n",
              "          4.99388112e-01, 4.99650350e-01, 4.99737762e-01, 4.99912587e-01,\n",
              "          5.00087413e-01, 5.00437063e-01, 5.00961538e-01, 5.01136364e-01,\n",
              "          5.01748252e-01, 5.19143357e-01, 5.19230769e-01, 5.19493007e-01,\n",
              "          5.19667832e-01, 5.19842657e-01, 5.19930070e-01, 5.20104895e-01,\n",
              "          5.20367133e-01, 5.20716783e-01, 5.23164336e-01, 5.23339161e-01,\n",
              "          5.23601399e-01, 5.23776224e-01, 5.23951049e-01, 5.24213287e-01,\n",
              "          5.24562937e-01, 5.24825175e-01, 5.25087413e-01, 5.25611888e-01,\n",
              "          5.25786713e-01, 5.26048951e-01, 5.26136364e-01, 5.52447552e-01,\n",
              "          5.52972028e-01, 5.53583916e-01, 5.53671329e-01, 5.54108392e-01,\n",
              "          5.54283217e-01, 5.54458042e-01, 5.54720280e-01, 5.54895105e-01,\n",
              "          5.55069930e-01, 5.55506993e-01, 5.55594406e-01, 5.55856643e-01,\n",
              "          5.56031469e-01, 5.56293706e-01, 5.56468531e-01, 5.56818182e-01,\n",
              "          5.56993007e-01, 5.57342657e-01, 5.57954545e-01, 5.58566434e-01,\n",
              "          5.59527972e-01, 5.59702797e-01, 5.60227273e-01, 5.60839161e-01,\n",
              "          5.83129371e-01, 5.83304196e-01, 5.83391608e-01, 5.83741259e-01,\n",
              "          5.83828671e-01, 5.84265734e-01, 5.84702797e-01, 6.14073427e-01,\n",
              "          6.14685315e-01, 6.15472028e-01, 6.17045455e-01, 6.17569930e-01,\n",
              "          6.43006993e-01, 6.43356643e-01, 6.43618881e-01, 6.43881119e-01,\n",
              "          6.44055944e-01, 6.45279720e-01, 6.45891608e-01, 6.46416084e-01,\n",
              "          6.47202797e-01, 6.47377622e-01, 6.47989510e-01, 6.48513986e-01,\n",
              "          6.49038462e-01, 6.50000000e-01, 6.50611888e-01, 6.50874126e-01,\n",
              "          6.51136364e-01, 6.51748252e-01, 6.53234266e-01, 6.54195804e-01,\n",
              "          6.54283217e-01, 6.54458042e-01, 6.54545455e-01, 6.56555944e-01,\n",
              "          6.56730769e-01, 6.57867133e-01, 6.58041958e-01, 6.58304196e-01,\n",
              "          6.58479021e-01, 6.58653846e-01, 6.59090909e-01, 6.60052448e-01,\n",
              "          6.61013986e-01, 6.61538462e-01, 6.61888112e-01, 6.62150350e-01,\n",
              "          6.62674825e-01, 6.64160839e-01, 6.64685315e-01, 6.65297203e-01,\n",
              "          6.65472028e-01, 6.65646853e-01, 6.65909091e-01, 6.66520979e-01,\n",
              "          6.66695804e-01, 6.70716783e-01, 6.70891608e-01, 6.71328671e-01,\n",
              "          6.71503497e-01, 6.73513986e-01, 6.73688811e-01, 6.75000000e-01,\n",
              "          6.75174825e-01, 6.76835664e-01, 6.79720280e-01, 6.79895105e-01,\n",
              "          6.80069930e-01, 6.80332168e-01, 6.83304196e-01, 6.83479021e-01,\n",
              "          6.83916084e-01, 6.84090909e-01, 6.85664336e-01, 6.85839161e-01,\n",
              "          6.87062937e-01, 6.87237762e-01, 6.89335664e-01, 6.89510490e-01,\n",
              "          6.89597902e-01, 6.89772727e-01, 6.94055944e-01, 6.97989510e-01,\n",
              "          6.99912587e-01, 7.00087413e-01, 7.00349650e-01, 7.00524476e-01,\n",
              "          7.04632867e-01, 7.04807692e-01, 7.09527972e-01, 7.09790210e-01,\n",
              "          7.13286713e-01, 7.13548951e-01, 7.14772727e-01, 7.14947552e-01,\n",
              "          7.15384615e-01, 7.15559441e-01, 7.15734266e-01, 7.15909091e-01,\n",
              "          7.16083916e-01, 7.21153846e-01, 7.23339161e-01, 7.23513986e-01,\n",
              "          7.23951049e-01, 7.24213287e-01, 7.27884615e-01, 7.28146853e-01,\n",
              "          7.30069930e-01, 7.30419580e-01, 7.30769231e-01, 7.36538462e-01,\n",
              "          7.36713287e-01, 7.36888112e-01, 7.37237762e-01, 7.37587413e-01,\n",
              "          7.37762238e-01, 7.39073427e-01, 7.39423077e-01, 7.41083916e-01,\n",
              "          7.41258741e-01, 7.42482517e-01, 7.42657343e-01, 7.44755245e-01,\n",
              "          7.44930070e-01, 7.48601399e-01, 7.48776224e-01, 7.55069930e-01,\n",
              "          7.55157343e-01, 7.55419580e-01, 7.60489510e-01, 7.60664336e-01,\n",
              "          7.63374126e-01, 7.63548951e-01, 7.64335664e-01, 7.64597902e-01,\n",
              "          7.64860140e-01, 7.65384615e-01, 7.66171329e-01, 7.66346154e-01,\n",
              "          7.68793706e-01, 7.68968531e-01, 7.71416084e-01, 7.71590909e-01,\n",
              "          7.72989510e-01, 7.73164336e-01, 7.74388112e-01, 7.74650350e-01,\n",
              "          7.75699301e-01, 7.80681818e-01, 7.80856643e-01, 7.81381119e-01,\n",
              "          7.81555944e-01, 7.84440559e-01, 7.84702797e-01, 7.85751748e-01,\n",
              "          7.86013986e-01, 7.88024476e-01, 7.88374126e-01, 7.89248252e-01,\n",
              "          7.89423077e-01, 7.89510490e-01, 7.89685315e-01, 7.92307692e-01,\n",
              "          7.92482517e-01, 7.92919580e-01, 7.93094406e-01, 7.93444056e-01,\n",
              "          7.97552448e-01, 7.98863636e-01, 7.99038462e-01, 8.00349650e-01,\n",
              "          8.00524476e-01, 8.00874126e-01, 8.01136364e-01, 8.02972028e-01,\n",
              "          8.09965035e-01, 8.12325175e-01, 8.12500000e-01, 8.13986014e-01,\n",
              "          8.14335664e-01, 8.14597902e-01, 8.14772727e-01, 8.15034965e-01,\n",
              "          8.15209790e-01, 8.20104895e-01, 8.20279720e-01, 8.23251748e-01,\n",
              "          8.29807692e-01, 8.31293706e-01, 8.41346154e-01, 8.44493007e-01,\n",
              "          8.44755245e-01, 8.48951049e-01, 8.56818182e-01, 8.57779720e-01,\n",
              "          8.57954545e-01, 8.70454545e-01, 8.71416084e-01, 8.71590909e-01,\n",
              "          8.72115385e-01, 8.72377622e-01, 8.73426573e-01, 8.84178322e-01,\n",
              "          8.85227273e-01, 8.85402098e-01, 8.88374126e-01, 8.88986014e-01,\n",
              "          8.89073427e-01, 8.89248252e-01, 8.93006993e-01, 8.93793706e-01,\n",
              "          8.94055944e-01, 8.94318182e-01, 8.99825175e-01, 9.00000000e-01,\n",
              "          9.00349650e-01, 9.00611888e-01, 9.00961538e-01, 9.01136364e-01,\n",
              "          9.05856643e-01, 9.06118881e-01, 9.06293706e-01, 9.08479021e-01,\n",
              "          9.08741259e-01, 9.10402098e-01, 9.10664336e-01, 9.35926573e-01,\n",
              "          9.36101399e-01, 9.50349650e-01, 9.50524476e-01, 9.52622378e-01,\n",
              "          9.52884615e-01, 9.76136364e-01, 9.76311189e-01, 9.80244755e-01,\n",
              "          9.80419580e-01, 9.82430070e-01, 9.82604895e-01, 9.84440559e-01,\n",
              "          9.84615385e-01, 9.85839161e-01, 9.86013986e-01, 9.86451049e-01,\n",
              "          9.86625874e-01, 9.87674825e-01, 9.87849650e-01, 9.87937063e-01,\n",
              "          9.88111888e-01, 9.93618881e-01, 9.93793706e-01, 9.93881119e-01,\n",
              "          9.94055944e-01, 9.96066434e-01, 9.96241259e-01, 9.96853147e-01,\n",
              "          9.97027972e-01, 9.99562937e-01, 9.99737762e-01, 1.00000000e+00]),\n",
              "   array([0.        , 0.        , 0.        , 0.00609756, 0.00609756,\n",
              "          0.01219512, 0.01219512, 0.01829268, 0.01829268, 0.02439024,\n",
              "          0.02439024, 0.03658537, 0.03658537, 0.04268293, 0.04268293,\n",
              "          0.04878049, 0.04878049, 0.05487805, 0.05487805, 0.06097561,\n",
              "          0.06097561, 0.07317073, 0.07317073, 0.08536585, 0.08536585,\n",
              "          0.09146341, 0.09146341, 0.09756098, 0.09756098, 0.10365854,\n",
              "          0.10365854, 0.11585366, 0.11585366, 0.12195122, 0.12195122,\n",
              "          0.12804878, 0.12804878, 0.1402439 , 0.1402439 , 0.15243902,\n",
              "          0.15243902, 0.16463415, 0.16463415, 0.17682927, 0.17682927,\n",
              "          0.18292683, 0.18292683, 0.19512195, 0.19512195, 0.20731707,\n",
              "          0.20731707, 0.2195122 , 0.2195122 , 0.22560976, 0.22560976,\n",
              "          0.23170732, 0.23170732, 0.23780488, 0.23780488, 0.25609756,\n",
              "          0.25609756, 0.26219512, 0.26219512, 0.26829268, 0.26829268,\n",
              "          0.2804878 , 0.2804878 , 0.28658537, 0.28658537, 0.29268293,\n",
              "          0.29268293, 0.30487805, 0.30487805, 0.31097561, 0.31097561,\n",
              "          0.31707317, 0.31707317, 0.32317073, 0.32317073, 0.32926829,\n",
              "          0.32926829, 0.34146341, 0.34146341, 0.34756098, 0.34756098,\n",
              "          0.35365854, 0.35365854, 0.3597561 , 0.3597561 , 0.36585366,\n",
              "          0.36585366, 0.37195122, 0.37195122, 0.37804878, 0.37804878,\n",
              "          0.38414634, 0.38414634, 0.3902439 , 0.3902439 , 0.39634146,\n",
              "          0.39634146, 0.40243902, 0.40243902, 0.40853659, 0.40853659,\n",
              "          0.42073171, 0.42073171, 0.42682927, 0.42682927, 0.43292683,\n",
              "          0.43292683, 0.43902439, 0.43902439, 0.45121951, 0.45121951,\n",
              "          0.45731707, 0.45731707, 0.47560976, 0.47560976, 0.48170732,\n",
              "          0.48170732, 0.48780488, 0.48780488, 0.5       , 0.5       ,\n",
              "          0.50609756, 0.50609756, 0.51219512, 0.51219512, 0.52439024,\n",
              "          0.52439024, 0.5304878 , 0.5304878 , 0.54268293, 0.54268293,\n",
              "          0.54878049, 0.54878049, 0.55487805, 0.55487805, 0.56097561,\n",
              "          0.56097561, 0.57317073, 0.57317073, 0.57926829, 0.57926829,\n",
              "          0.58536585, 0.58536585, 0.59146341, 0.59146341, 0.60365854,\n",
              "          0.60365854, 0.6097561 , 0.6097561 , 0.61585366, 0.61585366,\n",
              "          0.62195122, 0.62195122, 0.62804878, 0.62804878, 0.6402439 ,\n",
              "          0.6402439 , 0.6402439 , 0.64634146, 0.64634146, 0.65243902,\n",
              "          0.65243902, 0.65853659, 0.65853659, 0.66463415, 0.66463415,\n",
              "          0.67073171, 0.67073171, 0.68902439, 0.68902439, 0.69512195,\n",
              "          0.69512195, 0.70121951, 0.70121951, 0.70731707, 0.70731707,\n",
              "          0.7195122 , 0.7195122 , 0.72560976, 0.72560976, 0.73170732,\n",
              "          0.73170732, 0.73780488, 0.73780488, 0.75      , 0.75      ,\n",
              "          0.75609756, 0.75609756, 0.76219512, 0.76219512, 0.76829268,\n",
              "          0.76829268, 0.77439024, 0.77439024, 0.78658537, 0.78658537,\n",
              "          0.79268293, 0.79268293, 0.79878049, 0.79878049, 0.80487805,\n",
              "          0.80487805, 0.81097561, 0.81097561, 0.81097561, 0.81097561,\n",
              "          0.81097561, 0.81097561, 0.81707317, 0.81707317, 0.82317073,\n",
              "          0.82317073, 0.82926829, 0.82926829, 0.83536585, 0.83536585,\n",
              "          0.84146341, 0.84146341, 0.84756098, 0.84756098, 0.84756098,\n",
              "          0.85365854, 0.85365854, 0.8597561 , 0.8597561 , 0.86585366,\n",
              "          0.86585366, 0.87195122, 0.87195122, 0.87804878, 0.87804878,\n",
              "          0.88414634, 0.88414634, 0.8902439 , 0.8902439 , 0.89634146,\n",
              "          0.89634146, 0.89634146, 0.89634146, 0.90243902, 0.90243902,\n",
              "          0.90853659, 0.90853659, 0.91463415, 0.91463415, 0.91463415,\n",
              "          0.91463415, 0.92073171, 0.92073171, 0.92682927, 0.92682927,\n",
              "          0.92682927, 0.92682927, 0.93292683, 0.93292683, 0.93902439,\n",
              "          0.93902439, 0.93902439, 0.93902439, 0.94512195, 0.94512195,\n",
              "          0.95121951, 0.95121951, 0.95121951, 0.95121951, 0.95121951,\n",
              "          0.95121951, 0.95731707, 0.95731707, 0.95731707, 0.95731707,\n",
              "          0.95731707, 0.95731707, 0.9695122 , 0.9695122 , 0.9695122 ,\n",
              "          0.9695122 , 0.9695122 , 0.9695122 , 0.9695122 , 0.9695122 ,\n",
              "          0.9695122 , 0.9695122 , 0.9695122 , 0.9695122 , 0.97560976,\n",
              "          0.97560976, 0.97560976, 0.97560976, 0.98170732, 0.98170732,\n",
              "          0.98170732, 0.98170732, 0.98170732, 0.98170732, 0.98170732,\n",
              "          0.98170732, 0.98780488, 0.98780488, 0.99390244, 0.99390244,\n",
              "          0.99390244, 0.99390244, 0.99390244, 0.99390244, 0.99390244,\n",
              "          0.99390244, 0.99390244, 0.99390244, 0.99390244, 0.99390244,\n",
              "          0.99390244, 0.99390244, 0.99390244, 0.99390244, 0.99390244,\n",
              "          0.99390244, 0.99390244, 0.99390244, 0.99390244, 0.99390244,\n",
              "          0.99390244, 0.99390244, 0.99390244, 0.99390244, 0.99390244,\n",
              "          0.99390244, 0.99390244, 0.99390244, 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "          1.        , 1.        ]),\n",
              "   array([84.36464  , 83.36464  , 18.92464  , 18.773132 , 16.24997  ,\n",
              "          15.834392 , 13.632445 , 13.57276  , 13.400177 , 13.327147 ,\n",
              "          12.977669 , 12.886081 , 12.357874 , 12.317921 , 12.200608 ,\n",
              "          12.161238 , 12.128962 , 12.079018 , 11.863585 , 11.788183 ,\n",
              "          11.634776 , 11.610934 , 11.477646 , 11.379269 , 10.970701 ,\n",
              "          10.936843 , 10.850275 , 10.811746 , 10.69966  , 10.668967 ,\n",
              "          10.615039 , 10.603988 , 10.506026 , 10.503533 , 10.334854 ,\n",
              "          10.272155 , 10.014238 ,  9.979643 ,  9.945896 ,  9.895598 ,\n",
              "           9.794567 ,  9.73712  ,  9.629309 ,  9.507177 ,  9.495783 ,\n",
              "           9.488137 ,  9.439355 ,  9.420655 ,  9.325352 ,  9.217158 ,\n",
              "           9.152001 ,  9.129516 ,  9.120809 ,  9.101826 ,  9.095482 ,\n",
              "           9.051834 ,  9.039151 ,  8.996777 ,  8.972052 ,  8.935298 ,\n",
              "           8.91991  ,  8.891697 ,  8.7902355,  8.789346 ,  8.779994 ,\n",
              "           8.742266 ,  8.741912 ,  8.741135 ,  8.657486 ,  8.639873 ,\n",
              "           8.5170965,  8.485093 ,  8.3573885,  8.338151 ,  8.320228 ,\n",
              "           8.281048 ,  8.280399 ,  8.266541 ,  8.209293 ,  8.204224 ,\n",
              "           8.068245 ,  8.029482 ,  7.998648 ,  7.9799843,  7.840114 ,\n",
              "           7.837808 ,  7.818881 ,  7.8149934,  7.766789 ,  7.745035 ,\n",
              "           7.727275 ,  7.711736 ,  7.6980343,  7.669165 ,  7.6218004,\n",
              "           7.6138535,  7.5867243,  7.5812035,  7.544777 ,  7.5060277,\n",
              "           7.4952755,  7.4930325,  7.4785748,  7.4778605,  7.4580564,\n",
              "           7.439397 ,  7.276712 ,  7.2731266,  7.2435637,  7.234764 ,\n",
              "           7.229837 ,  7.2280655,  7.0386095,  7.025996 ,  6.992221 ,\n",
              "           6.9753246,  6.9456644,  6.9205284,  6.894867 ,  6.894313 ,\n",
              "           6.762273 ,  6.760448 ,  6.7603955,  6.756404 ,  6.7486467,\n",
              "           6.7477145,  6.7403216,  6.734823 ,  6.6959553,  6.691163 ,\n",
              "           6.672555 ,  6.667982 ,  6.6525764,  6.635779 ,  6.6346617,\n",
              "           6.6294227,  6.6037836,  6.5955334,  6.56331  ,  6.559075 ,\n",
              "           6.5131025,  6.4878006,  6.482888 ,  6.4798927,  6.4546595,\n",
              "           6.4382524,  6.432375 ,  6.4300995,  6.4134192,  6.382446 ,\n",
              "           6.3597198,  6.354487 ,  6.3487983,  6.343655 ,  6.2435226,\n",
              "           6.2433553,  6.2144804,  6.1974993,  6.163459 ,  6.154769 ,\n",
              "           6.147671 ,  6.140255 ,  6.140001 ,  6.1076503,  6.0961533,\n",
              "           6.0951285,  6.0889883,  6.0878854,  6.086156 ,  6.0659847,\n",
              "           6.0654716,  6.038995 ,  6.031904 ,  6.0075808,  6.007102 ,\n",
              "           5.993395 ,  5.9924026,  5.983128 ,  5.980322 ,  5.9691467,\n",
              "           5.965166 ,  5.894063 ,  5.8890524,  5.870467 ,  5.8703628,\n",
              "           5.859627 ,  5.857692 ,  5.8449235,  5.844189 ,  5.800892 ,\n",
              "           5.800816 ,  5.7669096,  5.766317 ,  5.757272 ,  5.7571716,\n",
              "           5.7315664,  5.730716 ,  5.5816383,  5.57742  ,  5.5758715,\n",
              "           5.5729523,  5.5572987,  5.5534945,  5.5456576,  5.5450544,\n",
              "           5.4800906,  5.4766455,  5.450695 ,  5.4457297,  5.374174 ,\n",
              "           5.373985 ,  5.369834 ,  5.3688116,  5.3645563,  5.359229 ,\n",
              "           5.311046 ,  5.3080816,  5.2575483,  5.2568703,  5.243103 ,\n",
              "           5.241555 ,  5.232238 ,  5.2288055,  5.2281365,  5.1881366,\n",
              "           5.1847744,  5.183383 ,  5.180021 ,  5.1247387,  5.1214557,\n",
              "           5.118902 ,  5.1128507,  5.047522 ,  5.046789 ,  5.0090075,\n",
              "           5.004584 ,  4.960616 ,  4.95992  ,  4.937885 ,  4.935044 ,\n",
              "           4.920313 ,  4.917156 ,  4.903612 ,  4.9001975,  4.8736873,\n",
              "           4.8699636,  4.8513794,  4.8501606,  4.838352 ,  4.8351507,\n",
              "           4.7707567,  4.770512 ,  4.7481117,  4.7454915,  4.736573 ,\n",
              "           4.736026 ,  4.730597 ,  4.7299848,  4.7154107,  4.714439 ,\n",
              "           4.65613  ,  4.6535006,  4.647517 ,  4.639806 ,  4.606172 ,\n",
              "           4.6060495,  4.5884295,  4.5878663,  4.587575 ,  4.5873923,\n",
              "           4.5563307,  4.55522  ,  4.5238957,  4.5234456,  4.4986553,\n",
              "           4.4981365,  4.415415 ,  4.4135146,  4.332533 ,  4.332456 ,\n",
              "           4.3239017,  4.320795 ,  4.3174524,  4.316826 ,  4.2328057,\n",
              "           4.231544 ,  4.2135816,  4.2124248,  4.1585455,  4.1557646,\n",
              "           4.144535 ,  4.1435876,  4.076432 ,  4.0745726,  3.9731884,\n",
              "           3.9730988,  3.9728985,  3.9712334,  3.9167109,  3.916151 ,\n",
              "           3.902575 ,  3.9013   ,  3.893299 ,  3.891796 ,  3.8861375,\n",
              "           3.8854945,  3.8047526,  3.8043814,  3.8035831,  3.7232966,\n",
              "           3.722295 ,  3.646409 ,  3.644843 ,  3.6398778,  3.56048  ,\n",
              "           3.5595193,  3.53197  ,  3.5316315,  3.5282624,  3.5270712,\n",
              "           3.480284 ,  3.479465 ,  3.4757168,  3.4693253,  3.4095173,\n",
              "           3.4085288,  3.3988366,  3.3983126,  3.3218613,  3.3212004,\n",
              "           3.2908807,  3.2908254,  3.262761 ,  3.2626572,  3.260467 ,\n",
              "           3.2601824,  3.2480814,  3.2453256,  3.2394938,  3.2385721,\n",
              "           3.1962328,  3.19554  ,  3.1937838,  3.1915507,  3.174277 ,\n",
              "           3.1741683,  3.1441832,  3.144023 ,  3.1064231,  3.1062207,\n",
              "           3.1011527,  3.1008167,  3.0543907,  3.054255 ,  3.0411458,\n",
              "           3.0400815,  3.0066676,  3.0064225,  2.9751441,  2.975043 ,\n",
              "           2.9498487,  2.9486246,  2.9086633,  2.9082046,  2.8941112,\n",
              "           2.8933563,  2.8394418,  2.839362 ,  2.8388996,  2.8385572,\n",
              "           2.7927995,  2.7925103,  2.7866073,  2.785441 ,  2.7817438,\n",
              "           2.7815742,  2.7785778,  2.7783704,  2.7414098,  2.7413554,\n",
              "           2.7216573,  2.7209997,  2.719205 ,  2.7188244,  2.7095406,\n",
              "           2.7092562,  2.698491 ,  2.6983685,  2.6651244,  2.6644073,\n",
              "           2.6639757,  2.663842 ,  2.6464858,  2.6463447,  2.6205409,\n",
              "           2.6203444,  2.6145005,  2.6141038,  2.5873063,  2.5872035,\n",
              "           2.5862308,  2.5859234,  2.5812902,  2.5812182,  2.580296 ,\n",
              "           2.5793452,  2.570656 ,  2.5700424,  2.5665255,  2.565726 ,\n",
              "           2.5618458,  2.5614831,  2.554922 ,  2.5546632,  2.542    ,\n",
              "           2.5417879,  2.5391932,  2.5391073,  2.5286157,  2.5284052,\n",
              "           2.524874 ,  2.5247653,  2.52184  ,  2.5217261,  2.5210953,\n",
              "           2.5210094,  2.5158062,  2.5148468,  2.5129209,  2.5121005,\n",
              "           2.5085547,  2.508439 ,  2.5030265,  2.502679 ,  2.5021896,\n",
              "           2.5021343,  2.5016842,  2.5016718,  2.4966009,  2.4959915,\n",
              "           2.4959383,  2.4958773,  2.489982 ,  2.4898472,  2.4898372,\n",
              "           2.4896631,  2.488862 ,  2.488584 ,  2.4839792,  2.4839778,\n",
              "           2.483821 ,  2.4815679,  2.4814026,  2.4813457,  2.4803548,\n",
              "           2.4801292,  2.4788342,  2.478705 ,  2.4739103,  2.473771 ,\n",
              "           2.4710965,  2.4705563,  2.466836 ,  2.466819 ,  2.4646008,\n",
              "           2.4645805,  2.4634159,  2.4633474,  2.4609108,  2.4604216,\n",
              "           2.4600852,  2.459888 ,  2.4598858,  2.4595385,  2.459266 ,\n",
              "           2.4574997,  2.4573958,  2.4571567,  2.457042 ,  2.4569037,\n",
              "           2.4564412,  2.4563239,  2.4559848,  2.45573  ,  2.4556572,\n",
              "           2.4553084,  2.455306 ,  2.4536147,  2.4535613,  2.4519014,\n",
              "           2.4517112,  2.4481819,  2.4477487,  2.4464831,  2.4462755,\n",
              "           2.4460998,  2.4460144,  2.444691 ,  2.4446592,  2.4433804,\n",
              "           2.4433637,  2.442463 ,  2.4424243,  2.4414167,  2.4414034,\n",
              "           2.4410763,  2.4409492,  2.4404778,  2.4395034,  2.4386437,\n",
              "           2.4384408,  2.4343767,  2.4341102,  2.4326015,  2.432436 ,\n",
              "           2.432405 ,  2.4318566,  2.4290218,  2.4287043,  2.428492 ,\n",
              "           2.4281936,  2.4281824,  2.4257197,  2.4256153,  2.4256034,\n",
              "           2.4244742,  2.4241867,  2.4239807,  2.4238753,  2.423721 ,\n",
              "           2.423364 ,  2.4229672,  2.4226108,  2.4225318,  2.4218545,\n",
              "           2.4207869,  2.420645 ,  2.419404 ,  2.4193301,  2.4180179,\n",
              "           2.4176235,  2.4162457,  2.4162436,  2.4157548,  2.4150622,\n",
              "           2.4140592,  2.414052 ,  2.4140506,  2.4111729,  2.4108963,\n",
              "           2.4108412,  2.4107149,  2.4094872,  2.4094143,  2.4086387,\n",
              "           2.4082937,  2.407167 ,  2.4071   ,  2.4070578,  2.4070024,\n",
              "           2.4067378,  2.4064796,  2.4057436,  2.4052997,  2.4045458,\n",
              "           2.4045153,  2.404446 ,  2.404434 ,  2.404322 ,  2.4043202,\n",
              "           2.4039955,  2.40374  ,  2.4033394,  2.4029343,  2.4028552,\n",
              "           2.4023056,  2.4022226,  2.4015555,  2.4013352,  2.4007847,\n",
              "           2.4007745,  2.4005523,  2.4003189,  2.4000828,  2.3998392,\n",
              "           2.3997967,  2.3997293,  2.3996544,  2.3991296,  2.3987467,\n",
              "           2.3985188,  2.3984902,  2.3984816,  2.3982933,  2.3979588,\n",
              "           2.3977535,  2.3972435,  2.3965144,  2.3963866,  2.396325 ,\n",
              "           2.3960638,  2.3957756,  2.3954515,  2.3953133,  2.394908 ,\n",
              "           2.3948812,  2.3939028,  2.3938699,  2.391792 ,  2.3917313,\n",
              "           2.3917103,  2.390873 ,  2.3901095,  2.3900244,  2.3898644,\n",
              "           2.3898263,  2.3897705,  2.38972  ,  2.389389 ,  2.3893638,\n",
              "           2.388112 ,  2.3879879,  2.3851652,  2.385126 ,  2.3849044,\n",
              "           2.3845322,  2.3845115,  2.3842058,  2.3841882,  2.3819108,\n",
              "           2.3819044,  2.3812783,  2.3812747,  2.381272 ,  2.3803878,\n",
              "           2.380331 ,  2.3789644,  2.3789139,  2.3780267,  2.3780005,\n",
              "           2.3776174,  2.3775718,  2.375252 ,  2.3746634,  2.3742976,\n",
              "           2.3742075,  2.3741503,  2.374135 ,  2.3739724,  2.3726492,\n",
              "           2.3725317,  2.3717   ,  2.371646 ,  2.3711047,  2.3706596,\n",
              "           2.3692381,  2.3689437,  2.3681512,  2.368144 ,  2.3676128,\n",
              "           2.367518 ,  2.3641284,  2.3639138,  2.3629432,  2.3626742,\n",
              "           2.3620088,  2.3619647,  2.3612647,  2.3612075,  2.3525064,\n",
              "           2.3524356,  2.351677 ,  2.35143  ,  2.34677  ,  2.3467612,\n",
              "           2.3443258,  2.344301 ,  2.3398786,  2.3398426,  2.339275 ,\n",
              "           2.3390694,  2.3390253,  2.3330684,  2.3329747,  2.3323674,\n",
              "           2.3321638,  2.329136 ,  2.3290756,  2.3268964,  2.326542 ,\n",
              "           2.3222966,  2.322208 ,  2.3221302,  2.3219557,  2.3121376,\n",
              "           2.3118763,  2.306529 ,  2.3062787,  2.3059192,  2.3058202,\n",
              "           2.296367 ,  2.2963467,  2.285194 ,  2.2847824,  2.278774 ,\n",
              "           2.2787352,  2.277126 ,  2.2770495,  2.276366 ,  2.276181 ,\n",
              "           2.2761095,  2.2760668,  2.2756503,  2.275305 ,  2.2728546,\n",
              "           2.2726836,  2.271733 ,  2.2715082,  2.2637596,  2.2635472,\n",
              "           2.2635055,  2.262846 ,  2.262766 ,  2.250219 ,  2.2501597,\n",
              "           2.2500942,  2.2500381,  2.249604 ,  2.2493582,  2.2472076,\n",
              "           2.2466774,  2.244059 ,  2.2440028,  2.2417817,  2.2416337,\n",
              "           2.2382493,  2.2382061,  2.2315354,  2.2312264,  2.2310052,\n",
              "           2.2308505,  2.2308247,  2.2215838,  2.221466 ,  2.217556 ,\n",
              "           2.2174304,  2.2167425,  2.2166212,  2.216115 ,  2.2155468,\n",
              "           2.2136433,  2.2131424,  2.2088294,  2.2087998,  2.203891 ,\n",
              "           2.2036393,  2.2014387,  2.200933 ,  2.1977844,  2.1977758,\n",
              "           2.1951942,  2.1951687,  2.1948829,  2.1929631,  2.192955 ,\n",
              "           2.1861622,  2.1859903,  2.1844451,  2.1843722,  2.1819775,\n",
              "           2.1812272,  2.1794271,  2.1793725,  2.1791744,  2.1788979,\n",
              "           2.1730778,  2.1727061,  2.171575 ,  2.171557 ,  2.1709366,\n",
              "           2.170907 ,  2.168369 ,  2.1682413,  2.1666663,  2.1665614,\n",
              "           2.1661713,  2.166171 ,  2.1624463,  2.1623785,  2.155984 ,\n",
              "           2.1558487,  2.1533852,  2.1530461,  2.152582 ,  2.1525245,\n",
              "           2.1521132,  2.1520982,  2.1432812,  2.1429615,  2.1379538,\n",
              "           2.1379185,  2.135961 ,  2.1359034,  2.130022 ,  2.130003 ,\n",
              "           2.1204603,  2.1204443,  2.1188664,  2.1184585,  2.1180396,\n",
              "           2.1158574,  2.1157134,  2.1149557,  2.1140866,  2.1125598,\n",
              "           2.1122475,  2.1100545,  2.109794 ,  2.104571 ,  2.104437 ,\n",
              "           2.104206 ,  2.1042037,  2.0975413,  2.097497 ,  2.0970504,\n",
              "           2.0966916,  2.0852962,  2.0850883,  2.0843024,  2.0842905,\n",
              "           2.083125 ,  2.0828555,  2.072497 ,  2.0723817,  2.071715 ,\n",
              "           2.066392 ,  2.0663369,  2.0625634,  2.062464 ,  2.0017343,\n",
              "           2.0013285,  1.9631786,  1.9621084,  1.9522347,  1.9519134,\n",
              "           1.858952 ,  1.8576162,  1.8391888,  1.8385134,  1.8254371,\n",
              "           1.8207518,  1.8111374,  1.8108051,  1.804599 ,  1.8034451,\n",
              "           1.8005853,  1.800546 ,  1.7934417,  1.7934376,  1.7922082,\n",
              "           1.7921245,  1.7304707,  1.728903 ,  1.728689 ,  1.7247056,\n",
              "           1.6916432,  1.6909802,  1.6805074,  1.6794362,  1.554109 ,\n",
              "           1.5510718,  1.454258 ], dtype=float32)),\n",
              "  'v_pr_curve': (array([0.07596109, 0.0755329 , 0.07556792, ..., 0.        , 0.        ,\n",
              "          1.        ]),\n",
              "   array([1.        , 0.99390244, 0.99390244, ..., 0.        , 0.        ,\n",
              "          0.        ]),\n",
              "   array([ 3.2626572,  3.262761 ,  3.2627907, ..., 37.365852 , 80.93202  ,\n",
              "          83.36464  ], dtype=float32)),\n",
              "  'v_roc_auc': 0.9681343808630394,\n",
              "  'v_pr_auc': 0.191033613583413,\n",
              "  'e_roc_curve': (array([0.00000000e+00, 2.59342825e-05, 1.29671413e-04, ...,\n",
              "          9.99533183e-01, 9.99610986e-01, 1.00000000e+00]),\n",
              "   array([0., 0., 0., ..., 1., 1., 1.]),\n",
              "   array([58.519962  , 57.519962  , 10.686086  , ...,  0.240365  ,\n",
              "           0.23990901,  0.1884938 ], dtype=float32)),\n",
              "  'e_pr_curve': (array([0.02722772, 0.02720238, 0.02720309, ..., 0.        , 0.        ,\n",
              "          1.        ]),\n",
              "   array([1.        , 0.99904306, 0.99904306, ..., 0.        , 0.        ,\n",
              "          0.        ]),\n",
              "   array([ 0.5081344,  0.5084414,  0.5084461, ..., 11.464611 , 57.230793 ,\n",
              "          57.519962 ], dtype=float32)),\n",
              "  'e_roc_auc': 0.9331381065070109,\n",
              "  'e_pr_auc': 0.22381965379320912},\n",
              " 'state_dict': OrderedDict([('encoder_convs.0.lin_u.weight',\n",
              "               tensor([[ 3.9665e-02, -5.5068e-03, -1.6756e-02,  9.0704e-02, -1.3125e-01,\n",
              "                        -1.7156e-01,  1.4610e-01,  1.4713e-01, -5.9311e-02, -1.1704e-01,\n",
              "                         6.3816e-02, -2.3046e-02, -6.0745e-02,  6.1440e-02,  1.1056e-01,\n",
              "                         7.8708e-03,  5.5668e-02, -1.6179e-01,  1.3620e-01, -9.6133e-03,\n",
              "                        -3.1897e-02, -1.4125e-02, -1.6628e-02,  1.8533e-01, -7.0695e-02,\n",
              "                         5.6311e-03, -5.5670e-02,  6.7192e-02],\n",
              "                       [ 1.6674e-01,  1.9591e-02,  4.2956e-02, -5.0798e-02, -6.9833e-02,\n",
              "                         1.1227e-01,  4.0676e-02,  1.8333e-01,  3.4587e-02, -1.8824e-01,\n",
              "                         1.4307e-01, -1.0621e-01, -1.4255e-01,  1.3842e-01, -2.7977e-02,\n",
              "                         1.0945e-01,  1.7362e-01, -5.6192e-02, -1.8104e-01,  8.6940e-02,\n",
              "                         1.3787e-01, -1.3207e-01,  1.4317e-01,  8.2941e-02,  1.6646e-01,\n",
              "                        -3.8487e-02,  1.4738e-01,  1.5013e-02],\n",
              "                       [-4.0558e-02,  1.6292e-01, -1.4419e-02,  1.7035e-01, -3.5450e-02,\n",
              "                         9.0018e-02,  1.2850e-01,  1.0861e-01, -1.0666e-01, -1.3144e-01,\n",
              "                        -1.3789e-01, -1.7702e-01,  1.3906e-01, -1.8170e-01,  1.2400e-01,\n",
              "                         4.5084e-03, -8.5102e-02,  4.8797e-02, -4.8670e-02,  2.4602e-02,\n",
              "                        -7.0472e-02,  1.2935e-01,  2.6906e-02,  9.5034e-02, -1.2512e-01,\n",
              "                         5.8455e-02, -1.5981e-01, -7.3529e-02],\n",
              "                       [-1.1627e-01, -8.1042e-02,  1.8172e-01,  1.3598e-01,  1.1221e-01,\n",
              "                         7.1224e-02, -1.1438e-01, -4.0881e-02, -1.8840e-01, -4.0160e-02,\n",
              "                         1.5373e-01,  3.3573e-02,  1.1143e-01, -1.1227e-01,  2.9340e-02,\n",
              "                         5.1188e-02,  5.8745e-02,  3.0565e-02, -1.7332e-01, -1.2877e-01,\n",
              "                         1.2848e-01, -1.0692e-01,  1.4803e-01, -7.4793e-02,  5.7117e-02,\n",
              "                        -1.2429e-02,  1.7165e-02,  3.0844e-02],\n",
              "                       [-9.4498e-03,  1.5880e-01, -1.4319e-01,  8.5004e-02, -6.8078e-03,\n",
              "                        -3.6919e-02, -8.5363e-02, -9.2351e-02, -1.5241e-01, -6.7808e-02,\n",
              "                        -1.5678e-01, -7.9537e-02,  8.7594e-02, -2.6447e-02, -4.9120e-02,\n",
              "                         1.6009e-01,  1.4777e-01, -6.0407e-02,  8.8087e-02, -3.8258e-02,\n",
              "                         8.4613e-02, -1.3996e-01, -1.4995e-01, -1.8701e-05, -2.6342e-02,\n",
              "                        -5.4124e-02,  1.4548e-01,  1.1045e-02],\n",
              "                       [ 1.7237e-01,  8.9805e-02,  1.0327e-01, -1.2191e-02, -7.6402e-02,\n",
              "                        -3.8047e-02, -1.1810e-01,  1.7725e-02,  7.1139e-02, -5.3915e-02,\n",
              "                         3.8451e-02,  8.2372e-02,  1.3406e-01,  3.7874e-02, -2.4691e-02,\n",
              "                         1.2620e-01,  8.8034e-02,  5.2490e-02,  1.5084e-01,  1.0409e-01,\n",
              "                        -1.1359e-01, -2.3586e-02, -1.4344e-01,  1.7189e-01,  1.1073e-01,\n",
              "                        -1.0564e-01, -1.8549e-01, -1.8287e-01],\n",
              "                       [-1.1219e-01,  1.1108e-01, -1.3628e-01, -4.5011e-02, -1.5058e-01,\n",
              "                         5.5501e-02,  1.8954e-03,  1.5684e-01,  1.4093e-01, -1.0526e-01,\n",
              "                         4.7116e-02, -9.1011e-02,  1.6829e-01, -1.0050e-01,  1.8475e-01,\n",
              "                        -1.3502e-01,  9.3388e-02, -1.1064e-02,  1.3299e-01,  1.2009e-01,\n",
              "                        -6.3200e-02, -5.1819e-02, -1.0826e-01,  6.3158e-02,  1.2227e-02,\n",
              "                         1.6518e-02,  1.4594e-01, -7.1850e-02],\n",
              "                       [ 1.1617e-01, -1.4132e-01, -1.7632e-01, -1.5428e-02, -1.2665e-01,\n",
              "                        -9.1815e-02,  1.7408e-02, -4.6230e-02,  3.9775e-02, -1.4696e-01,\n",
              "                        -1.6415e-01,  1.7173e-02,  1.8461e-01,  1.8762e-01, -8.8430e-02,\n",
              "                        -1.5530e-01,  8.1498e-02, -3.2111e-02, -7.9257e-02, -3.9426e-03,\n",
              "                         4.5168e-02,  1.7957e-01, -5.3597e-02, -1.7026e-01, -1.4671e-01,\n",
              "                         6.0797e-02, -1.5259e-01, -1.6142e-01],\n",
              "                       [ 7.0368e-03, -1.5666e-01, -7.3915e-03,  1.1447e-01,  5.6871e-02,\n",
              "                        -1.0456e-01,  1.1487e-01,  2.4242e-02,  1.5760e-01,  9.4881e-02,\n",
              "                         6.7042e-02,  1.1694e-01,  6.7972e-02,  1.6781e-01, -6.5562e-02,\n",
              "                        -9.5939e-02,  2.4236e-03,  1.4424e-01,  1.1375e-01, -1.3100e-01,\n",
              "                        -4.0662e-02,  1.6375e-01,  6.5434e-04, -1.5132e-01, -1.7804e-01,\n",
              "                        -3.5405e-02,  5.6558e-02,  5.8139e-02],\n",
              "                       [-6.7129e-02,  1.6742e-01, -6.8631e-02,  9.9985e-02,  1.6806e-01,\n",
              "                        -1.6519e-02, -3.6251e-02,  8.4034e-02, -7.1703e-02,  3.4220e-02,\n",
              "                         2.0533e-02, -9.7239e-02,  9.3909e-02,  6.7471e-02, -1.8121e-01,\n",
              "                         1.2026e-01, -1.5478e-01,  1.0458e-01, -1.7752e-01, -1.8384e-01,\n",
              "                         1.1820e-01,  5.3962e-02, -1.5179e-01,  4.1860e-02,  5.0610e-02,\n",
              "                         5.1396e-02,  5.3285e-02, -1.3456e-01],\n",
              "                       [-9.2278e-02, -4.8713e-02, -7.8022e-02,  7.3114e-04,  5.5282e-02,\n",
              "                         6.7566e-02,  4.6339e-02,  1.4695e-01,  6.5161e-02, -1.1878e-01,\n",
              "                         1.4224e-01,  2.1468e-02, -2.1540e-02, -3.3145e-02,  4.1247e-02,\n",
              "                        -5.8217e-02, -1.2030e-01, -3.9640e-02,  7.5715e-03,  1.5403e-01,\n",
              "                        -1.8557e-01,  6.3214e-02, -5.8938e-02, -5.8912e-02,  7.9289e-03,\n",
              "                         1.0826e-01, -3.0627e-02, -1.8553e-01],\n",
              "                       [-1.3086e-01,  4.4005e-02, -3.7177e-02,  1.2538e-01,  9.8451e-02,\n",
              "                         1.4627e-01,  1.6460e-01,  1.3174e-01,  9.0403e-02, -1.5482e-01,\n",
              "                        -2.9166e-02, -1.4529e-01,  1.3920e-01,  5.3163e-02, -1.4186e-01,\n",
              "                         1.3546e-01,  7.2471e-02, -4.7366e-02, -5.5368e-03,  1.0096e-01,\n",
              "                        -4.2863e-03, -2.7136e-02, -1.0136e-01,  6.2571e-02, -1.7745e-01,\n",
              "                        -8.9062e-02,  1.2872e-02, -1.3096e-01],\n",
              "                       [ 9.1800e-02,  6.7998e-02, -2.2470e-02,  8.4949e-02,  1.5501e-01,\n",
              "                         4.6685e-03,  3.5156e-02,  1.3699e-01, -1.3191e-01, -3.0456e-02,\n",
              "                         4.6392e-02,  2.4296e-02, -3.8194e-02,  5.3231e-02,  1.2270e-01,\n",
              "                         1.5976e-01,  1.8080e-01, -7.3655e-02,  2.6097e-02,  1.0445e-02,\n",
              "                         4.3095e-02, -3.8965e-02, -6.2559e-02, -1.9232e-02, -1.2271e-01,\n",
              "                         2.3239e-02,  1.4202e-01,  4.9379e-02],\n",
              "                       [-3.7438e-03,  1.3110e-01, -6.1493e-02,  7.3244e-02,  1.1215e-01,\n",
              "                        -1.6983e-01,  3.6531e-02, -2.8621e-03,  4.8825e-02, -3.1724e-02,\n",
              "                        -1.6553e-01,  1.4871e-02, -6.1567e-02, -4.7734e-02, -1.4007e-01,\n",
              "                         1.3590e-01, -7.9603e-02,  1.7573e-01,  1.4365e-01,  9.4996e-02,\n",
              "                         1.4917e-01,  1.4955e-01,  1.4643e-01, -1.1181e-01,  5.9827e-02,\n",
              "                         8.7008e-02, -7.6375e-02, -1.4256e-01],\n",
              "                       [ 1.5941e-02,  2.4697e-03, -9.3594e-02, -1.3387e-01, -1.7759e-01,\n",
              "                         1.0961e-01,  1.5231e-01, -1.6606e-01, -1.0215e-01,  5.4055e-02,\n",
              "                         6.2030e-02, -1.8082e-01,  1.8890e-01,  1.7643e-01,  7.1361e-02,\n",
              "                         1.3427e-02, -1.1428e-01, -1.0813e-01,  1.8813e-01,  5.7901e-02,\n",
              "                         1.4761e-01, -1.6669e-01, -1.0904e-01, -1.6431e-01,  1.2907e-01,\n",
              "                         9.2977e-02, -1.8685e-01, -3.0558e-03],\n",
              "                       [ 1.6337e-01, -1.4141e-01,  1.3651e-01, -1.1400e-01,  4.1746e-02,\n",
              "                         1.4827e-01, -1.0452e-01, -1.6920e-01,  1.6672e-01, -1.5165e-01,\n",
              "                        -7.9784e-03,  1.3520e-01,  1.6206e-01,  3.1645e-02, -8.4848e-02,\n",
              "                        -1.5687e-01, -1.1304e-01,  9.4725e-02, -4.2397e-02, -1.1691e-01,\n",
              "                        -7.1288e-02, -6.5211e-02, -8.8788e-02,  1.0199e-01,  5.7781e-02,\n",
              "                        -1.6919e-01,  1.1648e-01, -1.4673e-01],\n",
              "                       [ 1.3833e-01,  9.1462e-02, -1.4160e-01,  1.8653e-01,  4.1611e-02,\n",
              "                        -6.6101e-02, -1.3203e-01, -1.4869e-01, -4.5869e-02,  1.5872e-01,\n",
              "                        -9.0266e-02,  1.8639e-01,  6.6260e-02, -1.7820e-02, -1.1127e-01,\n",
              "                         1.6263e-01,  1.5806e-01,  3.8096e-02,  1.3036e-01, -7.8408e-02,\n",
              "                         9.1797e-02, -1.8349e-01,  1.3791e-01, -1.6371e-01,  1.2015e-01,\n",
              "                        -4.2151e-02, -8.0570e-02, -4.8069e-03],\n",
              "                       [ 1.5525e-01,  1.1288e-01, -1.0600e-02, -7.6592e-02, -1.4611e-01,\n",
              "                         4.0658e-02, -1.6507e-01,  1.6439e-01,  1.8756e-01, -1.2634e-01,\n",
              "                        -1.5800e-01,  1.7264e-01, -1.6462e-01, -7.8703e-02, -1.5406e-01,\n",
              "                         1.6768e-01,  1.3949e-01, -1.2517e-01,  1.6195e-02, -5.0453e-03,\n",
              "                         1.1131e-01,  1.8247e-01,  3.2742e-02, -7.9981e-02,  9.0219e-02,\n",
              "                         9.3838e-02,  9.6861e-02,  1.6300e-01],\n",
              "                       [ 3.5203e-02,  1.2362e-01, -1.2357e-01, -2.8785e-02,  7.6744e-02,\n",
              "                         1.0236e-01, -1.8358e-01,  1.2908e-01, -3.1363e-02,  1.4295e-01,\n",
              "                        -1.1281e-01,  4.0419e-02, -8.4958e-02, -1.8543e-01, -5.0572e-04,\n",
              "                         1.3423e-01,  1.5674e-01, -1.4855e-01,  2.0342e-02, -1.8526e-02,\n",
              "                         1.5574e-01,  1.8326e-01, -6.1894e-03, -3.6693e-02, -1.4258e-01,\n",
              "                        -1.6846e-01,  1.7325e-01, -1.7437e-01],\n",
              "                       [ 1.5782e-01, -3.0657e-02, -8.9142e-02, -4.3090e-02, -2.5686e-02,\n",
              "                        -2.0244e-02, -1.6285e-01,  1.1740e-01,  1.3668e-01, -1.0889e-01,\n",
              "                         1.0594e-01, -1.2525e-01,  2.1427e-02, -1.5948e-01,  2.5126e-02,\n",
              "                         7.8086e-02,  2.9458e-02,  1.7852e-01,  9.7901e-02,  1.8899e-02,\n",
              "                        -1.4037e-01,  4.3837e-02,  4.7603e-03,  5.5961e-02, -5.3529e-02,\n",
              "                        -8.1783e-02,  1.6916e-01,  1.8855e-01],\n",
              "                       [-1.4134e-01, -1.6144e-01, -1.6938e-01, -1.7934e-01, -1.7610e-01,\n",
              "                        -1.1685e-01, -1.8373e-01,  1.5415e-01, -1.0543e-01, -1.4523e-01,\n",
              "                        -6.3694e-02,  4.7000e-02,  4.6927e-02, -3.1760e-02, -8.6554e-02,\n",
              "                        -5.5578e-02, -2.9374e-02, -8.4593e-02,  1.6033e-01, -1.5866e-01,\n",
              "                         4.4083e-02, -1.8387e-01, -1.6472e-01, -3.4746e-02, -8.0880e-02,\n",
              "                        -5.5150e-02, -8.9080e-02, -2.5794e-02],\n",
              "                       [ 1.6186e-01, -6.5215e-02,  5.6173e-02, -1.6770e-01,  8.0111e-02,\n",
              "                        -5.6534e-02, -8.5778e-02,  1.2785e-02, -1.0544e-01,  6.4992e-02,\n",
              "                        -8.9357e-02, -2.6925e-02,  6.5887e-02,  2.9496e-02, -1.1835e-01,\n",
              "                         3.8070e-02, -1.0330e-01, -2.3308e-02,  9.8845e-02, -1.6370e-01,\n",
              "                        -4.6290e-02,  4.0536e-02,  1.5179e-01,  9.0914e-02,  1.8386e-01,\n",
              "                         1.8289e-01, -1.1503e-02,  5.0314e-02],\n",
              "                       [-1.0624e-01, -7.1084e-02, -5.4682e-03,  1.5024e-01,  1.3713e-01,\n",
              "                         1.4074e-01,  1.5683e-01,  1.8298e-01, -3.0571e-02,  8.3641e-02,\n",
              "                         9.6700e-02, -4.3470e-02, -1.8391e-01, -1.8135e-01,  9.3645e-02,\n",
              "                        -7.4000e-02,  1.7708e-01, -1.3242e-01, -9.6273e-03, -1.6950e-01,\n",
              "                         1.8126e-01,  1.1070e-01, -1.2884e-02, -1.9065e-03,  2.3158e-02,\n",
              "                        -4.5001e-03, -1.1323e-01, -5.7852e-03],\n",
              "                       [ 7.8934e-02, -1.4720e-01, -4.5388e-02, -7.3216e-02,  1.0213e-01,\n",
              "                        -1.8809e-01,  3.2239e-02, -1.3537e-01,  6.3886e-02,  1.1326e-01,\n",
              "                        -5.1406e-02, -9.1866e-02, -5.8805e-02, -1.2015e-01, -1.3186e-01,\n",
              "                         1.8004e-01, -1.1983e-01,  8.2985e-02, -8.5871e-02,  1.4452e-01,\n",
              "                        -8.2587e-02, -7.0807e-02, -8.7166e-02,  1.7221e-01,  4.3889e-02,\n",
              "                         1.0379e-01, -5.2017e-02, -4.2355e-02],\n",
              "                       [-3.1305e-02,  1.7667e-01, -1.8382e-01, -4.9663e-02, -6.6714e-02,\n",
              "                         1.2904e-01, -9.0185e-02, -1.7728e-01, -1.5459e-01,  1.0519e-01,\n",
              "                        -5.0794e-02,  1.1741e-01,  6.1746e-02,  9.0709e-02, -1.3141e-01,\n",
              "                         7.0039e-02, -1.1083e-01, -1.3242e-01, -1.6005e-01, -1.5545e-01,\n",
              "                         1.4337e-01,  1.2486e-01,  1.5125e-01, -1.5912e-01, -3.5015e-02,\n",
              "                         2.5911e-02,  3.1384e-02, -1.7619e-01],\n",
              "                       [-1.6333e-02, -8.3395e-02,  2.3225e-02, -3.1769e-02, -1.5996e-01,\n",
              "                        -7.1111e-02, -1.7888e-01,  1.2321e-01,  1.7725e-01,  1.4983e-01,\n",
              "                        -7.3273e-02, -1.5233e-01,  4.4638e-02,  1.6978e-01,  1.3078e-01,\n",
              "                         8.4661e-02,  1.1775e-01,  1.4868e-01, -1.3525e-02,  5.5978e-02,\n",
              "                         1.1958e-01,  3.8305e-02, -1.5142e-01,  1.2942e-01, -4.8635e-02,\n",
              "                         9.8331e-02, -9.4717e-02,  1.8207e-01],\n",
              "                       [-9.7679e-02,  6.4070e-02,  3.9721e-02, -1.7600e-01, -9.9955e-02,\n",
              "                        -5.0951e-02, -1.6008e-01, -9.6970e-02,  8.6796e-02, -1.0296e-01,\n",
              "                        -5.0088e-02, -3.9686e-02, -5.2171e-03,  3.3689e-02,  1.6705e-01,\n",
              "                         5.3193e-02, -9.2928e-02, -1.6601e-01,  1.2724e-01, -1.7878e-01,\n",
              "                         3.4123e-03,  2.9604e-02,  2.6285e-02, -1.8549e-01,  1.2302e-01,\n",
              "                         1.6210e-01,  1.2072e-01,  4.2018e-02],\n",
              "                       [-1.5408e-01,  1.4603e-01, -1.6053e-01, -5.8849e-02, -1.8519e-01,\n",
              "                        -1.1495e-01,  1.3996e-01,  1.1488e-01,  1.5638e-01,  1.4324e-01,\n",
              "                         9.6529e-02, -2.8756e-02,  9.0341e-03, -1.1012e-01, -5.1798e-02,\n",
              "                        -8.7634e-02, -1.0708e-02,  6.1770e-02, -6.2019e-02,  3.6938e-02,\n",
              "                         1.4427e-01,  3.9001e-02,  1.5120e-01, -9.5053e-02, -1.0776e-01,\n",
              "                        -9.2425e-02, -1.1836e-01, -2.7019e-02],\n",
              "                       [-1.2480e-01, -1.7843e-02,  1.7072e-01,  4.0431e-02, -1.6138e-01,\n",
              "                        -3.4458e-02,  6.3689e-02,  7.2400e-02,  1.6385e-01,  3.6804e-02,\n",
              "                         1.3597e-03, -5.1838e-02, -4.4841e-02,  1.3109e-01,  8.5660e-03,\n",
              "                        -1.1778e-01,  9.3580e-02,  1.2989e-01,  1.8192e-01,  6.5199e-02,\n",
              "                        -6.1039e-02, -2.8242e-02,  1.8391e-01,  1.8530e-01, -7.4349e-02,\n",
              "                         1.5914e-01,  5.4529e-02, -1.5510e-02],\n",
              "                       [-8.4440e-02,  7.0143e-02, -9.9549e-02,  9.1596e-02, -1.4495e-01,\n",
              "                        -6.2614e-02, -1.3303e-01, -9.9223e-02,  7.6035e-02,  1.3544e-01,\n",
              "                         3.5447e-02,  1.6934e-01, -2.3418e-02, -3.4257e-02, -3.4033e-02,\n",
              "                        -3.5528e-02,  9.0158e-03,  4.1049e-02, -7.7832e-02, -2.7002e-03,\n",
              "                        -1.9373e-02,  8.0104e-02,  5.0453e-02,  1.9099e-03,  5.0187e-02,\n",
              "                         1.3842e-01, -1.3951e-01, -1.8028e-01],\n",
              "                       [-1.2851e-02, -1.1223e-01,  8.3780e-02, -8.8009e-02, -3.5012e-02,\n",
              "                         9.6797e-02, -4.7959e-02,  4.2686e-02,  1.6976e-01,  1.2532e-02,\n",
              "                        -1.0921e-01, -1.6140e-01, -1.8703e-01, -1.0290e-01, -1.4792e-01,\n",
              "                         4.1247e-02,  9.3207e-02, -1.2435e-01, -9.9255e-02,  4.9587e-02,\n",
              "                        -5.6109e-02, -1.2528e-01,  1.8862e-01,  4.6482e-02, -1.9794e-02,\n",
              "                        -4.2144e-02, -9.2093e-02, -9.7557e-02],\n",
              "                       [ 1.3257e-01, -4.4348e-02, -5.1977e-02,  9.2206e-02,  4.3020e-02,\n",
              "                         8.0810e-02,  1.1496e-01, -1.5423e-01, -1.1236e-01,  1.8644e-01,\n",
              "                         5.8811e-02, -1.6998e-01,  6.2328e-02, -1.5682e-01, -1.7806e-01,\n",
              "                         2.2741e-02, -4.5819e-03,  6.5826e-02, -1.7145e-01, -8.4035e-02,\n",
              "                        -1.0265e-01, -1.7296e-01, -7.1314e-02,  1.5971e-01, -4.7017e-02,\n",
              "                         9.5987e-02, -1.4108e-01, -1.5959e-01]])),\n",
              "              ('encoder_convs.0.lin_u.bias',\n",
              "               tensor([-0.0194, -0.1555,  0.0375, -0.0865,  0.1242,  0.0995, -0.0234, -0.0360,\n",
              "                       -0.0631,  0.1855, -0.1550, -0.1035, -0.0527,  0.1028, -0.1476, -0.1043,\n",
              "                       -0.0971, -0.0852,  0.0801,  0.0755,  0.0065,  0.0192,  0.1595, -0.0662,\n",
              "                        0.1842,  0.0130,  0.1541, -0.0597, -0.0620,  0.0547, -0.1219, -0.1334])),\n",
              "              ('encoder_convs.0.lin_v.weight',\n",
              "               tensor([[-0.0423, -0.0481, -0.1055,  0.0094,  0.0141,  0.1372, -0.1371, -0.0830,\n",
              "                         0.1147, -0.1505,  0.0916, -0.1292,  0.0393, -0.1875,  0.0843, -0.1858,\n",
              "                        -0.1332,  0.0103, -0.1189, -0.0148,  0.1615, -0.0484, -0.0927, -0.1109,\n",
              "                         0.0367,  0.1023,  0.0935, -0.0592],\n",
              "                       [ 0.0846,  0.0314,  0.0167, -0.0100, -0.0459, -0.1300, -0.0213,  0.0854,\n",
              "                        -0.0114,  0.0253, -0.1688,  0.0576, -0.1384,  0.1614,  0.1241, -0.0835,\n",
              "                         0.1277,  0.1865, -0.0065, -0.1693,  0.0084,  0.1874, -0.0465,  0.1276,\n",
              "                         0.1379, -0.1739,  0.0064,  0.1803],\n",
              "                       [-0.0315,  0.0246, -0.0086,  0.0750, -0.1626,  0.1252, -0.0798,  0.1023,\n",
              "                         0.0078,  0.0246,  0.1458, -0.0635,  0.0145,  0.0770,  0.0397, -0.1587,\n",
              "                         0.1062,  0.0160,  0.0602,  0.0987,  0.1678,  0.1571,  0.0009,  0.1614,\n",
              "                        -0.1064,  0.0333, -0.0971, -0.1486],\n",
              "                       [-0.0668,  0.0444, -0.1125, -0.1558,  0.1120, -0.1307, -0.1739, -0.1565,\n",
              "                         0.1463,  0.0288, -0.1445,  0.0626, -0.0621, -0.1091,  0.1620, -0.0753,\n",
              "                        -0.1148,  0.1879,  0.1462,  0.1008,  0.0719, -0.1482,  0.1278, -0.0308,\n",
              "                         0.0306,  0.0821,  0.0888,  0.0229],\n",
              "                       [-0.0074,  0.1747, -0.0609, -0.1177,  0.0334, -0.1732, -0.1293, -0.0334,\n",
              "                         0.0207, -0.1818,  0.0017,  0.1129, -0.0287,  0.0200, -0.0891, -0.1151,\n",
              "                        -0.1145,  0.0213,  0.0365,  0.0525,  0.0975, -0.1070, -0.0224, -0.0093,\n",
              "                         0.0142, -0.1113, -0.1526,  0.1776],\n",
              "                       [ 0.1726, -0.0218,  0.0541,  0.0411, -0.1738,  0.0630,  0.1532,  0.0935,\n",
              "                         0.0444,  0.1176, -0.0344, -0.0548, -0.1530, -0.0947, -0.0545, -0.1648,\n",
              "                         0.1876,  0.0933,  0.1127,  0.1835,  0.1887,  0.1428,  0.1557, -0.1143,\n",
              "                         0.1676, -0.1255,  0.0451,  0.1645],\n",
              "                       [-0.0487, -0.1780, -0.1799, -0.1669, -0.0866, -0.1239,  0.0370, -0.1101,\n",
              "                        -0.0004, -0.0145,  0.0922, -0.1213,  0.1062, -0.1382, -0.0124,  0.0986,\n",
              "                        -0.1087,  0.0815, -0.0540, -0.0965,  0.1064, -0.1288, -0.0536,  0.0109,\n",
              "                         0.1621,  0.1467, -0.0207,  0.1127],\n",
              "                       [-0.0677,  0.0396, -0.0748,  0.0262,  0.0143, -0.1462,  0.0496,  0.1160,\n",
              "                        -0.0338,  0.1864,  0.0379,  0.0852, -0.1716,  0.0333, -0.0438,  0.0882,\n",
              "                        -0.0753, -0.0392,  0.0065, -0.0561,  0.0621,  0.1029, -0.1054, -0.0291,\n",
              "                         0.0322, -0.1582,  0.0679, -0.1748],\n",
              "                       [ 0.0025, -0.1405,  0.1414,  0.0344,  0.0696,  0.1740,  0.1574,  0.0054,\n",
              "                        -0.1044, -0.1426, -0.1382, -0.1215, -0.0975, -0.0433,  0.1826, -0.0678,\n",
              "                         0.1442, -0.0568, -0.0497, -0.1632, -0.1861, -0.1726, -0.1075,  0.1591,\n",
              "                         0.1248, -0.0737, -0.1884, -0.0787],\n",
              "                       [-0.0802,  0.1276, -0.1213, -0.0445, -0.1698, -0.1788, -0.1056, -0.0510,\n",
              "                         0.1048, -0.1115, -0.0459,  0.0921,  0.1446,  0.1419,  0.0816, -0.0620,\n",
              "                        -0.0859,  0.0928, -0.0932,  0.0517, -0.1513,  0.1637, -0.1577,  0.0494,\n",
              "                         0.0376,  0.0988,  0.1674, -0.1611],\n",
              "                       [ 0.0586, -0.1768, -0.0983, -0.0715,  0.0621, -0.1550,  0.1224,  0.0110,\n",
              "                         0.0550,  0.0032,  0.1105,  0.0085, -0.1297, -0.0382,  0.0584,  0.0047,\n",
              "                        -0.1722, -0.0589, -0.0740, -0.0817,  0.1408,  0.0605,  0.1448, -0.0456,\n",
              "                        -0.0246, -0.1421, -0.0493, -0.0222],\n",
              "                       [ 0.1427,  0.1286,  0.0566, -0.0202, -0.1027, -0.0586,  0.1779,  0.0280,\n",
              "                         0.0059,  0.1246, -0.0787, -0.1125,  0.0931,  0.0864,  0.1618, -0.1148,\n",
              "                        -0.0763, -0.1062, -0.0326, -0.1392, -0.1056,  0.1150, -0.1359, -0.0731,\n",
              "                        -0.1332, -0.0639,  0.0386,  0.0512],\n",
              "                       [-0.0043, -0.1092,  0.0833, -0.0198,  0.1123, -0.1536,  0.0647,  0.1880,\n",
              "                         0.1107,  0.1380,  0.1731,  0.1161, -0.1650,  0.0316, -0.0776,  0.0840,\n",
              "                         0.1400,  0.1059,  0.0264,  0.1049,  0.1140, -0.0770,  0.0961, -0.1873,\n",
              "                         0.0489, -0.0113,  0.1171, -0.0445],\n",
              "                       [ 0.0456, -0.1062,  0.1304, -0.0862,  0.1070,  0.1151,  0.0180, -0.1554,\n",
              "                        -0.0485,  0.1519, -0.1144, -0.0488, -0.1227,  0.1418, -0.1029, -0.1706,\n",
              "                         0.1398, -0.0514, -0.1389, -0.1804, -0.1178,  0.1846, -0.0411, -0.0227,\n",
              "                        -0.1410,  0.0134,  0.1759, -0.1666],\n",
              "                       [ 0.0042, -0.0486, -0.1276, -0.0511,  0.1388, -0.1876, -0.1551, -0.0441,\n",
              "                        -0.1449,  0.0625,  0.0777,  0.0008, -0.1519,  0.0561, -0.0279,  0.1017,\n",
              "                        -0.0978,  0.0743, -0.1662,  0.0936,  0.0487, -0.1338, -0.1287,  0.1434,\n",
              "                         0.0846, -0.1683, -0.1089, -0.0884],\n",
              "                       [-0.1659, -0.1174,  0.1208, -0.0192, -0.0749, -0.0096, -0.1760, -0.0955,\n",
              "                        -0.1875,  0.0855, -0.0738, -0.0829, -0.1583, -0.0226, -0.0750, -0.1238,\n",
              "                         0.1717, -0.0280,  0.1443,  0.1748, -0.0189, -0.1487,  0.0753,  0.1268,\n",
              "                         0.0669, -0.0213, -0.0736,  0.1318],\n",
              "                       [-0.1748,  0.0575, -0.1801,  0.1453, -0.0128,  0.1509, -0.1749,  0.1431,\n",
              "                        -0.0221, -0.0574, -0.1221, -0.1855,  0.1397,  0.0610, -0.1755,  0.0278,\n",
              "                        -0.1108,  0.0804,  0.1841,  0.0952,  0.0518, -0.1380, -0.1512, -0.1716,\n",
              "                         0.0133, -0.0070, -0.1125,  0.1560],\n",
              "                       [-0.0811,  0.0003, -0.1359, -0.0409,  0.1531,  0.1784, -0.0019,  0.0419,\n",
              "                        -0.0031,  0.0741,  0.0483,  0.1822,  0.1266, -0.1039,  0.0226, -0.0297,\n",
              "                        -0.1831,  0.1295, -0.0910,  0.1053,  0.1513, -0.1337, -0.0263,  0.1598,\n",
              "                         0.1322, -0.0095, -0.0981, -0.0972],\n",
              "                       [-0.0543, -0.1660, -0.1769, -0.1632,  0.0636,  0.0753, -0.0763,  0.0363,\n",
              "                        -0.0305, -0.1479,  0.1046, -0.0447,  0.0350, -0.0483, -0.1665,  0.0042,\n",
              "                        -0.0826, -0.1043, -0.0013,  0.0322, -0.0118, -0.1095, -0.1674,  0.1846,\n",
              "                        -0.0030, -0.1036,  0.0184, -0.0360],\n",
              "                       [ 0.0416,  0.1236, -0.0585, -0.0241,  0.0429,  0.1781, -0.0717, -0.0872,\n",
              "                        -0.0460,  0.0235,  0.1158, -0.0424,  0.1206, -0.1564,  0.0374, -0.1887,\n",
              "                        -0.0305, -0.0137,  0.0459,  0.1352,  0.0803, -0.1673,  0.1733, -0.1888,\n",
              "                        -0.0875,  0.1200, -0.0660, -0.1556],\n",
              "                       [ 0.0133, -0.1708,  0.1515,  0.1573, -0.0485, -0.1364,  0.0460, -0.0642,\n",
              "                        -0.0516, -0.0410,  0.1747, -0.1060, -0.0446,  0.0154, -0.1098, -0.0028,\n",
              "                        -0.0868,  0.0269,  0.1508,  0.0554,  0.1614, -0.1402,  0.0855,  0.0042,\n",
              "                         0.0712,  0.0454, -0.0989, -0.0092],\n",
              "                       [-0.1857, -0.0260, -0.0816, -0.1053,  0.1005,  0.0034, -0.1497, -0.1081,\n",
              "                         0.1276, -0.1640,  0.0169, -0.0889, -0.0012, -0.0653,  0.0003, -0.1608,\n",
              "                        -0.1404, -0.1022, -0.1161,  0.0167, -0.1280,  0.0601,  0.1526,  0.0788,\n",
              "                        -0.0836,  0.1242, -0.0826, -0.0947],\n",
              "                       [ 0.1489,  0.1838, -0.1639,  0.0443, -0.1003, -0.0719, -0.0199, -0.1016,\n",
              "                         0.0411,  0.0930, -0.1653, -0.1046,  0.0910, -0.0619,  0.1364,  0.0003,\n",
              "                         0.0316,  0.1208, -0.1764, -0.1378, -0.1841, -0.1141, -0.1279,  0.0581,\n",
              "                        -0.1485, -0.1806,  0.0105, -0.0127],\n",
              "                       [-0.0468,  0.0549,  0.0675, -0.1749, -0.0663, -0.1097,  0.1217, -0.1807,\n",
              "                         0.1697,  0.1446,  0.0111, -0.0436,  0.0588,  0.0054, -0.1309, -0.0329,\n",
              "                         0.0826,  0.1420, -0.0510, -0.0985,  0.1236,  0.0317,  0.0735, -0.1043,\n",
              "                        -0.1030,  0.0149,  0.1240, -0.1568],\n",
              "                       [ 0.0327,  0.0258, -0.1605,  0.0047,  0.0460, -0.0423, -0.0325, -0.0046,\n",
              "                         0.0141,  0.1299,  0.0904, -0.0814, -0.1087, -0.1008,  0.0971,  0.0173,\n",
              "                        -0.1646, -0.1199, -0.0431,  0.0113, -0.0930,  0.1198,  0.1239, -0.0911,\n",
              "                         0.0938, -0.1714,  0.0688,  0.0520],\n",
              "                       [ 0.1545, -0.0873, -0.0336, -0.0716,  0.0923,  0.1810, -0.0484, -0.0509,\n",
              "                         0.0901, -0.0519,  0.0627,  0.1352,  0.0251, -0.1213,  0.1558,  0.1487,\n",
              "                        -0.1859, -0.0880,  0.1594,  0.1721,  0.0901,  0.1174,  0.0895, -0.0870,\n",
              "                        -0.0025,  0.0526, -0.0024, -0.0004],\n",
              "                       [ 0.0432, -0.0451,  0.1793,  0.0022,  0.1233, -0.1885, -0.1842, -0.0353,\n",
              "                         0.1849, -0.1348,  0.1745,  0.0305, -0.1599,  0.0629, -0.0716,  0.0383,\n",
              "                        -0.1564,  0.1471,  0.0992,  0.0505,  0.1527, -0.1581, -0.1023,  0.1033,\n",
              "                         0.0919, -0.1490, -0.0596,  0.0822],\n",
              "                       [ 0.0358, -0.1690, -0.1879, -0.1130,  0.0108,  0.1145, -0.0198, -0.0949,\n",
              "                         0.0134,  0.1561,  0.0090, -0.0833,  0.1504,  0.0849,  0.0772, -0.0514,\n",
              "                        -0.0250, -0.1460, -0.1659,  0.0181,  0.0311,  0.1632, -0.0067,  0.0076,\n",
              "                         0.0495,  0.1744, -0.0024,  0.0194],\n",
              "                       [ 0.1450,  0.0597, -0.0500, -0.0155,  0.0119,  0.1426, -0.0125, -0.0818,\n",
              "                         0.0996, -0.1268,  0.1528,  0.1050,  0.1857,  0.1673,  0.1440, -0.1476,\n",
              "                        -0.1628, -0.0553, -0.0347,  0.0406,  0.0701, -0.1859, -0.1368, -0.1858,\n",
              "                         0.0623, -0.1534, -0.0686, -0.0784],\n",
              "                       [ 0.1655, -0.0282, -0.1680, -0.1015,  0.0601,  0.0775,  0.1130, -0.0855,\n",
              "                        -0.1018,  0.0677,  0.1300, -0.1742, -0.0371,  0.1441,  0.0113, -0.1165,\n",
              "                        -0.1372,  0.0832,  0.0898, -0.1574,  0.0533,  0.1118,  0.1031, -0.0139,\n",
              "                        -0.1487,  0.0945,  0.0384, -0.1413],\n",
              "                       [ 0.0287, -0.0493, -0.0056, -0.0372, -0.0115,  0.0998,  0.0373,  0.0983,\n",
              "                         0.1524,  0.1133,  0.1399,  0.1124, -0.1193,  0.1768, -0.0023,  0.0750,\n",
              "                         0.1168,  0.0700, -0.0753,  0.1149,  0.1218, -0.0320,  0.1488, -0.1755,\n",
              "                        -0.0635,  0.0031, -0.0932,  0.0835],\n",
              "                       [ 0.1857, -0.1619, -0.0523, -0.1359, -0.0411, -0.0228, -0.0046, -0.1243,\n",
              "                        -0.0857,  0.1406,  0.1337,  0.1486,  0.0518,  0.1175,  0.0616, -0.1758,\n",
              "                        -0.0099, -0.0334,  0.1593, -0.0610,  0.0871, -0.1118,  0.1402,  0.0560,\n",
              "                        -0.0091,  0.0492,  0.0082,  0.0165]])),\n",
              "              ('encoder_convs.0.lin_v.bias',\n",
              "               tensor([ 0.0023,  0.1259,  0.0869, -0.0679, -0.1041, -0.1609, -0.1224, -0.1783,\n",
              "                       -0.0822,  0.0565,  0.1109,  0.0834,  0.0769, -0.1322, -0.1731, -0.1385,\n",
              "                       -0.1663,  0.0110,  0.0449, -0.1698,  0.0822,  0.1401, -0.0570,  0.0655,\n",
              "                       -0.0714, -0.1103, -0.0775,  0.0622, -0.0844,  0.0366, -0.1148,  0.1422])),\n",
              "              ('encoder_convs.0.lin_e.weight',\n",
              "               tensor([[-1.1867e-01,  2.2305e-01, -2.0738e-01, -1.6230e-01,  1.2520e-02,\n",
              "                         1.9851e-02,  8.9376e-02, -2.3068e-01, -4.3186e-02, -1.2303e-01,\n",
              "                         9.3977e-02, -2.4107e-02, -1.2988e-01, -2.3481e-01,  1.3732e-01,\n",
              "                         1.8985e-01,  1.5951e-01],\n",
              "                       [ 7.4983e-02,  5.3769e-02, -2.0843e-01,  3.6862e-02, -1.0865e-01,\n",
              "                         1.3154e-01,  1.7834e-01,  1.8927e-01, -2.0377e-04, -1.4812e-01,\n",
              "                         6.1953e-03, -8.2164e-02,  1.7219e-01,  2.0949e-01, -1.2331e-01,\n",
              "                         2.3803e-01,  7.1159e-02],\n",
              "                       [ 1.4450e-01, -2.1682e-01, -1.8435e-02,  3.0732e-02, -2.3591e-01,\n",
              "                        -1.0483e-01, -1.8113e-01, -2.3341e-01,  2.3330e-01, -2.3867e-01,\n",
              "                        -1.1722e-01,  1.7844e-01,  7.4507e-02, -1.4534e-01, -2.1611e-02,\n",
              "                        -1.9502e-01, -1.5168e-01],\n",
              "                       [ 2.2746e-01,  2.0591e-01, -1.0464e-01, -3.0537e-02, -1.6092e-01,\n",
              "                        -1.8642e-02, -1.1248e-01, -2.3776e-01,  1.4587e-01, -1.2992e-01,\n",
              "                         1.8184e-01,  1.7034e-01,  9.9143e-02, -2.2283e-01,  7.8559e-02,\n",
              "                        -6.5137e-02, -1.0805e-01],\n",
              "                       [ 2.3099e-01,  3.9360e-02,  1.2904e-01,  7.3077e-02,  1.2517e-01,\n",
              "                        -2.7152e-02, -2.3498e-01,  3.7099e-02, -7.8788e-03,  1.2592e-01,\n",
              "                         8.4602e-02,  1.3842e-01, -2.4035e-01, -1.9456e-01, -1.3265e-01,\n",
              "                        -2.2648e-01,  5.0573e-02],\n",
              "                       [-7.9194e-02,  2.4135e-02,  3.0371e-02, -2.1111e-02, -8.4555e-03,\n",
              "                         2.3686e-01,  1.1141e-01, -6.7156e-02, -2.1892e-01,  1.1856e-01,\n",
              "                        -1.0492e-01, -1.5364e-02, -6.1415e-02, -1.0503e-01,  4.4851e-02,\n",
              "                        -4.1940e-02,  6.1133e-02],\n",
              "                       [ 2.0612e-01,  1.8033e-01,  2.1450e-02, -1.4860e-01,  1.7411e-01,\n",
              "                        -2.0558e-01,  1.7150e-01,  1.7018e-01,  2.0280e-01,  1.0755e-01,\n",
              "                        -3.2355e-02, -1.0805e-01, -6.6596e-02, -1.9024e-01,  8.8342e-02,\n",
              "                        -1.3237e-01, -1.0135e-01],\n",
              "                       [ 2.1477e-01,  1.5042e-01, -2.0293e-01, -3.1344e-02, -9.5045e-02,\n",
              "                         1.1075e-01, -1.5019e-01, -9.0746e-02,  3.8073e-02, -9.1977e-02,\n",
              "                         2.4197e-01,  1.8388e-02,  3.3368e-02,  1.0186e-02, -1.6470e-01,\n",
              "                        -3.8975e-02, -2.2905e-01],\n",
              "                       [-1.8748e-01,  1.8707e-01,  2.3622e-02,  4.7204e-02, -2.7276e-02,\n",
              "                         1.5160e-01,  9.3583e-02, -2.6253e-02,  7.9753e-02,  4.4105e-02,\n",
              "                         1.8988e-02,  3.8990e-02,  1.4884e-01,  2.2455e-02,  1.5825e-01,\n",
              "                         1.5135e-01, -1.9230e-01],\n",
              "                       [ 1.8711e-01, -1.9886e-01,  2.2533e-01,  4.8608e-02,  1.9199e-01,\n",
              "                         2.4069e-01,  1.8980e-02, -2.3474e-01,  5.9437e-02,  2.1422e-01,\n",
              "                         1.7934e-01,  5.6284e-02,  2.2419e-04,  2.1334e-01,  1.0850e-01,\n",
              "                        -1.0223e-02, -1.5477e-01],\n",
              "                       [-1.9960e-01,  2.8274e-02,  1.7537e-01, -1.2152e-01,  1.6509e-01,\n",
              "                         5.4956e-02, -1.3022e-02,  2.2894e-01,  2.4409e-02,  3.4069e-02,\n",
              "                         5.0420e-02,  1.4664e-01,  2.3290e-01, -7.0732e-02, -6.4642e-02,\n",
              "                         5.1068e-02, -1.5541e-01],\n",
              "                       [ 4.9561e-02,  1.5546e-01,  6.4668e-02,  1.5915e-01,  2.2103e-01,\n",
              "                        -1.3273e-01,  4.0891e-02,  4.8836e-02,  1.3765e-01,  5.9339e-02,\n",
              "                        -1.1635e-01, -1.9378e-02, -8.8557e-02,  1.2006e-01,  4.1664e-05,\n",
              "                         1.7676e-01,  2.1649e-01],\n",
              "                       [ 2.9020e-02, -1.5440e-01,  1.3110e-02,  6.6903e-02, -7.4545e-03,\n",
              "                        -3.4555e-02,  9.4904e-02,  2.8926e-02, -1.3681e-01, -3.9335e-02,\n",
              "                        -1.4588e-01, -1.8515e-01, -1.2133e-02, -1.1046e-01, -1.2216e-01,\n",
              "                        -9.4160e-02, -2.9572e-02],\n",
              "                       [ 4.1930e-03, -1.5930e-02,  2.1697e-01, -1.3668e-01, -6.7195e-02,\n",
              "                         2.3042e-01, -8.1087e-02,  9.4015e-02,  1.2248e-01, -2.3926e-01,\n",
              "                        -1.0996e-03, -1.2207e-01,  3.7786e-02,  1.8435e-01, -1.7455e-02,\n",
              "                        -1.6307e-01, -2.2588e-01],\n",
              "                       [-5.8505e-02,  1.0785e-01, -1.2924e-02, -2.1256e-01,  1.9909e-01,\n",
              "                        -2.0774e-02, -4.9774e-02, -6.6156e-02, -6.5327e-02, -1.5113e-01,\n",
              "                         2.3693e-01, -1.9613e-01, -1.9296e-01, -1.4074e-01,  1.7106e-01,\n",
              "                         7.6764e-02, -2.2512e-01],\n",
              "                       [-2.1694e-01, -9.2839e-02,  1.2734e-01,  2.0102e-01, -1.8924e-01,\n",
              "                         1.4297e-01, -6.4931e-02, -1.4377e-01, -2.1530e-01,  2.0265e-01,\n",
              "                        -1.1524e-01,  6.1425e-02,  1.2387e-01,  5.7487e-02, -1.7858e-01,\n",
              "                        -6.6852e-02,  1.2382e-01],\n",
              "                       [-1.0050e-01,  1.7721e-01,  7.6960e-02, -9.2816e-02, -1.9903e-01,\n",
              "                         3.5222e-02,  1.3079e-01, -2.3488e-01, -1.5138e-04, -1.9490e-01,\n",
              "                        -1.5125e-01,  1.8262e-01,  1.7207e-03, -1.1714e-01,  3.1940e-02,\n",
              "                        -3.8083e-02,  9.2524e-02],\n",
              "                       [-2.0561e-01, -1.1459e-01,  2.3861e-01, -1.2387e-01, -1.9864e-01,\n",
              "                        -5.9459e-02,  9.4716e-02, -1.4420e-01, -1.7151e-01, -6.3205e-02,\n",
              "                         1.6420e-01,  1.4328e-01, -1.3110e-01,  2.1749e-01,  2.4795e-03,\n",
              "                        -1.8118e-01,  5.1172e-02],\n",
              "                       [ 9.4291e-02,  1.4817e-02,  4.5201e-02, -3.6480e-02, -2.2463e-01,\n",
              "                        -2.1626e-01,  2.3642e-02,  1.6485e-01,  1.1997e-01,  2.0223e-01,\n",
              "                         2.2249e-01, -1.1894e-01, -9.0580e-02, -1.8559e-01,  6.4634e-02,\n",
              "                        -1.5759e-01, -7.9905e-03],\n",
              "                       [-1.1571e-01,  1.5891e-01,  2.3106e-01, -5.4015e-02,  1.6544e-01,\n",
              "                        -1.2792e-01,  1.1482e-01,  3.2467e-02,  6.3691e-02,  1.4049e-01,\n",
              "                        -2.3267e-01, -1.7190e-02, -9.5640e-02,  4.8969e-02, -1.9447e-01,\n",
              "                        -5.0209e-02,  1.3014e-01],\n",
              "                       [-1.0501e-01,  1.4973e-01,  1.0001e-01,  2.1623e-01, -1.4633e-01,\n",
              "                        -1.0184e-03, -1.6316e-01,  1.1710e-01, -9.8647e-02,  8.3152e-02,\n",
              "                        -1.4226e-01,  1.9943e-01, -1.7753e-01, -2.3879e-01, -1.0370e-01,\n",
              "                         1.4231e-01,  1.9325e-01],\n",
              "                       [ 2.1481e-01,  2.2586e-01,  2.1790e-03, -2.2837e-01, -7.1454e-02,\n",
              "                         1.7266e-01,  1.3950e-02,  1.6385e-01, -3.8419e-02, -2.7692e-02,\n",
              "                         1.3060e-01, -3.6350e-02,  1.3186e-01,  1.1377e-01,  9.9431e-02,\n",
              "                         2.1381e-01,  1.6129e-01],\n",
              "                       [-2.1962e-01, -1.1530e-01,  2.3027e-01, -1.4105e-01,  1.6356e-01,\n",
              "                         4.9814e-03,  1.7534e-01,  7.1937e-02,  1.5138e-01, -1.0809e-01,\n",
              "                        -1.6291e-01, -4.8628e-02,  5.8718e-02, -4.5281e-02,  7.0255e-02,\n",
              "                        -4.7965e-02, -1.0737e-01],\n",
              "                       [-7.0990e-02,  1.9750e-01, -9.9173e-02,  3.5411e-02, -1.4615e-01,\n",
              "                         4.4201e-02,  2.3925e-01,  7.9302e-02,  1.8412e-01, -1.9473e-01,\n",
              "                         2.0056e-01, -3.8798e-02,  5.6034e-02,  2.3462e-01,  8.0358e-02,\n",
              "                        -1.9223e-02,  5.5994e-02],\n",
              "                       [-8.9324e-02,  8.2469e-02,  5.9187e-02, -1.0873e-01,  7.8132e-02,\n",
              "                        -3.1286e-02, -6.8547e-02,  1.0419e-01,  8.7671e-02,  1.1328e-01,\n",
              "                        -1.8437e-01, -3.2714e-02, -8.9075e-02, -1.0368e-01,  1.4496e-01,\n",
              "                        -1.0633e-01,  2.2584e-02],\n",
              "                       [-1.1629e-01, -1.7152e-01,  2.1534e-01,  1.1166e-01,  5.5142e-03,\n",
              "                        -1.6433e-01,  2.3778e-01,  1.5447e-01, -4.1946e-02,  4.7250e-02,\n",
              "                        -3.4363e-03,  6.8843e-02, -1.5207e-01,  1.7673e-01, -7.4774e-02,\n",
              "                        -6.1498e-03,  1.5653e-02],\n",
              "                       [-1.1509e-01,  2.0778e-01, -4.7034e-02,  1.5520e-01, -8.3971e-02,\n",
              "                        -1.6962e-01,  1.5742e-01, -2.2700e-01,  1.3381e-01,  2.1290e-01,\n",
              "                        -3.2438e-03, -1.8846e-02,  5.7428e-02, -1.4948e-01, -1.0592e-01,\n",
              "                        -7.3870e-02,  1.6947e-02],\n",
              "                       [-3.7102e-02,  1.0584e-01, -2.2609e-01, -7.2312e-02,  1.9462e-01,\n",
              "                        -1.6348e-01,  2.3988e-01, -1.1823e-01,  2.0022e-01,  5.1247e-02,\n",
              "                         3.9613e-02,  1.7668e-01,  2.1127e-01,  1.1976e-01,  7.0921e-02,\n",
              "                        -2.2724e-02, -8.8878e-02],\n",
              "                       [ 5.0192e-02,  1.2346e-01, -1.8518e-01,  3.6120e-02,  2.3452e-01,\n",
              "                         1.0233e-01,  1.2374e-01, -1.8451e-01,  7.4133e-02, -1.4166e-01,\n",
              "                        -2.3455e-01, -1.3470e-01, -1.9214e-01, -7.8567e-03, -1.0654e-01,\n",
              "                         1.8768e-01, -2.2374e-01],\n",
              "                       [-3.1785e-02, -1.8865e-01,  8.8071e-02, -9.8850e-02, -3.6425e-02,\n",
              "                        -1.9052e-01, -1.9434e-01,  2.1520e-01,  1.7588e-01, -2.3675e-01,\n",
              "                        -2.4208e-01, -5.6965e-02, -1.6940e-01,  2.3802e-01, -1.8932e-01,\n",
              "                         1.5190e-01,  2.5254e-02],\n",
              "                       [ 2.0126e-01, -4.7495e-02, -1.0663e-02,  7.6315e-02,  1.1071e-01,\n",
              "                        -1.3178e-01, -1.8626e-01, -3.4376e-02,  2.2633e-02,  6.5254e-03,\n",
              "                        -7.8453e-02, -4.9107e-02,  2.3447e-03, -2.0426e-01, -1.8814e-01,\n",
              "                         7.0506e-02, -7.9056e-02],\n",
              "                       [ 3.6891e-02,  1.9399e-01, -5.9593e-02, -1.2811e-01,  1.6788e-01,\n",
              "                        -7.1969e-02,  1.5473e-01, -1.5851e-01, -2.1625e-01,  1.6902e-01,\n",
              "                         1.1453e-01, -1.8013e-01, -2.2715e-01, -8.4520e-02,  8.1458e-02,\n",
              "                         6.0223e-02,  6.6742e-02]])),\n",
              "              ('encoder_convs.0.lin_e.bias',\n",
              "               tensor([ 0.2414,  0.2421,  0.1965, -0.1743, -0.2224,  0.0032,  0.1943, -0.0240,\n",
              "                       -0.0618,  0.2336,  0.2132, -0.0997, -0.1046, -0.0569, -0.0771,  0.2389,\n",
              "                       -0.1349,  0.1057,  0.2116, -0.2164, -0.0105, -0.1268, -0.0089,  0.0485,\n",
              "                       -0.1024, -0.2143,  0.0243,  0.2345, -0.0280, -0.1744, -0.1101,  0.0055])),\n",
              "              ('encoder_convs.0.bn_u.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('encoder_convs.0.bn_u.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('encoder_convs.0.bn_u.running_mean',\n",
              "               tensor([ 0.1929,  0.1048,  0.0705, -0.2033,  0.0220,  0.2758, -0.0313,  0.0479,\n",
              "                        0.5163,  0.5576, -0.1367,  0.5378,  0.3992,  0.1119,  0.4471, -0.4804,\n",
              "                        0.1305, -0.2776, -0.0825, -0.2083, -0.2992,  0.1761, -0.0580, -0.0162,\n",
              "                        0.1484,  0.5734, -0.2928,  0.2046,  0.1908, -0.1948, -0.4569, -0.0147])),\n",
              "              ('encoder_convs.0.bn_u.running_var',\n",
              "               tensor([1.0077, 0.9197, 0.6781, 0.9049, 0.7463, 1.1835, 0.7776, 0.7188, 2.5017,\n",
              "                       2.3824, 0.7442, 3.2374, 1.9397, 0.7184, 2.9358, 2.3500, 0.9322, 1.1309,\n",
              "                       0.7742, 1.1521, 1.7116, 0.9850, 0.9537, 0.7085, 0.8629, 2.6240, 1.7273,\n",
              "                       1.2223, 0.9741, 1.0485, 1.5518, 0.8193])),\n",
              "              ('encoder_convs.0.bn_u.num_batches_tracked', tensor(5)),\n",
              "              ('encoder_convs.0.bn_v.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('encoder_convs.0.bn_v.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('encoder_convs.0.bn_v.running_mean',\n",
              "               tensor([-0.0660,  0.0670,  0.1011, -0.1542, -0.2841, -0.2287, -0.0559, -0.0671,\n",
              "                        0.0085,  0.2252, -0.0455,  0.2317,  0.0534, -0.0487, -0.1748, -0.3003,\n",
              "                       -0.0959, -0.0420, -0.0536, -0.1861, -0.0831, -0.0597,  0.0509,  0.0549,\n",
              "                       -0.1154, -0.0525, -0.1885,  0.2080,  0.0835,  0.0384,  0.0121,  0.0266])),\n",
              "              ('encoder_convs.0.bn_v.running_var',\n",
              "               tensor([0.7995, 0.7317, 0.6603, 0.7021, 0.9414, 0.7605, 0.7104, 0.7451, 0.8351,\n",
              "                       0.8163, 0.7969, 0.9417, 0.9057, 0.7795, 0.8248, 1.1060, 0.7256, 0.7037,\n",
              "                       0.8780, 0.7221, 0.6848, 0.9071, 0.8807, 0.6974, 0.7033, 0.6927, 0.7313,\n",
              "                       0.7583, 0.9497, 0.6837, 0.9179, 0.6365])),\n",
              "              ('encoder_convs.0.bn_v.num_batches_tracked', tensor(5)),\n",
              "              ('encoder_convs.0.bn_e.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('encoder_convs.0.bn_e.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('encoder_convs.0.bn_e.running_mean',\n",
              "               tensor([ 0.0755,  0.1605,  0.2516,  0.2534, -0.3984,  0.0035, -0.0058, -0.2014,\n",
              "                        0.3788,  0.2745,  0.4619, -0.1198, -0.4112, -0.1273, -0.3620,  0.1089,\n",
              "                        0.2015,  0.0524, -0.0846, -0.4037, -0.0933,  0.2534,  0.1231,  0.2253,\n",
              "                       -0.0090, -0.2239, -0.0895,  0.6282, -0.5387, -0.5576, -0.3947, -0.3910])),\n",
              "              ('encoder_convs.0.bn_e.running_var',\n",
              "               tensor([0.8070, 0.9036, 1.4080, 2.2711, 1.1827, 0.6557, 0.7929, 0.8536, 1.8877,\n",
              "                       1.2552, 1.7593, 0.8549, 1.9233, 0.9350, 1.1720, 0.7024, 1.1542, 0.7147,\n",
              "                       0.9003, 1.7717, 0.8051, 1.2796, 0.7601, 0.9186, 0.6408, 1.0534, 0.8058,\n",
              "                       3.0783, 3.0203, 2.9433, 1.3306, 1.7070])),\n",
              "              ('encoder_convs.0.bn_e.num_batches_tracked', tensor(5)),\n",
              "              ('encoder_convs.1.lin_u.weight',\n",
              "               tensor([[ 0.0083, -0.0795, -0.0312,  ..., -0.0001, -0.0311,  0.0352],\n",
              "                       [ 0.0638, -0.0595, -0.0205,  ..., -0.0670,  0.0557, -0.0478],\n",
              "                       [ 0.0568, -0.0613,  0.0387,  ..., -0.0671,  0.0734, -0.0850],\n",
              "                       ...,\n",
              "                       [ 0.0038, -0.0251, -0.0230,  ...,  0.0199, -0.0770,  0.0343],\n",
              "                       [ 0.0202,  0.0446, -0.0527,  ..., -0.0607,  0.0230, -0.0189],\n",
              "                       [-0.0593, -0.0624,  0.0772,  ..., -0.0337, -0.0304, -0.0423]])),\n",
              "              ('encoder_convs.1.lin_u.bias',\n",
              "               tensor([-3.0848e-03, -3.9246e-02,  7.5814e-02, -1.4122e-02,  8.4236e-02,\n",
              "                       -1.6805e-02, -7.5207e-02,  5.6638e-02,  5.4166e-02, -2.7295e-02,\n",
              "                       -7.0378e-03,  5.9596e-03, -2.6063e-02, -1.5985e-03, -2.5286e-02,\n",
              "                       -7.3853e-02, -1.8953e-02, -1.5769e-02,  4.1153e-03,  8.1009e-02,\n",
              "                        7.8791e-02,  5.5555e-02,  1.7245e-03,  1.0006e-02,  2.1146e-02,\n",
              "                        4.9320e-02, -8.2745e-02,  7.7963e-05, -1.4631e-02, -6.2140e-02,\n",
              "                       -3.0808e-02, -5.3492e-02])),\n",
              "              ('encoder_convs.1.lin_v.weight',\n",
              "               tensor([[ 0.0004, -0.0293,  0.0762,  ..., -0.0749, -0.0639,  0.0116],\n",
              "                       [ 0.0429,  0.0803, -0.0512,  ...,  0.0220,  0.0054, -0.0417],\n",
              "                       [-0.0710,  0.0811,  0.0866,  ...,  0.0719, -0.0726, -0.0171],\n",
              "                       ...,\n",
              "                       [-0.0360, -0.0853, -0.0502,  ...,  0.0593,  0.0487, -0.0381],\n",
              "                       [-0.0646, -0.0744, -0.0876,  ..., -0.0255,  0.0751,  0.0732],\n",
              "                       [ 0.0077, -0.0135, -0.0297,  ..., -0.0427,  0.0608,  0.0812]])),\n",
              "              ('encoder_convs.1.lin_v.bias',\n",
              "               tensor([ 0.0575,  0.0880,  0.0801,  0.0783,  0.0726,  0.0300,  0.0595,  0.0390,\n",
              "                       -0.0455, -0.0693, -0.0575, -0.0505,  0.0169, -0.0530, -0.0481, -0.0683,\n",
              "                       -0.0873,  0.0770,  0.0352, -0.0798, -0.0226,  0.0395,  0.0841,  0.0630,\n",
              "                        0.0088, -0.0278,  0.0091,  0.0284,  0.0628,  0.0587, -0.0805,  0.0434])),\n",
              "              ('encoder_convs.1.bn_u.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('encoder_convs.1.bn_u.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('encoder_convs.1.bn_u.running_mean',\n",
              "               tensor([-0.1572,  0.1350,  0.0534,  0.0976, -0.0113, -0.1062, -0.1755,  0.0239,\n",
              "                       -0.0456,  0.1091,  0.0542,  0.0695,  0.1691, -0.0113,  0.0935,  0.1130,\n",
              "                        0.2299,  0.0530,  0.1093,  0.0424, -0.0768, -0.2269,  0.1427,  0.1107,\n",
              "                        0.0997,  0.2333, -0.1496,  0.0172,  0.0817, -0.1589,  0.0535, -0.0917])),\n",
              "              ('encoder_convs.1.bn_u.running_var',\n",
              "               tensor([0.7722, 0.6658, 0.6530, 0.7255, 0.7903, 0.6633, 0.7243, 0.6316, 0.6678,\n",
              "                       0.7016, 0.6282, 0.6309, 0.7802, 0.7225, 0.6313, 0.6788, 0.7090, 0.6517,\n",
              "                       0.6862, 0.7782, 0.7104, 0.6762, 0.7890, 0.7018, 0.6781, 0.7198, 0.7137,\n",
              "                       0.6521, 0.6313, 0.7094, 0.6986, 0.6303])),\n",
              "              ('encoder_convs.1.bn_u.num_batches_tracked', tensor(5)),\n",
              "              ('encoder_convs.1.bn_v.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('encoder_convs.1.bn_v.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('encoder_convs.1.bn_v.running_mean',\n",
              "               tensor([ 0.0586,  0.1037, -0.0859, -0.0336, -0.0249,  0.0746,  0.0896, -0.0137,\n",
              "                       -0.1013, -0.0361,  0.0113,  0.0545,  0.0336, -0.1005, -0.0078, -0.0710,\n",
              "                       -0.1571,  0.0046, -0.0810, -0.0599, -0.0247, -0.1392,  0.0333,  0.0088,\n",
              "                        0.0589, -0.0582, -0.0912,  0.0830,  0.0173,  0.0149,  0.0744, -0.0660])),\n",
              "              ('encoder_convs.1.bn_v.running_var',\n",
              "               tensor([0.6177, 0.6386, 0.6477, 0.6182, 0.6439, 0.6302, 0.6385, 0.6697, 0.6205,\n",
              "                       0.6096, 0.6372, 0.6381, 0.6114, 0.6205, 0.6992, 0.6059, 0.6062, 0.6202,\n",
              "                       0.6467, 0.6142, 0.6210, 0.6232, 0.6280, 0.6101, 0.6393, 0.6307, 0.6125,\n",
              "                       0.6454, 0.6157, 0.6145, 0.6366, 0.6288])),\n",
              "              ('encoder_convs.1.bn_v.num_batches_tracked', tensor(5)),\n",
              "              ('decoder_convs.0.lin_u.weight',\n",
              "               tensor([[-0.0233,  0.0729,  0.0307,  ...,  0.0600,  0.0519,  0.0261],\n",
              "                       [ 0.0907,  0.0319,  0.0094,  ..., -0.0155,  0.0744, -0.0178],\n",
              "                       [ 0.0706, -0.0945, -0.0338,  ...,  0.0646, -0.0639, -0.0158],\n",
              "                       ...,\n",
              "                       [-0.0833, -0.0810,  0.0935,  ...,  0.0461,  0.0186, -0.0774],\n",
              "                       [ 0.0971, -0.0736,  0.0973,  ...,  0.0200, -0.0501,  0.0135],\n",
              "                       [ 0.0116, -0.0775, -0.0386,  ..., -0.0652, -0.0126,  0.0105]])),\n",
              "              ('decoder_convs.0.lin_u.bias',\n",
              "               tensor([-0.0272,  0.0054,  0.0791, -0.0183, -0.0627,  0.0521,  0.0321,  0.0733,\n",
              "                        0.0555, -0.0443,  0.0363,  0.0122,  0.0213, -0.0803, -0.0072,  0.0002,\n",
              "                       -0.0708,  0.0873, -0.0412, -0.0896, -0.0470, -0.0814, -0.0398,  0.0999,\n",
              "                       -0.0912,  0.0420,  0.0246, -0.0445, -0.0601,  0.0941, -0.0729, -0.0235])),\n",
              "              ('decoder_convs.0.lin_v.weight',\n",
              "               tensor([[ 0.0428, -0.0817,  0.0102,  ...,  0.0212,  0.0360, -0.0260],\n",
              "                       [-0.0469,  0.0302, -0.0379,  ...,  0.0412,  0.0030,  0.0263],\n",
              "                       [ 0.0610,  0.0701, -0.0955,  ...,  0.0566, -0.0408, -0.0307],\n",
              "                       ...,\n",
              "                       [ 0.0134,  0.0857, -0.0260,  ...,  0.0830, -0.0738, -0.0836],\n",
              "                       [-0.0003,  0.0815, -0.0460,  ..., -0.0798, -0.0080,  0.0370],\n",
              "                       [ 0.0437,  0.0981, -0.0985,  ..., -0.0013,  0.0327, -0.0013]])),\n",
              "              ('decoder_convs.0.lin_v.bias',\n",
              "               tensor([ 0.0103, -0.0801, -0.0017,  0.0005, -0.0469,  0.0031,  0.0230,  0.0844,\n",
              "                       -0.0987, -0.0761, -0.0064,  0.0039,  0.0130,  0.0583,  0.0071, -0.0484,\n",
              "                       -0.0581, -0.0584,  0.0347,  0.0268,  0.0097, -0.0402,  0.0174, -0.0283,\n",
              "                        0.0171, -0.0912,  0.0976,  0.0327, -0.0687, -0.0176,  0.0003, -0.0146])),\n",
              "              ('decoder_convs.0.lin_e.weight',\n",
              "               tensor([[ 0.0159, -0.1097,  0.0243,  ...,  0.1026, -0.1198, -0.0291],\n",
              "                       [ 0.0054, -0.0562, -0.0062,  ..., -0.0193, -0.1242,  0.0091],\n",
              "                       [-0.1115,  0.0637,  0.1144,  ..., -0.0945,  0.0289, -0.0471],\n",
              "                       ...,\n",
              "                       [-0.1026, -0.0742,  0.0702,  ..., -0.0814, -0.0587,  0.0663],\n",
              "                       [-0.0955, -0.0845,  0.0632,  ...,  0.0673, -0.1153, -0.0739],\n",
              "                       [ 0.0893, -0.0534, -0.1209,  ..., -0.0478,  0.0817,  0.0216]])),\n",
              "              ('decoder_convs.0.lin_e.bias',\n",
              "               tensor([-0.0746, -0.0994, -0.0889, -0.0892,  0.0735,  0.0774, -0.0147,  0.1181,\n",
              "                        0.1209, -0.0414,  0.0576,  0.0940, -0.0408, -0.0424, -0.0939, -0.1224,\n",
              "                        0.1229, -0.0883,  0.0751, -0.1115,  0.0735, -0.0003, -0.0640,  0.1173,\n",
              "                        0.0653, -0.0360, -0.0334, -0.0274, -0.0034,  0.1028,  0.0411,  0.0665])),\n",
              "              ('decoder_convs.0.bn_u.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('decoder_convs.0.bn_u.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('decoder_convs.0.bn_u.running_mean',\n",
              "               tensor([-0.0586,  0.0019,  0.1031,  0.1154, -0.0518,  0.0843,  0.2487, -0.0856,\n",
              "                       -0.3515, -0.4045,  0.0142, -0.0213, -0.2485, -0.1845,  0.2390,  0.0968,\n",
              "                       -0.0859, -0.1483, -0.0677,  0.0395,  0.1150,  0.1075, -0.1617, -0.0450,\n",
              "                       -0.0198,  0.0835,  0.2054, -0.1084, -0.0138,  0.0250, -0.2781,  0.0973])),\n",
              "              ('decoder_convs.0.bn_u.running_var',\n",
              "               tensor([0.7660, 0.8122, 0.8543, 0.7984, 0.7086, 0.8821, 1.2002, 0.7696, 1.7914,\n",
              "                       1.3292, 0.8173, 0.6895, 1.1389, 0.9196, 1.1586, 1.1797, 0.9507, 1.0961,\n",
              "                       0.7312, 0.9919, 0.9169, 0.7366, 0.6601, 0.7807, 0.6911, 0.8456, 1.3692,\n",
              "                       0.8253, 0.9140, 0.7867, 0.9990, 1.0217])),\n",
              "              ('decoder_convs.0.bn_u.num_batches_tracked', tensor(5)),\n",
              "              ('decoder_convs.0.bn_v.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('decoder_convs.0.bn_v.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('decoder_convs.0.bn_v.running_mean',\n",
              "               tensor([-0.0483,  0.0107, -0.0051, -0.1003, -0.0723,  0.0286, -0.0893,  0.0581,\n",
              "                       -0.0848, -0.0758,  0.0254,  0.0442, -0.0360, -0.0156, -0.0164,  0.0019,\n",
              "                        0.0494, -0.0301, -0.0452, -0.0007,  0.0414, -0.0157,  0.0373, -0.0195,\n",
              "                       -0.0158, -0.0721,  0.0513, -0.1087, -0.1280,  0.2003, -0.0309, -0.0055])),\n",
              "              ('decoder_convs.0.bn_v.running_var',\n",
              "               tensor([0.6914, 0.6395, 0.6674, 0.6854, 0.6637, 0.7387, 0.6548, 0.7441, 0.6473,\n",
              "                       0.6444, 0.7436, 0.6656, 0.6177, 0.6363, 0.6613, 0.6571, 0.6834, 0.6660,\n",
              "                       0.7663, 0.6751, 0.6515, 0.6488, 0.6679, 0.6507, 0.6497, 0.6535, 0.7118,\n",
              "                       0.7002, 0.7064, 0.6960, 0.7103, 0.6408])),\n",
              "              ('decoder_convs.0.bn_v.num_batches_tracked', tensor(5)),\n",
              "              ('decoder_convs.0.bn_e.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "              ('decoder_convs.0.bn_e.bias',\n",
              "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "              ('decoder_convs.0.bn_e.running_mean',\n",
              "               tensor([-0.1045,  0.0038,  0.0102, -0.1580,  0.2359, -0.1013,  0.0181,  0.1045,\n",
              "                       -0.2194, -0.0889, -0.0016,  0.0839, -0.2932,  0.1201,  0.1994, -0.1774,\n",
              "                       -0.0248,  0.1491, -0.1755, -0.0879, -0.0063, -0.3611, -0.0331, -0.1244,\n",
              "                       -0.1222, -0.1267, -0.1730,  0.0655,  0.0067,  0.1256,  0.0401,  0.2513])),\n",
              "              ('decoder_convs.0.bn_e.running_var',\n",
              "               tensor([0.8816, 0.7232, 0.8266, 0.7529, 1.0100, 0.7658, 0.8041, 0.9127, 1.0583,\n",
              "                       0.7320, 0.7365, 0.8197, 1.3535, 0.9443, 1.1725, 1.3487, 0.7341, 0.8917,\n",
              "                       1.2063, 0.6685, 0.8228, 1.4648, 0.8730, 0.9847, 0.7463, 0.7786, 0.7563,\n",
              "                       0.7931, 0.6948, 0.7467, 0.7941, 1.3621])),\n",
              "              ('decoder_convs.0.bn_e.num_batches_tracked', tensor(5)),\n",
              "              ('decoder_convs.1.lin_u.weight',\n",
              "               tensor([[ 5.5278e-02, -6.4173e-02, -5.1283e-02,  2.5740e-02, -4.6880e-02,\n",
              "                         7.4017e-02, -2.2392e-02,  6.3820e-02,  4.8506e-02, -4.4504e-02,\n",
              "                        -5.1348e-03, -5.9480e-02, -3.5885e-02,  6.7371e-02, -4.8542e-02,\n",
              "                         3.1913e-02, -1.5640e-02,  2.4796e-02, -2.6219e-02,  2.3456e-03,\n",
              "                         6.7730e-02,  4.8986e-02, -3.8534e-02,  7.7527e-02,  8.3137e-03,\n",
              "                         7.3806e-02,  6.6423e-02,  1.6512e-02, -7.7188e-02, -7.8080e-02,\n",
              "                         3.0555e-02,  1.6695e-02, -2.6255e-02,  3.2169e-02, -7.6993e-02,\n",
              "                         2.0041e-03,  1.2489e-03, -7.3557e-02, -1.6468e-02, -5.6063e-02,\n",
              "                        -4.0113e-02, -5.2135e-02, -6.0010e-02, -3.1014e-02,  3.3869e-02,\n",
              "                        -2.9766e-02,  5.0091e-02, -1.9702e-02,  2.3165e-02, -6.1511e-02,\n",
              "                        -3.6296e-02,  1.0103e-02,  1.0159e-02, -5.7414e-02, -2.0540e-02,\n",
              "                        -3.9848e-02,  2.5387e-02,  1.6823e-02, -6.5861e-02, -2.3408e-02,\n",
              "                         7.0416e-02, -2.7063e-02,  2.3532e-04, -1.4409e-02,  7.3767e-02,\n",
              "                        -4.5685e-02,  3.9350e-02,  6.4000e-02, -2.4516e-03, -8.0830e-03,\n",
              "                         3.7970e-02, -5.6144e-02,  4.8669e-02, -1.1298e-02,  1.7536e-02,\n",
              "                        -7.8696e-02,  5.6419e-02,  5.6006e-02, -3.4669e-02,  1.8834e-02,\n",
              "                        -3.9394e-02,  5.1846e-02, -6.7228e-02,  2.8740e-02, -5.7970e-02,\n",
              "                        -3.7722e-02,  1.3581e-02,  1.3007e-02,  2.5302e-02,  7.6711e-02,\n",
              "                         4.3243e-02,  6.3643e-02, -1.3055e-02,  7.4239e-03,  1.0148e-02,\n",
              "                         4.9090e-02,  2.8685e-03,  6.5710e-02,  2.7487e-02,  6.6758e-02,\n",
              "                        -5.9192e-02, -2.3383e-02, -4.1173e-03, -5.7556e-02, -4.2438e-02,\n",
              "                        -1.0039e-02, -5.6023e-02,  8.9708e-03,  6.7970e-02,  3.4642e-02,\n",
              "                         3.0514e-02,  7.1258e-02,  1.4550e-02,  1.1164e-02, -9.2197e-04,\n",
              "                         1.4887e-02,  5.3173e-02, -6.9138e-02, -2.9929e-02, -7.0413e-02,\n",
              "                        -3.2622e-02,  2.5460e-02, -7.8774e-02,  4.6736e-02, -4.9229e-02,\n",
              "                        -6.6534e-02, -6.6278e-02,  7.5031e-02,  4.9554e-02,  5.8762e-02,\n",
              "                         5.9386e-02, -1.4338e-04, -6.1401e-02, -5.7922e-02, -6.3964e-02,\n",
              "                         1.1619e-02,  4.0960e-02,  1.0117e-02,  4.5392e-02, -7.2907e-02,\n",
              "                         3.9126e-02,  1.0707e-02, -4.1477e-02, -7.3942e-02, -3.7951e-02,\n",
              "                        -7.0167e-02, -3.2012e-02,  9.6635e-03,  3.6823e-02,  1.0013e-02,\n",
              "                         6.2642e-02,  2.2331e-02,  1.3013e-02, -7.1679e-02,  1.9138e-02,\n",
              "                        -6.6946e-02,  5.2699e-02, -1.6185e-03, -3.7908e-02, -4.5085e-02],\n",
              "                       [-1.3023e-02, -1.2074e-02,  1.0299e-02,  3.5127e-02,  4.8294e-02,\n",
              "                        -2.7305e-02, -7.1998e-02,  5.7577e-02,  7.2329e-02,  6.0889e-02,\n",
              "                         4.0435e-02, -1.6726e-02, -5.8060e-02,  1.2900e-02,  4.2150e-02,\n",
              "                         2.4585e-02, -5.4508e-02,  1.1829e-02, -5.7688e-03, -4.1611e-02,\n",
              "                        -2.6228e-02,  4.2257e-02, -4.0378e-02, -6.2849e-02,  1.7399e-02,\n",
              "                         7.3697e-02, -4.4953e-03, -2.0619e-02,  5.0587e-02, -7.0798e-02,\n",
              "                         3.5745e-03,  6.8258e-02,  1.0096e-02, -4.3161e-02,  1.3606e-02,\n",
              "                        -4.0450e-02, -4.8388e-02, -6.3771e-02, -3.1173e-03, -1.3679e-03,\n",
              "                        -7.2869e-02,  2.2075e-02, -2.1959e-03, -1.0325e-02, -5.3526e-02,\n",
              "                         2.5608e-02,  2.6101e-03,  1.6641e-02, -4.3522e-02, -5.2940e-02,\n",
              "                         4.3640e-02,  2.1343e-02,  1.4586e-02, -1.9581e-02,  4.9955e-02,\n",
              "                         3.5153e-02, -4.5946e-03, -6.4556e-02, -7.7682e-02, -5.2858e-02,\n",
              "                        -6.8058e-02, -2.9342e-02, -7.6571e-02,  5.3692e-02, -2.1183e-02,\n",
              "                         5.6237e-02,  3.6378e-02,  3.6324e-02,  1.7864e-03, -2.7473e-02,\n",
              "                        -2.4991e-02, -9.3959e-03, -1.0127e-02, -4.6919e-02, -2.4385e-03,\n",
              "                         4.9130e-02, -5.4104e-02,  9.6336e-03,  1.5825e-02,  1.4801e-02,\n",
              "                         4.8122e-02, -5.8371e-02,  2.5667e-04,  7.4671e-02,  1.4082e-02,\n",
              "                         5.1221e-02,  7.5798e-02, -1.7252e-02,  5.3697e-02,  7.7916e-02,\n",
              "                        -4.2254e-02,  6.2669e-02,  6.4230e-02, -5.4216e-02,  5.8080e-03,\n",
              "                        -3.2836e-02, -4.7580e-02, -4.8765e-02,  6.8513e-02,  2.5668e-02,\n",
              "                        -4.2465e-02,  7.3820e-02,  5.6062e-02, -6.9522e-02,  3.8173e-02,\n",
              "                         7.0756e-02,  3.2852e-02, -2.3235e-02,  5.7165e-03, -7.4829e-02,\n",
              "                         1.8713e-02, -3.5800e-02, -3.3978e-02,  6.6113e-02,  1.5342e-03,\n",
              "                        -7.4185e-03,  4.1108e-02,  7.2602e-02,  4.9469e-02, -6.0447e-02,\n",
              "                         6.2138e-02,  6.9630e-02,  4.9395e-02, -9.1656e-03, -9.2171e-03,\n",
              "                        -7.2102e-02,  3.1172e-02, -3.1384e-02,  5.4119e-02, -7.8673e-02,\n",
              "                        -1.4732e-02,  5.6332e-02, -2.6020e-02,  2.0982e-02, -5.3188e-02,\n",
              "                        -4.5285e-02, -3.2532e-02, -4.1565e-02,  6.5906e-02,  1.2640e-02,\n",
              "                        -1.9952e-02, -3.8114e-02,  5.1431e-02, -1.2204e-02,  7.2988e-02,\n",
              "                         3.5644e-02, -3.3534e-02, -1.8029e-02,  3.1569e-02,  7.6158e-02,\n",
              "                        -6.5537e-02, -5.8441e-03,  6.8046e-02, -7.2956e-02,  3.5302e-03,\n",
              "                        -3.9118e-02,  1.3040e-02,  7.8027e-02, -5.1519e-02, -6.9575e-02],\n",
              "                       [ 2.8102e-02, -5.0831e-02, -6.5615e-03,  3.9254e-02,  3.5358e-02,\n",
              "                         2.9990e-02,  4.3235e-02,  4.1702e-02, -4.5518e-02,  2.0862e-02,\n",
              "                        -2.4160e-02,  6.4559e-02, -4.8299e-02,  4.3009e-02, -7.0448e-02,\n",
              "                         2.5461e-02,  5.4023e-02, -6.0247e-02,  7.5111e-02,  5.7691e-02,\n",
              "                        -1.2494e-03, -2.1610e-02, -6.3769e-02, -1.9741e-02, -3.0771e-02,\n",
              "                         3.2447e-02,  6.7379e-02, -6.0320e-02, -3.1113e-02, -2.4564e-02,\n",
              "                        -3.0600e-02, -3.4862e-02,  7.4334e-02,  7.8860e-02,  6.6558e-02,\n",
              "                        -4.9502e-02, -4.2333e-02, -3.7008e-02,  3.3958e-02,  3.7620e-04,\n",
              "                        -1.9833e-02,  1.3389e-02,  3.0508e-02, -1.6269e-02,  2.2343e-02,\n",
              "                         7.6342e-02, -6.5853e-02, -5.3313e-02, -1.1693e-02,  3.2713e-02,\n",
              "                        -5.0592e-02,  4.8184e-02, -2.7987e-02,  5.7250e-02,  4.8956e-03,\n",
              "                         6.5305e-02,  7.4296e-02, -4.9341e-02,  9.1062e-04,  5.0824e-02,\n",
              "                         3.0383e-02, -5.7738e-02, -2.0823e-02, -7.1871e-02, -5.4385e-02,\n",
              "                        -6.8801e-02, -3.4265e-02,  4.5889e-02, -6.8075e-02,  4.2085e-03,\n",
              "                         7.6471e-02, -4.6489e-02,  4.0980e-02, -5.2813e-02,  6.7052e-02,\n",
              "                        -3.7125e-02, -6.8354e-02, -6.5705e-02, -2.6747e-02,  2.9563e-02,\n",
              "                         4.6679e-02,  5.9554e-02,  1.4380e-02,  5.6510e-02,  3.3865e-02,\n",
              "                        -8.4263e-03, -1.6116e-02, -6.2852e-02, -3.7497e-02, -4.4679e-02,\n",
              "                        -4.5739e-02,  1.6948e-02, -5.5436e-02,  1.1334e-02, -2.7421e-02,\n",
              "                        -4.3032e-02, -7.2329e-02,  5.5530e-02,  4.4315e-02, -5.2656e-02,\n",
              "                         3.7106e-02,  3.7790e-02,  2.6243e-02,  4.3741e-03,  8.2627e-03,\n",
              "                         3.1150e-02,  7.0417e-02, -4.9451e-02,  7.0834e-02,  8.2439e-03,\n",
              "                         7.5163e-02,  4.7548e-02, -2.2672e-02, -4.2433e-02, -3.9369e-02,\n",
              "                        -4.6321e-02,  6.0156e-02, -2.3626e-02, -7.3345e-02, -2.8458e-02,\n",
              "                         1.8164e-02,  4.9800e-03, -4.7771e-02, -7.0652e-02, -2.5932e-02,\n",
              "                         4.9975e-02, -2.6907e-02, -2.4885e-02, -2.9934e-02,  1.4061e-02,\n",
              "                         4.5813e-02,  3.3802e-02,  6.7898e-02,  5.4512e-02,  4.5644e-02,\n",
              "                         8.0676e-03,  2.1373e-02, -3.4254e-02, -4.2804e-02, -2.3518e-02,\n",
              "                         5.3632e-02, -7.5631e-03, -4.0800e-02, -4.6091e-02, -5.6481e-02,\n",
              "                        -2.2509e-02,  2.4912e-02, -1.4133e-02, -5.9695e-02, -6.9481e-02,\n",
              "                         5.6081e-05, -1.4956e-02,  5.2997e-02, -5.4205e-02, -7.3142e-02,\n",
              "                        -5.2597e-02,  4.6536e-02,  2.2415e-02,  2.5351e-03, -5.9031e-02],\n",
              "                       [-2.9672e-02,  5.9627e-03,  1.0657e-02,  5.7751e-02,  7.4467e-02,\n",
              "                         1.4880e-02, -1.7149e-03, -6.3014e-02,  5.2447e-02, -4.7038e-02,\n",
              "                        -1.3725e-02, -1.5349e-02, -5.2880e-02,  7.4943e-03, -1.6466e-02,\n",
              "                         2.4998e-02,  7.3312e-02,  4.6325e-02,  3.0003e-02,  2.7235e-02,\n",
              "                        -3.0867e-02,  3.8465e-02,  1.8020e-02, -2.5934e-02,  2.9816e-02,\n",
              "                         5.5607e-02,  1.8779e-02,  5.5189e-02,  3.7599e-03,  2.1366e-02,\n",
              "                         1.1249e-02, -2.8582e-02, -7.0663e-02,  5.6860e-02, -6.6607e-02,\n",
              "                         1.6272e-03,  5.2150e-03, -6.6476e-02, -5.5131e-02, -6.6468e-02,\n",
              "                         3.7068e-02, -5.1976e-02, -7.8549e-02,  1.2246e-02, -3.5085e-02,\n",
              "                        -6.0612e-03,  4.9621e-02,  7.9539e-03, -6.1949e-02, -3.9587e-02,\n",
              "                         1.1259e-03, -3.9728e-02, -6.1046e-02,  7.8939e-02,  2.8914e-02,\n",
              "                        -5.5040e-02, -3.7693e-02,  4.3720e-02, -3.9469e-02, -9.8605e-03,\n",
              "                         3.3168e-02,  7.4607e-03,  6.0242e-02, -7.6778e-02, -3.7114e-02,\n",
              "                        -6.9300e-02, -2.5751e-02, -3.8064e-02, -4.4193e-02,  7.9856e-03,\n",
              "                         4.8706e-02,  1.0255e-02,  5.5516e-02, -3.9300e-02, -5.2848e-03,\n",
              "                        -4.0022e-02,  3.3577e-02,  7.5922e-02,  9.9253e-03, -2.1897e-02,\n",
              "                         5.6585e-03,  6.9831e-02,  6.1957e-02, -4.4746e-02,  4.6439e-02,\n",
              "                        -1.4743e-02,  1.9512e-02,  6.6341e-02,  3.3434e-02, -7.6118e-02,\n",
              "                         6.2451e-02, -7.2096e-02, -2.8423e-02, -1.5249e-02,  4.4402e-02,\n",
              "                        -2.9868e-02,  7.3595e-02,  2.7193e-03, -2.8430e-02, -6.0955e-02,\n",
              "                         2.4713e-03,  7.5730e-02, -5.6235e-03,  1.0704e-02, -5.7926e-02,\n",
              "                         5.8861e-02, -7.6494e-02, -4.2893e-02,  7.5770e-02,  6.5339e-02,\n",
              "                        -1.5330e-02, -3.8782e-02, -6.0150e-02, -2.9969e-02, -5.1559e-02,\n",
              "                        -1.4456e-02, -7.7118e-02, -5.2354e-02, -7.4222e-02, -5.7937e-03,\n",
              "                        -2.7456e-03, -6.0017e-02,  1.2074e-02, -2.8932e-02,  2.2216e-02,\n",
              "                         4.8984e-02,  4.8750e-02, -2.0420e-02, -6.4488e-02, -6.8755e-02,\n",
              "                         2.8434e-02, -6.5963e-02,  4.1550e-03, -5.8231e-02, -1.4648e-02,\n",
              "                        -3.8191e-02, -6.6595e-02,  7.0626e-02,  4.0783e-02,  7.7738e-02,\n",
              "                        -5.6420e-02, -5.9438e-02,  7.0137e-02,  6.4163e-03,  2.4463e-02,\n",
              "                        -3.9127e-02,  4.2029e-02, -8.4156e-03,  6.3070e-02,  2.7231e-02,\n",
              "                         6.9242e-02,  4.5969e-03,  3.1648e-03, -7.7645e-02, -6.7992e-03,\n",
              "                         3.9819e-03, -5.7941e-02, -6.5115e-02,  6.5072e-02, -6.1283e-02],\n",
              "                       [-3.3336e-02,  1.4449e-02, -7.4916e-02, -7.5859e-02, -5.8545e-02,\n",
              "                        -6.0815e-02, -7.8361e-02, -4.0610e-02,  7.5223e-02, -1.7423e-02,\n",
              "                        -3.6737e-02,  2.3171e-02, -2.6248e-03, -2.4892e-02, -5.7835e-02,\n",
              "                         7.1461e-02, -5.4524e-02,  4.6873e-02, -2.5262e-02, -1.0003e-02,\n",
              "                        -7.5112e-02,  2.6127e-02,  4.2687e-02,  2.5921e-02,  3.2851e-03,\n",
              "                         7.7179e-02, -5.7731e-02, -9.3509e-03, -4.2574e-02, -7.0475e-02,\n",
              "                        -5.5736e-02, -7.6164e-02, -3.3788e-02, -5.2093e-02,  7.5468e-02,\n",
              "                         6.1284e-02,  4.4143e-02, -1.7861e-02,  3.7902e-02,  3.6398e-02,\n",
              "                         4.3716e-02,  3.6674e-02, -3.2727e-02, -7.5912e-02,  6.2377e-02,\n",
              "                         6.9592e-02,  2.4429e-02, -1.9672e-02,  3.6804e-02, -2.1507e-02,\n",
              "                        -1.0942e-02, -3.9903e-02,  6.7355e-02, -4.3907e-02,  3.7317e-02,\n",
              "                        -2.4374e-03, -4.8098e-02, -2.9138e-02,  1.3734e-02,  4.0917e-02,\n",
              "                         5.2740e-03, -1.6322e-02,  5.1369e-02,  6.5003e-02,  6.7793e-02,\n",
              "                        -3.5809e-02,  5.8742e-02, -6.4857e-02, -2.3016e-03, -1.9729e-02,\n",
              "                         7.6115e-02, -1.2991e-02,  6.7500e-02, -4.0362e-02, -9.3306e-04,\n",
              "                        -3.9143e-02, -1.1522e-02, -2.0958e-02, -2.1404e-02, -2.5097e-02,\n",
              "                        -6.8425e-02,  3.9199e-02,  2.8411e-02,  6.9134e-02, -3.3207e-04,\n",
              "                        -7.2392e-02,  3.0015e-02, -5.6009e-02, -6.7133e-02,  2.8200e-02,\n",
              "                        -7.1245e-02,  7.2256e-02, -2.8002e-02, -1.2839e-02,  2.7587e-02,\n",
              "                        -2.2092e-02, -3.3802e-02,  3.7307e-03, -6.9093e-03,  6.6342e-02,\n",
              "                        -4.9190e-02, -3.8896e-02, -1.8598e-02, -1.3130e-02, -4.4238e-02,\n",
              "                         2.1217e-02, -5.8775e-02,  4.5204e-03,  5.7415e-02, -3.7814e-02,\n",
              "                         4.8298e-02,  2.2414e-02, -1.6224e-02,  1.0126e-02, -7.4243e-02,\n",
              "                         5.8257e-02, -6.5687e-02, -4.3100e-02, -4.0099e-02, -4.5192e-02,\n",
              "                         3.7413e-02, -2.4731e-03, -4.0800e-02,  1.9868e-02, -2.7712e-02,\n",
              "                        -6.2446e-02,  3.9669e-02, -2.6539e-02, -4.1660e-02, -3.8415e-02,\n",
              "                        -7.5719e-02, -6.8223e-02, -5.1806e-02,  9.3239e-03, -2.1609e-02,\n",
              "                         5.3487e-02,  8.0253e-03,  5.2820e-02,  4.9664e-02,  1.6148e-02,\n",
              "                        -7.8463e-02, -7.7062e-02, -2.9384e-02,  4.0655e-02,  1.9757e-02,\n",
              "                        -2.8087e-02,  5.0617e-02, -4.2516e-02, -6.0573e-02, -4.1499e-02,\n",
              "                        -2.3754e-02, -9.2144e-03,  7.8564e-02, -3.6878e-02, -4.0243e-02,\n",
              "                         5.9109e-02, -3.9012e-02,  6.8335e-02, -6.3543e-02,  2.6047e-02],\n",
              "                       [-4.9591e-02,  2.9169e-02, -4.9622e-02, -2.2411e-02,  3.5982e-03,\n",
              "                        -4.4188e-03,  3.8654e-02, -3.5140e-02, -4.2326e-02,  4.8914e-02,\n",
              "                         1.6611e-02, -4.3260e-02, -4.9263e-02, -6.1579e-02, -3.5443e-02,\n",
              "                        -4.1455e-02, -5.9854e-02, -1.6589e-02,  4.0221e-02, -6.2509e-02,\n",
              "                        -4.2866e-02, -6.1283e-02, -2.3340e-02, -4.5103e-02,  3.2802e-02,\n",
              "                         4.2704e-02, -7.2460e-02,  9.7291e-03, -7.4197e-02, -4.7941e-02,\n",
              "                        -6.0641e-02, -2.0288e-03,  4.3270e-03,  7.0691e-02, -2.6653e-02,\n",
              "                         6.2241e-02, -3.0190e-02,  3.6363e-02,  8.5288e-03, -5.2623e-02,\n",
              "                         6.0871e-02,  6.9738e-02,  3.2174e-02,  2.1923e-02,  7.1657e-03,\n",
              "                        -6.8721e-02,  9.4831e-03, -7.2490e-02,  7.7374e-02, -4.5028e-02,\n",
              "                        -4.2036e-02,  2.6237e-02,  2.8452e-02,  4.0797e-02,  4.3513e-02,\n",
              "                        -2.4559e-02,  3.3157e-03,  4.0255e-02, -2.1911e-02, -2.8453e-02,\n",
              "                         2.6012e-02, -2.4916e-02,  7.5982e-02, -4.2919e-02, -7.1625e-02,\n",
              "                        -8.9734e-03, -7.5498e-02,  7.2362e-02, -5.8964e-03, -2.1655e-02,\n",
              "                         6.7159e-02, -7.0620e-02, -7.2632e-03,  5.3395e-02,  5.6730e-02,\n",
              "                         1.9461e-02,  1.4727e-02,  4.3999e-02, -6.6023e-02, -2.2934e-02,\n",
              "                         6.8114e-02, -5.0649e-02,  3.1933e-02, -6.7934e-02, -5.2146e-02,\n",
              "                        -6.5677e-02, -5.4020e-02,  3.4772e-02,  6.8759e-03,  7.3350e-02,\n",
              "                        -5.2998e-02,  6.1597e-02,  3.4945e-02,  6.3500e-02,  3.8317e-02,\n",
              "                        -6.0709e-02,  2.3789e-02, -5.4403e-02, -2.0062e-02,  1.8960e-02,\n",
              "                        -3.2303e-02, -4.5289e-02,  5.5469e-02,  1.6130e-02, -5.0598e-02,\n",
              "                         5.7743e-02,  4.0253e-02, -3.2579e-02,  9.0171e-03, -8.9975e-03,\n",
              "                        -4.9797e-02, -7.0847e-02, -7.1948e-02,  5.2494e-02, -7.4980e-02,\n",
              "                        -6.8988e-02,  7.2162e-02, -3.7201e-02, -2.0336e-02,  3.2992e-02,\n",
              "                         7.6157e-02, -2.4824e-03, -4.4551e-02,  7.8202e-02, -6.1151e-02,\n",
              "                         7.0938e-02,  7.7554e-02,  5.7236e-02, -4.7725e-02,  3.2658e-02,\n",
              "                        -5.7343e-02,  5.1055e-02, -6.9523e-02, -2.2336e-02, -3.9984e-03,\n",
              "                        -1.2738e-03,  1.3722e-02,  6.4438e-02, -3.1099e-02,  5.0503e-02,\n",
              "                        -1.7277e-03, -6.8704e-02, -4.7951e-02,  3.5794e-02, -4.4362e-02,\n",
              "                         6.3601e-02,  4.6518e-02,  6.2359e-02, -6.0053e-02,  5.6815e-02,\n",
              "                         2.2990e-02,  6.7927e-02,  5.3015e-03,  5.2278e-02, -6.3659e-02,\n",
              "                         7.3987e-02, -5.4118e-02,  1.8610e-02, -2.0706e-02,  2.9699e-03]])),\n",
              "              ('decoder_convs.1.lin_u.bias',\n",
              "               tensor([0.0715, 0.0376, 0.0441, 0.0167, 0.0515, 0.0458])),\n",
              "              ('decoder_convs.1.lin_v.weight',\n",
              "               tensor([[ 1.1597e-02,  1.7938e-02, -7.6798e-02, -5.3265e-02, -5.1654e-02,\n",
              "                        -5.8378e-02, -6.0500e-02, -6.4614e-02, -7.0483e-03,  2.3173e-02,\n",
              "                        -4.6369e-02, -6.4873e-02, -3.1436e-02, -1.0015e-03,  5.9753e-02,\n",
              "                        -5.7165e-02, -2.4297e-02, -4.0076e-02, -4.3912e-02,  3.2875e-02,\n",
              "                         2.2646e-02, -4.8086e-02, -7.9043e-02,  9.4596e-04,  1.9193e-02,\n",
              "                         4.7185e-02, -3.2773e-02, -7.5907e-02,  4.8812e-02,  4.5822e-02,\n",
              "                         1.6456e-02,  1.9461e-02, -6.0110e-02, -6.8902e-02, -5.4204e-02,\n",
              "                        -5.0409e-02,  7.6595e-02,  1.1670e-02, -4.7210e-02, -7.5830e-02,\n",
              "                        -9.2495e-03,  6.8108e-02, -7.5497e-02,  3.4038e-02, -2.3878e-03,\n",
              "                         4.1558e-02,  3.7276e-02,  7.5358e-02, -8.5988e-03,  2.2248e-02,\n",
              "                        -4.4978e-02, -7.5289e-03, -1.7183e-02,  2.9503e-02, -3.2182e-02,\n",
              "                        -3.2494e-03,  5.4575e-02,  7.3943e-02, -4.6107e-02,  3.8112e-02,\n",
              "                         4.9399e-02,  5.4064e-02, -2.1108e-02,  1.4022e-02,  4.4887e-02,\n",
              "                         5.1526e-02,  6.5687e-02, -3.1694e-02, -5.2811e-02, -1.2407e-02,\n",
              "                         1.2942e-02, -7.5743e-03, -7.5718e-02, -1.0130e-02, -6.5252e-02,\n",
              "                        -3.3313e-02, -7.7200e-02,  7.0704e-02,  7.8915e-02,  4.9285e-02,\n",
              "                        -6.4980e-02, -4.4428e-03, -7.3216e-02,  7.2931e-02,  5.6783e-02,\n",
              "                        -1.5728e-05, -3.1537e-02,  1.5458e-02, -6.6956e-02,  4.0774e-02,\n",
              "                        -3.1776e-02, -6.6307e-02, -5.0894e-02,  5.1175e-02,  1.7824e-02,\n",
              "                        -7.3686e-02,  2.2082e-02, -3.1275e-02,  2.9652e-03, -1.4513e-03,\n",
              "                        -9.9961e-03, -6.8845e-02,  2.7808e-02, -5.9787e-02,  3.5857e-02,\n",
              "                        -6.7454e-02,  6.3549e-02,  1.1958e-02,  3.6719e-02,  4.6236e-02,\n",
              "                        -7.5899e-02, -3.2854e-02,  2.1299e-02, -7.2418e-02, -2.6028e-02,\n",
              "                         2.5448e-02, -4.1614e-02,  5.2609e-02, -8.7601e-03, -6.3617e-03,\n",
              "                        -1.4722e-02, -6.4113e-02,  6.9845e-02,  5.2390e-02, -7.2488e-02,\n",
              "                        -4.6846e-02, -7.7144e-02,  9.3174e-03,  7.1273e-02, -5.9269e-02,\n",
              "                        -4.9831e-02, -3.7924e-02,  4.8949e-02,  7.4670e-02,  7.0767e-02,\n",
              "                         1.0998e-02, -1.6855e-02,  4.7352e-02, -2.3246e-02,  4.8819e-02,\n",
              "                         4.0163e-02, -4.7863e-02,  1.6644e-02, -5.1488e-02, -6.3770e-02,\n",
              "                         4.0557e-02, -5.6300e-02,  7.0839e-02,  3.7958e-02,  4.0864e-02,\n",
              "                         6.6790e-02, -5.2056e-02,  1.2016e-02, -1.9898e-02, -4.3945e-02,\n",
              "                         4.8410e-02, -7.8867e-02,  1.7776e-02,  7.6830e-02,  1.0761e-03],\n",
              "                       [ 6.4783e-02, -7.7282e-02,  1.5206e-02, -3.6196e-02, -7.0174e-02,\n",
              "                         7.3738e-02,  7.5150e-02,  2.4967e-03, -6.9009e-02, -6.1304e-02,\n",
              "                         2.3270e-02, -2.9053e-02,  5.6359e-02,  7.8304e-02, -1.9106e-02,\n",
              "                        -5.5396e-04, -2.4309e-03, -1.0163e-02,  3.7338e-02,  3.4536e-02,\n",
              "                        -6.9733e-02, -7.7020e-04, -7.1926e-02,  5.8887e-03,  6.0389e-02,\n",
              "                        -1.1198e-02,  2.4266e-02, -6.3121e-02, -6.6632e-02,  4.2399e-02,\n",
              "                        -4.8724e-02, -2.8229e-02, -1.4993e-02, -1.7813e-02,  5.9448e-02,\n",
              "                         1.0793e-02,  4.5264e-02,  6.4624e-02,  5.8647e-02, -3.5934e-03,\n",
              "                         1.3941e-02, -1.2077e-02, -4.0192e-02,  6.2261e-02,  5.6247e-02,\n",
              "                        -3.9203e-02, -1.2141e-02, -3.4431e-02,  5.2629e-02,  1.4408e-02,\n",
              "                         6.2898e-02,  5.7493e-02,  2.6552e-02,  4.6915e-02, -1.1870e-02,\n",
              "                        -1.7404e-02,  5.3428e-02, -3.4980e-02,  7.2460e-02,  5.5840e-02,\n",
              "                        -7.1883e-02,  9.3753e-03, -2.0903e-02, -5.0543e-02,  5.8663e-02,\n",
              "                        -5.6596e-02, -4.0410e-03, -8.4639e-06,  4.9512e-02, -9.6817e-03,\n",
              "                         6.4258e-02,  6.5914e-03, -4.7595e-02, -1.2213e-02, -1.1578e-02,\n",
              "                         6.5321e-02, -7.6574e-02,  2.9858e-02, -3.9724e-02, -6.2851e-02,\n",
              "                         7.3138e-02,  2.6480e-02, -9.2436e-03, -3.4931e-02,  6.8172e-02,\n",
              "                         6.7690e-03, -4.2296e-02,  2.1798e-02, -3.3953e-02,  7.4211e-02,\n",
              "                        -5.6114e-03, -1.0130e-02,  4.2297e-02, -2.2447e-03, -5.9719e-02,\n",
              "                         5.5340e-02,  4.8467e-03,  5.0603e-02,  3.0285e-02,  4.0168e-02,\n",
              "                         3.7909e-03,  6.2379e-03,  2.1131e-02, -7.2533e-02, -7.1488e-02,\n",
              "                         6.3186e-03, -4.8951e-02, -1.6715e-02,  5.4186e-02, -3.4520e-02,\n",
              "                         6.0038e-02, -7.5678e-02, -3.3821e-02, -2.3714e-02, -5.0573e-02,\n",
              "                        -1.1146e-02, -3.3533e-02, -4.9438e-02,  7.1167e-02,  2.6865e-02,\n",
              "                        -7.1193e-02, -7.1262e-02,  4.0983e-03, -6.8992e-02,  2.8940e-02,\n",
              "                        -1.1447e-02, -1.9127e-02,  5.7398e-02, -1.9036e-02, -5.9624e-02,\n",
              "                         3.8223e-02, -5.0441e-02,  4.1180e-02,  3.5881e-02,  3.3292e-02,\n",
              "                        -7.4247e-03,  2.9398e-02, -6.0058e-02,  6.8410e-02,  6.5507e-02,\n",
              "                         6.5961e-02, -4.4113e-02,  5.1603e-02,  4.8573e-02, -6.0389e-02,\n",
              "                         1.1607e-02, -5.1873e-02, -6.0200e-02,  6.0468e-02,  1.5970e-02,\n",
              "                         5.0175e-02,  2.2563e-02, -7.8329e-02,  4.3728e-02,  1.0851e-02,\n",
              "                         6.0002e-02, -4.6596e-02, -3.2669e-02, -4.5048e-02,  4.8637e-02],\n",
              "                       [ 7.2152e-02, -3.0110e-02,  4.0491e-02, -1.9958e-02,  4.5667e-02,\n",
              "                        -4.6831e-02, -5.5764e-02,  7.4695e-03,  5.6687e-02, -4.6117e-02,\n",
              "                         7.8127e-02,  4.9816e-02, -5.6117e-02,  2.1589e-02,  7.4205e-02,\n",
              "                        -6.8859e-02, -8.8740e-03, -1.6721e-03,  1.5460e-02,  3.9354e-02,\n",
              "                        -6.8747e-02,  2.9611e-02, -4.8318e-02, -7.7914e-02, -5.0784e-02,\n",
              "                        -1.4644e-02, -6.4365e-02, -5.1872e-02,  7.9104e-03, -4.7177e-02,\n",
              "                        -5.0301e-02, -1.6124e-02,  2.3917e-02, -5.4240e-02, -6.6068e-02,\n",
              "                        -4.9896e-02, -2.0045e-02,  8.9064e-03, -1.7341e-02,  1.1807e-03,\n",
              "                         5.6604e-02, -1.4841e-02, -4.1060e-02,  7.5354e-02, -7.3566e-02,\n",
              "                         5.0738e-02, -1.9955e-02,  4.3080e-02,  4.1480e-03,  6.3932e-02,\n",
              "                         4.0670e-02,  1.7315e-03,  2.0483e-02,  5.9360e-02, -5.8257e-02,\n",
              "                        -7.7074e-02,  4.9597e-02,  1.5012e-02, -8.4996e-03,  6.8921e-02,\n",
              "                        -3.7983e-02,  5.1187e-02,  2.2342e-02, -4.1266e-02, -1.7417e-02,\n",
              "                         3.1469e-02,  6.9050e-02, -6.2747e-02, -1.0536e-02,  1.4600e-02,\n",
              "                        -4.2211e-02, -5.3320e-02,  5.6461e-02, -2.6295e-02, -4.8658e-02,\n",
              "                         7.0214e-03, -3.1475e-02, -5.9924e-02, -2.1146e-03, -1.8047e-02,\n",
              "                         4.0146e-03,  1.9302e-02, -4.2371e-02, -1.4452e-02, -6.1856e-02,\n",
              "                        -3.7197e-02, -1.0815e-02,  5.6158e-02, -1.0050e-02, -1.0714e-02,\n",
              "                        -7.3999e-02, -1.9760e-02, -3.9424e-02, -6.8325e-02, -7.0231e-02,\n",
              "                         4.1277e-02,  6.4893e-02,  6.5378e-02,  6.1047e-02,  4.7559e-02,\n",
              "                        -4.0743e-02, -1.0172e-03,  3.0986e-02,  3.0536e-02, -1.7964e-02,\n",
              "                         2.0699e-02, -5.4675e-02,  1.4373e-02,  7.3446e-02,  1.9058e-02,\n",
              "                         5.1696e-02,  6.5283e-02,  4.5939e-02, -6.2906e-03,  6.1406e-02,\n",
              "                         1.1654e-02, -1.2423e-02,  7.1989e-02,  7.0378e-02, -4.7414e-02,\n",
              "                        -6.3235e-04,  1.2125e-04,  1.5511e-02, -3.9300e-02, -7.3085e-02,\n",
              "                        -3.9924e-02, -1.3716e-02, -2.6182e-02,  7.3105e-02,  7.4502e-02,\n",
              "                         6.3537e-02,  6.0067e-02,  1.2390e-03, -2.1483e-02,  2.1391e-02,\n",
              "                        -6.0613e-02,  6.0091e-02,  5.5125e-02, -4.7136e-02,  1.3308e-02,\n",
              "                        -2.0837e-02,  2.1759e-02, -2.7497e-02,  4.8068e-03, -6.9842e-02,\n",
              "                         7.7966e-02,  3.4702e-03, -6.6025e-02,  2.5801e-02, -1.9573e-02,\n",
              "                         5.3509e-04, -1.7728e-03, -1.3527e-02,  4.5820e-02, -3.4069e-02,\n",
              "                         1.7698e-02,  7.5421e-02, -6.3700e-02, -3.0448e-02, -2.7430e-02],\n",
              "                       [ 2.8988e-02,  6.9403e-02, -3.2563e-03,  7.7987e-02,  8.0167e-03,\n",
              "                         1.6089e-02, -3.8839e-02, -4.0839e-02, -3.2082e-02,  4.6589e-02,\n",
              "                         7.4931e-02,  4.9474e-02,  3.2856e-02, -6.6400e-02, -6.1781e-02,\n",
              "                         4.6829e-02, -3.7084e-02, -6.8850e-02,  3.2971e-02,  4.1927e-02,\n",
              "                        -3.2635e-02,  4.4352e-02, -4.0929e-03,  3.7669e-02, -3.6937e-03,\n",
              "                         6.4815e-02, -4.2580e-02, -5.4322e-02, -5.8127e-02, -8.7050e-03,\n",
              "                         4.7818e-02, -6.5117e-02,  6.7883e-02, -3.3214e-02, -7.7505e-02,\n",
              "                        -7.8778e-02, -7.5189e-03, -2.0560e-02,  4.2127e-02,  9.5291e-03,\n",
              "                        -2.9575e-02,  7.1921e-02,  5.6600e-02,  2.9830e-02,  4.2586e-02,\n",
              "                         2.1425e-02, -1.1421e-03, -4.6738e-02,  1.8713e-02, -5.3729e-02,\n",
              "                        -4.5245e-02, -3.8495e-02, -1.7641e-02, -6.9647e-02, -6.7312e-02,\n",
              "                         6.1950e-03,  3.6677e-02, -1.3874e-02,  1.3966e-02,  1.8666e-02,\n",
              "                        -1.0319e-03, -2.8271e-02, -6.9216e-02,  1.5687e-02, -2.1901e-03,\n",
              "                        -5.5632e-02, -3.5504e-02,  6.0306e-02, -4.7511e-02, -2.1211e-02,\n",
              "                        -5.8940e-02, -5.0557e-02, -1.1214e-02, -3.0668e-02, -6.7047e-02,\n",
              "                         5.7950e-02,  2.5869e-03,  6.6520e-02, -6.4824e-02,  2.5034e-02,\n",
              "                        -6.4842e-02, -2.3826e-02, -7.0049e-02, -6.2261e-02,  2.8883e-02,\n",
              "                        -3.0564e-02,  6.7790e-02,  4.4660e-02, -5.0830e-02, -6.1571e-02,\n",
              "                        -3.9503e-02,  1.1074e-02,  4.0051e-02, -4.8128e-03,  2.3322e-02,\n",
              "                         5.1566e-02, -6.5171e-03, -5.6739e-02,  8.5478e-03, -4.4048e-02,\n",
              "                        -6.4217e-02, -2.2351e-02, -7.5686e-02, -5.2335e-02,  4.8960e-02,\n",
              "                         6.4844e-02, -5.4482e-02, -2.6947e-02,  6.4613e-03, -1.9299e-02,\n",
              "                         1.7257e-02,  4.2767e-02, -5.5453e-02, -2.8436e-02, -2.3573e-02,\n",
              "                        -1.1951e-02, -7.0426e-02, -3.7376e-02, -1.0475e-02,  8.2089e-03,\n",
              "                         6.7040e-02, -4.0894e-02,  5.2658e-03, -3.9855e-02, -2.0522e-02,\n",
              "                         6.2064e-02,  5.7081e-02, -7.7989e-02, -6.9501e-03,  5.5737e-02,\n",
              "                        -5.2463e-02,  3.4290e-03,  2.9451e-02,  1.6528e-03, -4.9900e-02,\n",
              "                        -2.2880e-02,  7.5731e-02, -7.0878e-02, -2.0355e-02,  5.1069e-03,\n",
              "                        -1.1689e-02,  1.1988e-02, -5.9860e-02,  1.7217e-02,  5.7058e-02,\n",
              "                        -6.1574e-02, -7.0457e-02,  3.9816e-02,  6.7962e-02, -6.4541e-02,\n",
              "                        -4.0068e-02, -7.5959e-02, -1.8775e-02,  1.7740e-02, -6.3422e-02,\n",
              "                        -2.6274e-02,  1.8288e-02,  7.7721e-02, -9.2234e-03, -2.4866e-02],\n",
              "                       [ 6.4263e-02, -6.1574e-02, -6.1713e-02,  6.2419e-02,  1.4259e-02,\n",
              "                        -3.8742e-02,  7.0762e-02,  7.1175e-02,  2.9931e-02,  2.6730e-03,\n",
              "                         1.0708e-02,  5.7530e-02, -5.3496e-02, -3.6104e-02, -6.4829e-02,\n",
              "                        -2.9283e-02, -5.2778e-02,  7.6030e-02,  6.5780e-02, -3.8622e-03,\n",
              "                         3.9266e-02,  6.8062e-02, -5.5540e-02,  1.4649e-03, -5.4339e-03,\n",
              "                        -7.0327e-02, -4.5121e-02,  5.6382e-02, -5.7877e-03,  4.1537e-03,\n",
              "                         4.1295e-02, -4.0355e-02, -1.5572e-02,  3.8245e-02, -4.0065e-02,\n",
              "                         5.8485e-02,  4.1704e-02, -1.3455e-02, -2.3254e-02, -5.2204e-02,\n",
              "                        -2.4966e-03, -4.3189e-02, -2.2893e-02,  5.3244e-02, -7.0607e-03,\n",
              "                        -1.7732e-02, -1.9230e-02, -5.6366e-02, -6.1179e-02, -1.8354e-02,\n",
              "                        -6.0567e-02, -2.3320e-02, -4.3976e-02,  4.0913e-02, -3.1212e-02,\n",
              "                        -4.1659e-02, -7.2322e-02, -4.1199e-02, -6.5248e-02,  5.3023e-02,\n",
              "                        -4.0819e-02,  5.7153e-02,  4.9970e-02,  6.4011e-02, -6.1865e-02,\n",
              "                        -6.3785e-02, -8.5311e-03,  7.3273e-02,  6.4877e-02,  7.2991e-02,\n",
              "                         3.4765e-02,  2.2143e-02,  7.1527e-02,  1.9274e-02,  6.2087e-02,\n",
              "                        -5.6306e-02, -3.8437e-02,  5.8906e-03, -3.0751e-02, -6.6345e-02,\n",
              "                         4.5941e-02, -2.3031e-02,  4.7147e-02, -2.0008e-02, -3.5152e-03,\n",
              "                         2.0568e-02,  4.0258e-02, -5.3604e-02,  1.2212e-02, -3.8988e-02,\n",
              "                         2.8843e-02, -6.4879e-02,  1.1598e-02, -6.2990e-02,  2.7353e-02,\n",
              "                         5.2782e-02,  4.1709e-02, -2.0803e-02,  2.9150e-02, -2.7373e-04,\n",
              "                        -7.7288e-02,  7.0322e-02,  7.5400e-02, -2.1585e-02,  5.4124e-02,\n",
              "                        -5.0513e-02, -2.0249e-02, -2.2350e-02,  1.8043e-02, -3.1297e-02,\n",
              "                         2.0886e-02, -4.1085e-02, -5.5434e-02,  6.4465e-02, -1.5737e-02,\n",
              "                        -1.2623e-02, -1.0322e-02,  4.3433e-03, -5.8885e-02,  1.5752e-02,\n",
              "                        -2.0617e-02, -3.9256e-02, -1.0676e-02,  6.6347e-05,  7.4531e-02,\n",
              "                        -7.1859e-02,  7.8623e-02, -1.3345e-02, -2.6058e-02,  1.5113e-02,\n",
              "                         3.2870e-02,  3.4857e-02, -4.6625e-02,  1.5080e-02, -6.8522e-02,\n",
              "                         4.9217e-03,  3.2605e-02, -6.2482e-02, -3.7014e-04, -5.3625e-02,\n",
              "                        -3.2733e-02, -4.7461e-02,  4.3238e-02, -3.2503e-02, -4.7115e-02,\n",
              "                         7.1121e-02,  5.1146e-02, -6.2272e-02,  1.9944e-02, -5.2840e-02,\n",
              "                        -3.4124e-02, -1.3733e-02,  1.2509e-03,  2.9995e-02, -6.0652e-02,\n",
              "                        -4.6542e-02, -4.7072e-02,  6.0250e-02,  5.7881e-02, -2.5673e-02],\n",
              "                       [-7.1043e-02,  7.3670e-02, -7.1870e-02, -3.3862e-03, -2.9097e-02,\n",
              "                         8.1307e-03, -1.5885e-02, -3.0013e-02, -2.6780e-02,  2.7364e-02,\n",
              "                        -3.2649e-02,  1.9437e-02,  5.4239e-02, -3.7688e-02,  5.9701e-02,\n",
              "                         6.9348e-03, -1.3386e-03, -3.7355e-03,  2.2883e-02,  4.9745e-02,\n",
              "                        -1.1484e-02, -4.7117e-02, -1.5319e-02, -7.4800e-02, -5.1432e-02,\n",
              "                         9.1760e-03, -2.1073e-03, -3.0927e-02, -1.8241e-02,  5.2669e-03,\n",
              "                        -6.0246e-02,  3.8015e-02,  5.6848e-02, -4.4772e-03, -1.4649e-02,\n",
              "                        -4.4796e-02,  3.0536e-02,  1.2583e-02,  1.3102e-02,  6.1619e-02,\n",
              "                        -4.5188e-04,  3.6558e-02,  1.7413e-02, -4.1605e-02, -7.9668e-03,\n",
              "                        -6.5653e-02,  4.9994e-02, -3.8456e-02, -2.9510e-02, -5.9715e-02,\n",
              "                         1.0897e-02,  7.0487e-02,  1.3824e-02,  3.9480e-02, -6.8693e-02,\n",
              "                         1.4868e-02, -5.9084e-02, -3.8009e-02,  7.7545e-02, -3.4998e-03,\n",
              "                        -3.3854e-02, -1.0610e-03, -3.6534e-02, -1.0584e-03, -1.2117e-02,\n",
              "                         5.2359e-02, -4.7393e-03,  4.4409e-02,  1.9212e-04, -4.0374e-02,\n",
              "                        -9.8645e-03, -2.3429e-02, -2.3216e-02, -2.0565e-02, -6.0259e-02,\n",
              "                        -2.4129e-02, -3.9444e-02, -5.2712e-02,  2.6125e-02, -1.4401e-02,\n",
              "                        -2.6762e-02,  3.8344e-02,  1.5920e-02,  8.9026e-03,  2.2886e-02,\n",
              "                        -6.6686e-02, -6.6124e-02,  5.4906e-03, -5.3193e-02,  2.1446e-02,\n",
              "                        -4.0379e-02, -4.2676e-02, -7.6450e-02,  7.0398e-02, -5.9340e-02,\n",
              "                        -5.7293e-02,  6.9000e-02, -1.9689e-02,  4.5104e-02,  8.0609e-03,\n",
              "                        -4.8827e-02, -1.1214e-02,  3.9182e-02, -4.4561e-02, -4.0636e-02,\n",
              "                         1.7123e-02,  7.0131e-02,  6.9537e-02,  4.8117e-02,  2.2336e-02,\n",
              "                         9.8900e-03,  5.6715e-02,  3.2458e-02,  3.1917e-02,  4.1953e-02,\n",
              "                        -1.3695e-02,  3.1141e-02,  3.9588e-02,  4.2855e-02,  2.9139e-02,\n",
              "                         1.2428e-02, -5.6020e-02, -1.9136e-02,  3.0429e-02,  1.1052e-02,\n",
              "                        -1.0995e-03, -2.2619e-02, -1.8750e-02,  6.3244e-02,  6.8526e-02,\n",
              "                         7.8325e-02,  7.1386e-02, -2.1783e-02,  1.4489e-02, -6.0941e-02,\n",
              "                        -4.4410e-02, -5.8821e-02,  2.6472e-03,  3.6976e-02, -7.5360e-02,\n",
              "                        -7.6657e-02,  5.5360e-02,  1.2187e-02,  2.8455e-02,  4.2731e-02,\n",
              "                        -3.4134e-02,  3.7745e-02, -5.3589e-02, -7.3090e-02,  9.4743e-03,\n",
              "                         5.9991e-02,  4.3679e-03, -5.7644e-02,  6.1886e-02,  6.7184e-02,\n",
              "                         7.0642e-02,  7.7267e-02,  6.7485e-02,  4.7296e-02, -7.8148e-02]])),\n",
              "              ('decoder_convs.1.lin_v.bias',\n",
              "               tensor([-0.0215,  0.0405,  0.0230,  0.0130,  0.0154,  0.0390])),\n",
              "              ('decoder_convs.1.lin_e.weight',\n",
              "               tensor([[-0.0512,  0.0851,  0.0392, -0.0466,  0.0437, -0.0483,  0.0091, -0.0375,\n",
              "                         0.0086,  0.0867,  0.0731,  0.0661, -0.0332,  0.0566,  0.0767, -0.0916,\n",
              "                        -0.0331,  0.0158,  0.0767,  0.1002,  0.0775,  0.0240, -0.0030, -0.0972,\n",
              "                         0.0638, -0.0342,  0.0452,  0.0600, -0.0112,  0.0310,  0.0618, -0.0297,\n",
              "                         0.0830,  0.0373,  0.0197,  0.0823,  0.0319,  0.0222,  0.0701, -0.0123,\n",
              "                        -0.0970,  0.0405, -0.0170,  0.0141, -0.0660, -0.0023, -0.0580, -0.0107,\n",
              "                         0.0353,  0.0560,  0.0593,  0.0873, -0.0860,  0.0553, -0.0447,  0.0100,\n",
              "                         0.0475, -0.0719,  0.0316,  0.0764, -0.0690, -0.0483, -0.0174,  0.0850,\n",
              "                        -0.0173, -0.0108, -0.0840, -0.0498,  0.0773,  0.0113, -0.0924, -0.0153,\n",
              "                         0.0741,  0.0132, -0.0846,  0.0571,  0.0513, -0.0914,  0.0970, -0.0400,\n",
              "                        -0.0890, -0.0330, -0.0942,  0.0714,  0.0700,  0.0169,  0.0400,  0.0972,\n",
              "                        -0.0335,  0.0621,  0.0177,  0.0651, -0.0348,  0.0102,  0.0045, -0.0508],\n",
              "                       [ 0.0022,  0.0444, -0.0717,  0.0656, -0.0090,  0.0736, -0.0639,  0.0490,\n",
              "                        -0.0595,  0.0199,  0.0578,  0.0328,  0.0200, -0.0408, -0.0973,  0.0793,\n",
              "                        -0.0387,  0.0908, -0.0123,  0.0643,  0.0735,  0.0338,  0.0444,  0.0413,\n",
              "                         0.0640, -0.0737, -0.0067,  0.0181, -0.0161, -0.0231,  0.0051, -0.0961,\n",
              "                        -0.0762, -0.0876,  0.0911,  0.0594, -0.0964, -0.0621, -0.0784,  0.0135,\n",
              "                         0.0002, -0.0094, -0.0343,  0.0048,  0.0557,  0.0505, -0.0304,  0.0651,\n",
              "                        -0.0776, -0.0846, -0.0771, -0.0150,  0.0943, -0.0587,  0.0941,  0.0234,\n",
              "                        -0.0724,  0.0756, -0.0619, -0.0819, -0.0018,  0.0533, -0.0848, -0.0268,\n",
              "                        -0.0309,  0.0800,  0.0714, -0.1017,  0.0176, -0.0105,  0.0163, -0.0293,\n",
              "                        -0.0176,  0.0373,  0.0690, -0.0053,  0.0374, -0.0246,  0.0363,  0.0541,\n",
              "                         0.0506,  0.0599, -0.0780, -0.0281,  0.0568, -0.0405,  0.0507, -0.0439,\n",
              "                         0.0143, -0.0455,  0.0066, -0.0014,  0.0244, -0.0562, -0.0578, -0.0148],\n",
              "                       [ 0.0631,  0.0337,  0.0148, -0.0733, -0.0282,  0.0469,  0.0960, -0.0331,\n",
              "                         0.0447, -0.0302,  0.0144,  0.0467,  0.0833, -0.0588, -0.0803,  0.0116,\n",
              "                         0.0513,  0.0397, -0.0704,  0.0994, -0.0230, -0.0309,  0.0914, -0.0838,\n",
              "                        -0.0653, -0.0471,  0.0612,  0.0683, -0.0826,  0.0994,  0.0508, -0.0893,\n",
              "                         0.0531, -0.0399, -0.0734, -0.0281,  0.0496,  0.0804,  0.0601, -0.0761,\n",
              "                         0.0944,  0.0142,  0.0175,  0.0181, -0.0274,  0.0243,  0.0940, -0.0286,\n",
              "                        -0.0652, -0.0569, -0.0818,  0.0318,  0.0287, -0.0853, -0.0167,  0.0439,\n",
              "                         0.0814,  0.0765, -0.0737, -0.0167, -0.0051, -0.0852,  0.0116,  0.0464,\n",
              "                         0.0415, -0.0624,  0.0070, -0.0710,  0.0360, -0.0806, -0.0769,  0.0158,\n",
              "                         0.0348,  0.0205,  0.0811,  0.0323,  0.0088, -0.0178,  0.0419, -0.0297,\n",
              "                         0.0895,  0.0420,  0.0370, -0.0350,  0.1006, -0.0460, -0.0512,  0.0770,\n",
              "                         0.0942, -0.0175, -0.0133, -0.0548, -0.0617,  0.0837,  0.0896, -0.0032],\n",
              "                       [ 0.0660,  0.0310,  0.0078, -0.0460, -0.0417,  0.0667,  0.0086,  0.0340,\n",
              "                         0.0838,  0.0493,  0.0991,  0.0804,  0.0598,  0.0993,  0.0448,  0.0788,\n",
              "                        -0.0030, -0.0988,  0.0945,  0.0292,  0.0035, -0.0088, -0.0412, -0.0396,\n",
              "                         0.0277, -0.0173,  0.0425, -0.0413, -0.0660, -0.0739, -0.0250,  0.0773,\n",
              "                        -0.0100,  0.0679,  0.0648,  0.0213, -0.0007, -0.0824, -0.0521,  0.0028,\n",
              "                         0.0836, -0.0846, -0.0183,  0.0969, -0.0915,  0.0794,  0.0850, -0.0677,\n",
              "                         0.0689,  0.0548,  0.0972,  0.0580,  0.0582,  0.0902, -0.0613,  0.0994,\n",
              "                        -0.0328, -0.0520,  0.0677,  0.0230, -0.0228, -0.0390, -0.0324,  0.0746,\n",
              "                         0.0384, -0.0846, -0.0014, -0.0008, -0.0475, -0.0623, -0.0607, -0.0361,\n",
              "                         0.0399, -0.0670, -0.0007, -0.0535,  0.0728, -0.0386,  0.0032, -0.0726,\n",
              "                        -0.0819, -0.0111,  0.0912, -0.0916, -0.0774,  0.0566,  0.1020, -0.0114,\n",
              "                         0.0669,  0.0154, -0.0499, -0.0197, -0.0648,  0.0470,  0.0709, -0.0051],\n",
              "                       [ 0.0142, -0.0869,  0.0031,  0.0438,  0.0572,  0.0479, -0.0894, -0.0662,\n",
              "                         0.0538, -0.1003,  0.0404, -0.0083,  0.0032, -0.0881, -0.0140, -0.0151,\n",
              "                         0.0673, -0.0555,  0.0709, -0.0870, -0.0071,  0.0299, -0.0477, -0.0639,\n",
              "                        -0.0299,  0.0500, -0.0633, -0.0546, -0.0135,  0.0700,  0.0643, -0.0307,\n",
              "                         0.0766,  0.0817,  0.0694, -0.0071, -0.0174, -0.0087, -0.0400, -0.0106,\n",
              "                        -0.1012,  0.0700, -0.0327, -0.0224,  0.0093, -0.0447,  0.0066, -0.0677,\n",
              "                         0.0928,  0.0880, -0.0686, -0.0570,  0.1005,  0.0809, -0.0171, -0.0330,\n",
              "                        -0.0934,  0.0547,  0.0506,  0.0272, -0.0737, -0.0199, -0.0384, -0.0890,\n",
              "                         0.0511, -0.0072, -0.0626,  0.0878,  0.0670,  0.1011, -0.0938, -0.0240,\n",
              "                        -0.0734,  0.0417,  0.0707, -0.0375, -0.0813,  0.0609, -0.0067, -0.0026,\n",
              "                         0.0701,  0.0808,  0.0563,  0.0363, -0.0010,  0.0743, -0.0012, -0.0128,\n",
              "                        -0.0978, -0.0804, -0.0959,  0.0746, -0.0592,  0.0754, -0.0721, -0.0814]])),\n",
              "              ('decoder_convs.1.lin_e.bias',\n",
              "               tensor([-0.0747,  0.0940,  0.0798,  0.0519,  0.0788])),\n",
              "              ('decoder_convs.1.bn_u.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1.])),\n",
              "              ('decoder_convs.1.bn_u.bias', tensor([0., 0., 0., 0., 0., 0.])),\n",
              "              ('decoder_convs.1.bn_u.running_mean',\n",
              "               tensor([-0.1282,  0.0252, -0.0364, -0.2114, -0.0257,  0.0349])),\n",
              "              ('decoder_convs.1.bn_u.running_var',\n",
              "               tensor([0.7229, 0.6742, 0.6509, 0.7823, 0.7243, 0.7477])),\n",
              "              ('decoder_convs.1.bn_u.num_batches_tracked', tensor(5)),\n",
              "              ('decoder_convs.1.bn_v.weight',\n",
              "               tensor([1., 1., 1., 1., 1., 1.])),\n",
              "              ('decoder_convs.1.bn_v.bias', tensor([0., 0., 0., 0., 0., 0.])),\n",
              "              ('decoder_convs.1.bn_v.running_mean',\n",
              "               tensor([-0.1582, -0.0071,  0.0268, -0.0914, -0.0563, -0.0520])),\n",
              "              ('decoder_convs.1.bn_v.running_var',\n",
              "               tensor([0.6400, 0.6076, 0.6422, 0.6326, 0.6108, 0.6317])),\n",
              "              ('decoder_convs.1.bn_v.num_batches_tracked', tensor(5)),\n",
              "              ('decoder_convs.1.bn_e.weight', tensor([1., 1., 1., 1., 1.])),\n",
              "              ('decoder_convs.1.bn_e.bias', tensor([0., 0., 0., 0., 0.])),\n",
              "              ('decoder_convs.1.bn_e.running_mean',\n",
              "               tensor([0.0141, 0.1006, 0.1487, 0.0183, 0.0337])),\n",
              "              ('decoder_convs.1.bn_e.running_var',\n",
              "               tensor([0.6293, 0.6292, 0.6664, 0.6944, 0.7237])),\n",
              "              ('decoder_convs.1.bn_e.num_batches_tracked', tensor(5)),\n",
              "              ('u_mlp_layers.0.weight',\n",
              "               tensor([[-0.0385,  0.1498,  0.1702,  ...,  0.0640, -0.1229,  0.1076],\n",
              "                       [ 0.0198, -0.1193, -0.0945,  ..., -0.0440,  0.0432,  0.0407],\n",
              "                       [-0.0813,  0.1307,  0.1270,  ...,  0.0534,  0.0161, -0.1701],\n",
              "                       ...,\n",
              "                       [-0.1244,  0.0818, -0.1009,  ...,  0.0123,  0.0478, -0.1482],\n",
              "                       [-0.0005,  0.0661,  0.0241,  ...,  0.0208,  0.0729, -0.0297],\n",
              "                       [ 0.0255,  0.1738,  0.0116,  ...,  0.0729,  0.1579,  0.0654]])),\n",
              "              ('u_mlp_layers.0.bias',\n",
              "               tensor([-0.1424,  0.1219,  0.0247, -0.0738,  0.1003, -0.1287, -0.1081,  0.1721,\n",
              "                       -0.0594,  0.1002,  0.0053, -0.0485,  0.0838,  0.1293, -0.1417,  0.0974,\n",
              "                        0.0006,  0.0879, -0.0255,  0.1622,  0.1085, -0.0920, -0.1445, -0.1299,\n",
              "                       -0.0424, -0.0096,  0.1021,  0.0237,  0.0182, -0.0134, -0.0228,  0.1181])),\n",
              "              ('u_mlp_layers.1.weight',\n",
              "               tensor([[ 0.1666, -0.1478, -0.0842,  ...,  0.1325, -0.0051,  0.1213],\n",
              "                       [ 0.0909,  0.0584,  0.0666,  ..., -0.0279, -0.0147,  0.0442],\n",
              "                       [ 0.0124, -0.0739, -0.1529,  ...,  0.1568, -0.0176, -0.0614],\n",
              "                       ...,\n",
              "                       [-0.1615, -0.0389,  0.0030,  ...,  0.1192,  0.1051, -0.0579],\n",
              "                       [-0.1392,  0.0344, -0.0341,  ...,  0.1125,  0.0238, -0.0925],\n",
              "                       [ 0.0369,  0.0803,  0.0904,  ..., -0.1005, -0.1320,  0.1437]])),\n",
              "              ('u_mlp_layers.1.bias',\n",
              "               tensor([ 0.0364,  0.1502,  0.0726, -0.0839,  0.1273, -0.0779, -0.0507, -0.0637,\n",
              "                       -0.0741,  0.0156,  0.0226,  0.0329, -0.0146, -0.0496,  0.0163, -0.1395,\n",
              "                       -0.0449, -0.0336, -0.0036,  0.1387,  0.0292,  0.1055,  0.1554,  0.0221,\n",
              "                       -0.0437, -0.1042, -0.0156, -0.0990, -0.1609,  0.1764,  0.0167, -0.1309])),\n",
              "              ('v_mlp_layers.0.weight',\n",
              "               tensor([[-0.1557, -0.1607, -0.1369,  ..., -0.0370,  0.0938,  0.0150],\n",
              "                       [ 0.1113,  0.0552,  0.0877,  ..., -0.0675, -0.1727, -0.1100],\n",
              "                       [ 0.0164,  0.0111,  0.0594,  ..., -0.1195,  0.1228,  0.0510],\n",
              "                       ...,\n",
              "                       [-0.1221, -0.0647, -0.1033,  ...,  0.0587,  0.0346, -0.0046],\n",
              "                       [ 0.1171, -0.1041, -0.0679,  ...,  0.0073, -0.0356,  0.0167],\n",
              "                       [-0.0946, -0.0613,  0.1632,  ...,  0.1082, -0.1508,  0.1036]])),\n",
              "              ('v_mlp_layers.0.bias',\n",
              "               tensor([-0.1352,  0.0965,  0.0291,  0.0546,  0.1188,  0.0027,  0.1418,  0.0621,\n",
              "                        0.0788,  0.0076, -0.1483,  0.0916,  0.0602,  0.0385, -0.0133, -0.1274,\n",
              "                       -0.1477,  0.0307, -0.1656,  0.0762,  0.1219,  0.0336, -0.0033, -0.1600,\n",
              "                        0.1063, -0.1177,  0.0171,  0.0260,  0.0236,  0.0465,  0.0164, -0.0138])),\n",
              "              ('v_mlp_layers.1.weight',\n",
              "               tensor([[-2.6940e-02,  1.6081e-01,  4.0464e-02,  ..., -1.4659e-01,\n",
              "                         9.8477e-02, -6.6494e-02],\n",
              "                       [-7.8240e-02,  3.1538e-02, -1.3547e-01,  ..., -1.3691e-01,\n",
              "                         7.8576e-02, -8.8625e-02],\n",
              "                       [ 4.1653e-02, -1.0514e-01, -1.5990e-01,  ...,  1.0191e-01,\n",
              "                         5.8335e-02, -4.8924e-02],\n",
              "                       ...,\n",
              "                       [ 3.0158e-02, -6.3745e-02, -9.9152e-02,  ..., -4.1276e-05,\n",
              "                        -6.4017e-02,  4.3746e-02],\n",
              "                       [ 1.7463e-01,  4.7884e-02,  1.6530e-02,  ..., -6.2960e-02,\n",
              "                         1.2066e-01,  1.4906e-01],\n",
              "                       [-1.6290e-01,  5.7225e-02,  3.6661e-02,  ..., -9.5748e-02,\n",
              "                         1.3060e-01, -6.0078e-02]])),\n",
              "              ('v_mlp_layers.1.bias',\n",
              "               tensor([ 0.1112, -0.0615,  0.0818,  0.1064, -0.1558, -0.0185, -0.1152, -0.0652,\n",
              "                        0.0522, -0.1272, -0.0386,  0.0136,  0.0557, -0.0505,  0.1522, -0.1155,\n",
              "                        0.0355, -0.1070, -0.0693,  0.0542, -0.0183,  0.0385,  0.0718,  0.1383,\n",
              "                        0.0129,  0.0157,  0.0613, -0.1335, -0.0356, -0.0743,  0.1641, -0.1161]))]),\n",
              " 'optimizer_state_dict': {'state': {},\n",
              "  'param_groups': [{'lr': 0.01,\n",
              "    'betas': (0.9, 0.999),\n",
              "    'eps': 1e-08,\n",
              "    'weight_decay': 0,\n",
              "    'amsgrad': False,\n",
              "    'maximize': False,\n",
              "    'foreach': None,\n",
              "    'capturable': False,\n",
              "    'differentiable': False,\n",
              "    'fused': False,\n",
              "    'initial_lr': 0.01,\n",
              "    'params': [0,\n",
              "     1,\n",
              "     2,\n",
              "     3,\n",
              "     4,\n",
              "     5,\n",
              "     6,\n",
              "     7,\n",
              "     8,\n",
              "     9,\n",
              "     10,\n",
              "     11,\n",
              "     12,\n",
              "     13,\n",
              "     14,\n",
              "     15,\n",
              "     16,\n",
              "     17,\n",
              "     18,\n",
              "     19,\n",
              "     20,\n",
              "     21,\n",
              "     22,\n",
              "     23,\n",
              "     24,\n",
              "     25,\n",
              "     26,\n",
              "     27,\n",
              "     28,\n",
              "     29,\n",
              "     30,\n",
              "     31,\n",
              "     32,\n",
              "     33,\n",
              "     34,\n",
              "     35,\n",
              "     36,\n",
              "     37,\n",
              "     38,\n",
              "     39,\n",
              "     40,\n",
              "     41,\n",
              "     42,\n",
              "     43,\n",
              "     44,\n",
              "     45,\n",
              "     46,\n",
              "     47,\n",
              "     48,\n",
              "     49,\n",
              "     50,\n",
              "     51]}]}}"
            ]
          },
          "execution_count": 571,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_stored = {\n",
        "    \"loss\": loss,\n",
        "    \"loss_component\": loss_component,\n",
        "    \"epred_metric\": epred_metric,\n",
        "    \"eval_metrics\": eval_metrics,\n",
        "#     \"loss_hist\": loss_hist,\n",
        "#     \"loss_component_hist\": loss_component_hist,\n",
        "#     \"epred_metric_hist\": epred_metric_hist,\n",
        "    \"state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "model_stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-02T03:29:02.588366Z",
          "start_time": "2024-01-02T03:29:02.578037Z"
        },
        "scrolled": false,
        "id": "Hv8RXznmFfSn",
        "outputId": "1904363d-b380-4fa9-8dda-98e8d71c779a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'xu_error': tensor([0.9949, 0.9385, 0.7005,  ..., 0.6952, 1.0288, 1.1901]),\n",
              " 'xv_error': tensor([0.5136, 0.7549, 0.6990,  ..., 0.6257, 0.5189, 0.7509]),\n",
              " 'xe_error': tensor([1.0124, 0.8771, 0.8788,  ..., 0.6671, 0.5064, 0.8901]),\n",
              " 'edge_ce': tensor([0.4791, 0.8085, 0.7113,  ..., 0.5795, 0.6261, 0.4625]),\n",
              " 'e_score': tensor([1.4916, 1.6857, 1.5901,  ..., 1.2466, 1.1326, 1.3526]),\n",
              " 'u_score_edge_max': tensor([2.4865, 2.6242, 2.2906,  ..., 1.9417, 2.1614, 2.5426]),\n",
              " 'u_score_edge_mean': tensor([2.4865, 2.6242, 2.2906,  ..., 1.9417, 2.1614, 2.5426]),\n",
              " 'u_score_edge_sum': tensor([2.4865, 2.6242, 2.2906,  ..., 1.9417, 2.1614, 2.5426]),\n",
              " 'v_score_edge_max': tensor([2.1359, 2.3901, 2.2753,  ..., 2.1165, 2.1122, 2.5027]),\n",
              " 'v_score_edge_mean': tensor([2.1359, 2.3901, 2.2753,  ..., 2.1165, 2.1122, 2.5027]),\n",
              " 'v_score_edge_sum': tensor([2.1359, 2.3901, 2.2753,  ..., 2.1165, 2.1122, 2.5027])}"
            ]
          },
          "execution_count": 573,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anomaly_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5eXeScfFfSn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_nn",
      "language": "python",
      "name": "venv_nn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}